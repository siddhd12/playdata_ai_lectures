{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "- 학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다. \n",
    "- 파이토치는 모델의 파라미터만 저장하는 방법과 모델 구조와 파라미터 모두를 저장하는 두가지 방식을 제공한다.\n",
    "- 저장 함수\n",
    "    - `torch.save(저장할 객체, 저장경로)`\n",
    "- 보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.\n",
    "\n",
    "## 모델 전체 저장하기 및 불러오기\n",
    "\n",
    "- 저장하기\n",
    "    - `torch.save(model, 저장경로)`\n",
    "- 불러오기\n",
    "    - `load_model = torch.load(저장경로)`\n",
    "- 저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 모델을 저장할 때 사용한 클래스가 있어야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 모델의 파라미터만 저장\n",
    "- 모델을 구성하는 파라미터만 저장한다.\n",
    "- 모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "- 모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "- 모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "- `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "- 모델의 state_dict을 조회 후 저장한다.\n",
    "    - `torch.save(model.state_dict(), \"저장경로\")`\n",
    "- 생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    - `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(784, 64)\n",
    "        self.out = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = torch.flatten(X, start_dim=1)\n",
    "        X = nn.ReLU()(self.in_layer(X))\n",
    "        X = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (in_layer): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model = Network()\n",
    "sample_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['in_layer.weight', 'in_layer.bias', 'out.weight', 'out.bias'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict 조회\n",
    "sd = sample_model.state_dict()\n",
    "print(type(sd))\n",
    "sd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['in_layer.weight'].shape, sd['in_layer.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001CB0A21D850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0055,  0.0246, -0.0072, -0.0142, -0.0218,  0.0126,  0.0249, -0.0322,\n",
       "         0.0211,  0.0097, -0.0193,  0.0220, -0.0068, -0.0100, -0.0125,  0.0239,\n",
       "        -0.0040,  0.0238, -0.0121,  0.0220,  0.0180,  0.0205,  0.0301,  0.0046,\n",
       "        -0.0354, -0.0190, -0.0125,  0.0032, -0.0087, -0.0123, -0.0008, -0.0132,\n",
       "        -0.0321, -0.0076, -0.0132, -0.0089, -0.0293, -0.0228, -0.0021, -0.0180,\n",
       "         0.0047, -0.0042, -0.0003, -0.0150, -0.0047, -0.0205, -0.0142,  0.0205,\n",
       "        -0.0019, -0.0147,  0.0291, -0.0250,  0.0012,  0.0170,  0.0240,  0.0094,\n",
       "        -0.0316,  0.0333,  0.0057, -0.0159, -0.0007,  0.0087,  0.0045, -0.0062],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.in_layer.weight\n",
    "sample_model.in_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint를 저장 및 불러오기\n",
    "- 학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.\n",
    "- Dictionary에 필요한 요소들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameter())\n",
    "\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#### 이어학습\n",
    "model.train()\n",
    "#### 추론\n",
    "model.eval()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- MLP(Multi Layer Perceptron)\n",
    "    - Fully Connected Layer로 구성된 네트워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchinfo\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Regression(회귀)\n",
    "\n",
    "## Boston Housing Dataset\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "- CRIM: 범죄율\n",
    "- ZN: 25,000 평방피트당 주거지역 비율\n",
    "- INDUS: 비소매 상업지구 비율\n",
    "- CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "- NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "- RM: 주택당 방의 수\n",
    "- AGE: 1940년 이전에 건설된 주택의 비율\n",
    "- DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "- RAD: 고속도로 접근성\n",
    "- TAX: 재산세율\n",
    "- PTRATIO: 학생/교사 비율\n",
    "- B: 흑인 비율\n",
    "- LSTAT: 하위 계층 비율\n",
    "<br><br>\n",
    "- **Target**\n",
    "    - MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = pd.read_csv('data/boston_hosing.csv')\n",
    "boston.shape\n",
    "\n",
    "X_boston = boston.drop(columns='MEDV').values\n",
    "y_boston = boston['MEDV'].values.reshape(-1, 1) # 2차원\n",
    "X_boston.shape, y_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13) (404, 1) (102, 13) (102, 1)\n"
     ]
    }
   ],
   "source": [
    "# train/test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n",
    "X_test_scaled = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n",
    "# y를 Tensor 타입으로 변환.\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 102\n",
      "(tensor([-0.3726, -0.4996, -0.7049,  3.6645, -0.4249,  0.9357,  0.6937, -0.4372,\n",
      "        -0.1622, -0.5617, -0.4846,  0.3717, -0.4110]), tensor([26.7000]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "boston_train_set = TensorDataset(X_train_scaled, y_train_tensor)\n",
    "boston_test_set = TensorDataset(X_test_scaled, y_test_tensor)\n",
    "print(len(boston_train_set), len(boston_test_set))\n",
    "print(boston_train_set[0])\n",
    "\n",
    "# DataLoader\n",
    "boston_train_loader = DataLoader(boston_train_set, batch_size=200, \n",
    "                                 shuffle=True, drop_last=True)\n",
    "boston_test_loader = DataLoader(boston_test_set, batch_size=len(boston_test_set))\n",
    "len(boston_train_loader), len(boston_test_loader)  # epoch 당 step 수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 입력 layer => in_feature: input data의 feature개수에 맞춘다.\n",
    "        self.lr1 = nn.Linear(13, 32)   # input\n",
    "        # Hidden layer => in_feature: 앞 Layer의 out_feature 개수에 맞춘다.\n",
    "        self.lr2 = nn.Linear(32, 16)  \n",
    "        # output layer => out_feature: 모델의 최종 출력 개수에 맞춘다. (집값 1개->1)\n",
    "        self.lr3 = nn.Linear(16, 1)\n",
    "    \n",
    "        \n",
    "    def forward(self, X):\n",
    "        # input layer\n",
    "        out = self.lr1(X)\n",
    "        out = nn.ReLU()(out)\n",
    "        # hidden\n",
    "        out = self.lr2(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        # output -> 회귀처리 모델에서 output layer에서는 활성함수를 적용하지 않는다.\n",
    "        #   예외: 출력결과가 특정 활성함수의 출력과 매칭될 경우.\n",
    "        #        output: 0 ~ 1 => logistic 함수사용.\n",
    "        #        output: -1 ~ 1=> hyperbolic tangent (tanh)\n",
    "        out  = self.lr3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_model = BostonModel()\n",
    "torchinfo.summary(boston_model, (200, 13))  #(모델, 입력데이터shape-(batch size, feature) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 1000\n",
    "LR = 0.001\n",
    "# 결과 저장할 리스트\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# 모델, loss함수(회귀-MSE), optimizer\n",
    "boston_model = BostonModel()\n",
    "boston_model = boston_model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(boston_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 595.98843, val loss: 566.95111\n",
      "[2/1000] train loss: 585.96432, val loss: 560.97638\n",
      "[3/1000] train loss: 579.13293, val loss: 553.27539\n",
      "[4/1000] train loss: 569.53342, val loss: 544.35529\n",
      "[5/1000] train loss: 562.20175, val loss: 534.11798\n",
      "[6/1000] train loss: 549.77957, val loss: 522.74512\n",
      "[7/1000] train loss: 533.58051, val loss: 510.18384\n",
      "[8/1000] train loss: 520.68010, val loss: 496.31909\n",
      "[9/1000] train loss: 503.73001, val loss: 481.61789\n",
      "[10/1000] train loss: 485.39366, val loss: 466.17053\n",
      "[11/1000] train loss: 469.57910, val loss: 449.87064\n",
      "[12/1000] train loss: 451.25645, val loss: 433.05493\n",
      "[13/1000] train loss: 431.95697, val loss: 415.92972\n",
      "[14/1000] train loss: 413.01254, val loss: 398.61359\n",
      "[15/1000] train loss: 390.09718, val loss: 381.34540\n",
      "[16/1000] train loss: 372.66281, val loss: 364.19467\n",
      "[17/1000] train loss: 351.85046, val loss: 347.31561\n",
      "[18/1000] train loss: 334.90665, val loss: 330.93991\n",
      "[19/1000] train loss: 316.14717, val loss: 315.11990\n",
      "[20/1000] train loss: 295.27005, val loss: 300.12695\n",
      "[21/1000] train loss: 280.71286, val loss: 285.70914\n",
      "[22/1000] train loss: 261.22734, val loss: 272.28384\n",
      "[23/1000] train loss: 249.56560, val loss: 259.46448\n",
      "[24/1000] train loss: 235.46650, val loss: 247.55972\n",
      "[25/1000] train loss: 223.12869, val loss: 236.45995\n",
      "[26/1000] train loss: 209.98835, val loss: 226.22871\n",
      "[27/1000] train loss: 198.32277, val loss: 216.74371\n",
      "[28/1000] train loss: 189.14356, val loss: 207.96642\n",
      "[29/1000] train loss: 179.31109, val loss: 199.85800\n",
      "[30/1000] train loss: 168.03806, val loss: 192.47339\n",
      "[31/1000] train loss: 163.57765, val loss: 185.52785\n",
      "[32/1000] train loss: 154.89570, val loss: 179.15692\n",
      "[33/1000] train loss: 148.64415, val loss: 173.22229\n",
      "[34/1000] train loss: 142.32336, val loss: 167.64316\n",
      "[35/1000] train loss: 136.69334, val loss: 162.43053\n",
      "[36/1000] train loss: 128.03063, val loss: 157.55020\n",
      "[37/1000] train loss: 125.77913, val loss: 152.95477\n",
      "[38/1000] train loss: 121.63189, val loss: 148.60201\n",
      "[39/1000] train loss: 116.63834, val loss: 144.50818\n",
      "[40/1000] train loss: 113.04422, val loss: 140.61369\n",
      "[41/1000] train loss: 108.60038, val loss: 136.91614\n",
      "[42/1000] train loss: 105.25777, val loss: 133.41927\n",
      "[43/1000] train loss: 102.01999, val loss: 130.06413\n",
      "[44/1000] train loss: 98.79583, val loss: 126.83259\n",
      "[45/1000] train loss: 96.06041, val loss: 123.74035\n",
      "[46/1000] train loss: 93.21969, val loss: 120.77589\n",
      "[47/1000] train loss: 90.07495, val loss: 117.95766\n",
      "[48/1000] train loss: 87.21862, val loss: 115.23959\n",
      "[49/1000] train loss: 84.70315, val loss: 112.63427\n",
      "[50/1000] train loss: 80.83907, val loss: 110.17773\n",
      "[51/1000] train loss: 80.12934, val loss: 107.78100\n",
      "[52/1000] train loss: 78.05824, val loss: 105.46439\n",
      "[53/1000] train loss: 73.33895, val loss: 103.27727\n",
      "[54/1000] train loss: 73.32412, val loss: 101.15025\n",
      "[55/1000] train loss: 70.88184, val loss: 99.11111\n",
      "[56/1000] train loss: 69.84229, val loss: 97.12695\n",
      "[57/1000] train loss: 67.94035, val loss: 95.22123\n",
      "[58/1000] train loss: 64.33097, val loss: 93.46998\n",
      "[59/1000] train loss: 64.35643, val loss: 91.71030\n",
      "[60/1000] train loss: 62.35460, val loss: 90.01525\n",
      "[61/1000] train loss: 61.07262, val loss: 88.35549\n",
      "[62/1000] train loss: 59.49357, val loss: 86.75928\n",
      "[63/1000] train loss: 58.08808, val loss: 85.24097\n",
      "[64/1000] train loss: 56.88383, val loss: 83.76659\n",
      "[65/1000] train loss: 55.37637, val loss: 82.35724\n",
      "[66/1000] train loss: 53.91171, val loss: 81.00085\n",
      "[67/1000] train loss: 53.01351, val loss: 79.70312\n",
      "[68/1000] train loss: 51.97792, val loss: 78.40405\n",
      "[69/1000] train loss: 50.14256, val loss: 77.18745\n",
      "[70/1000] train loss: 49.55259, val loss: 75.98668\n",
      "[71/1000] train loss: 48.04534, val loss: 74.85302\n",
      "[72/1000] train loss: 47.31319, val loss: 73.76340\n",
      "[73/1000] train loss: 46.42617, val loss: 72.70619\n",
      "[74/1000] train loss: 45.27574, val loss: 71.70005\n",
      "[75/1000] train loss: 44.50299, val loss: 70.72736\n",
      "[76/1000] train loss: 43.55596, val loss: 69.79044\n",
      "[77/1000] train loss: 42.70561, val loss: 68.88986\n",
      "[78/1000] train loss: 41.94957, val loss: 68.01525\n",
      "[79/1000] train loss: 41.24657, val loss: 67.16758\n",
      "[80/1000] train loss: 40.13304, val loss: 66.37300\n",
      "[81/1000] train loss: 39.78792, val loss: 65.58350\n",
      "[82/1000] train loss: 39.07205, val loss: 64.83463\n",
      "[83/1000] train loss: 38.26011, val loss: 64.11399\n",
      "[84/1000] train loss: 37.51412, val loss: 63.40563\n",
      "[85/1000] train loss: 37.12196, val loss: 62.72123\n",
      "[86/1000] train loss: 36.18683, val loss: 62.09061\n",
      "[87/1000] train loss: 36.00039, val loss: 61.46177\n",
      "[88/1000] train loss: 34.76400, val loss: 60.85500\n",
      "[89/1000] train loss: 34.35554, val loss: 60.28533\n",
      "[90/1000] train loss: 33.83250, val loss: 59.70061\n",
      "[91/1000] train loss: 33.74870, val loss: 59.16160\n",
      "[92/1000] train loss: 33.28968, val loss: 58.65005\n",
      "[93/1000] train loss: 32.02001, val loss: 58.16571\n",
      "[94/1000] train loss: 31.93417, val loss: 57.69197\n",
      "[95/1000] train loss: 31.74034, val loss: 57.22093\n",
      "[96/1000] train loss: 28.33834, val loss: 56.88942\n",
      "[97/1000] train loss: 28.37350, val loss: 56.48618\n",
      "[98/1000] train loss: 30.90816, val loss: 56.05665\n",
      "[99/1000] train loss: 30.49472, val loss: 55.64630\n",
      "[100/1000] train loss: 29.90863, val loss: 55.23542\n",
      "[101/1000] train loss: 29.43660, val loss: 54.83997\n",
      "[102/1000] train loss: 29.53971, val loss: 54.46818\n",
      "[103/1000] train loss: 29.29355, val loss: 54.07627\n",
      "[104/1000] train loss: 28.68110, val loss: 53.71992\n",
      "[105/1000] train loss: 28.62515, val loss: 53.35124\n",
      "[106/1000] train loss: 28.02427, val loss: 52.99057\n",
      "[107/1000] train loss: 28.10257, val loss: 52.66052\n",
      "[108/1000] train loss: 27.88821, val loss: 52.33552\n",
      "[109/1000] train loss: 26.09681, val loss: 52.03765\n",
      "[110/1000] train loss: 27.23519, val loss: 51.69346\n",
      "[111/1000] train loss: 27.13517, val loss: 51.40547\n",
      "[112/1000] train loss: 26.72765, val loss: 51.12832\n",
      "[113/1000] train loss: 26.74553, val loss: 50.84747\n",
      "[114/1000] train loss: 26.17802, val loss: 50.59107\n",
      "[115/1000] train loss: 26.29081, val loss: 50.31304\n",
      "[116/1000] train loss: 25.87452, val loss: 50.05449\n",
      "[117/1000] train loss: 25.78985, val loss: 49.79388\n",
      "[118/1000] train loss: 25.49863, val loss: 49.53991\n",
      "[119/1000] train loss: 25.33762, val loss: 49.29501\n",
      "[120/1000] train loss: 25.33908, val loss: 49.04326\n",
      "[121/1000] train loss: 22.39444, val loss: 48.87146\n",
      "[122/1000] train loss: 24.89523, val loss: 48.62739\n",
      "[123/1000] train loss: 24.56524, val loss: 48.40253\n",
      "[124/1000] train loss: 24.60002, val loss: 48.16545\n",
      "[125/1000] train loss: 23.36557, val loss: 47.96972\n",
      "[126/1000] train loss: 24.07763, val loss: 47.73381\n",
      "[127/1000] train loss: 23.97946, val loss: 47.50198\n",
      "[128/1000] train loss: 23.82019, val loss: 47.26236\n",
      "[129/1000] train loss: 23.08611, val loss: 47.04432\n",
      "[130/1000] train loss: 23.59368, val loss: 46.82973\n",
      "[131/1000] train loss: 22.59799, val loss: 46.67456\n",
      "[132/1000] train loss: 23.31988, val loss: 46.45052\n",
      "[133/1000] train loss: 23.15400, val loss: 46.23817\n",
      "[134/1000] train loss: 22.81597, val loss: 46.04225\n",
      "[135/1000] train loss: 22.92696, val loss: 45.81889\n",
      "[136/1000] train loss: 22.64487, val loss: 45.60245\n",
      "[137/1000] train loss: 22.64314, val loss: 45.37920\n",
      "[138/1000] train loss: 21.95173, val loss: 45.19345\n",
      "[139/1000] train loss: 22.27638, val loss: 44.99766\n",
      "[140/1000] train loss: 22.24302, val loss: 44.80241\n",
      "[141/1000] train loss: 21.80150, val loss: 44.64021\n",
      "[142/1000] train loss: 22.03192, val loss: 44.43887\n",
      "[143/1000] train loss: 21.77515, val loss: 44.22301\n",
      "[144/1000] train loss: 21.82159, val loss: 44.02344\n",
      "[145/1000] train loss: 21.59202, val loss: 43.82837\n",
      "[146/1000] train loss: 21.37615, val loss: 43.62482\n",
      "[147/1000] train loss: 21.35998, val loss: 43.44145\n",
      "[148/1000] train loss: 21.25504, val loss: 43.26266\n",
      "[149/1000] train loss: 21.15017, val loss: 43.07647\n",
      "[150/1000] train loss: 20.95701, val loss: 42.88164\n",
      "[151/1000] train loss: 20.84127, val loss: 42.67889\n",
      "[152/1000] train loss: 20.79645, val loss: 42.51418\n",
      "[153/1000] train loss: 20.51120, val loss: 42.35125\n",
      "[154/1000] train loss: 20.56930, val loss: 42.17717\n",
      "[155/1000] train loss: 20.42420, val loss: 41.97286\n",
      "[156/1000] train loss: 20.19376, val loss: 41.83424\n",
      "[157/1000] train loss: 20.15735, val loss: 41.59165\n",
      "[158/1000] train loss: 20.06285, val loss: 41.39346\n",
      "[159/1000] train loss: 19.93664, val loss: 41.24317\n",
      "[160/1000] train loss: 19.71301, val loss: 41.09446\n",
      "[161/1000] train loss: 19.76839, val loss: 40.91039\n",
      "[162/1000] train loss: 19.49280, val loss: 40.76814\n",
      "[163/1000] train loss: 19.39363, val loss: 40.56345\n",
      "[164/1000] train loss: 19.45180, val loss: 40.41180\n",
      "[165/1000] train loss: 19.34054, val loss: 40.20798\n",
      "[166/1000] train loss: 19.21933, val loss: 40.06890\n",
      "[167/1000] train loss: 19.00105, val loss: 39.93759\n",
      "[168/1000] train loss: 19.03089, val loss: 39.76101\n",
      "[169/1000] train loss: 18.82524, val loss: 39.62062\n",
      "[170/1000] train loss: 18.58249, val loss: 39.43920\n",
      "[171/1000] train loss: 18.42135, val loss: 39.27053\n",
      "[172/1000] train loss: 18.53383, val loss: 39.08666\n",
      "[173/1000] train loss: 18.37979, val loss: 38.93183\n",
      "[174/1000] train loss: 16.31211, val loss: 38.93887\n",
      "[175/1000] train loss: 18.13306, val loss: 38.73112\n",
      "[176/1000] train loss: 18.11750, val loss: 38.57729\n",
      "[177/1000] train loss: 18.05617, val loss: 38.42842\n",
      "[178/1000] train loss: 17.93778, val loss: 38.24820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179/1000] train loss: 15.82525, val loss: 38.15768\n",
      "[180/1000] train loss: 15.77191, val loss: 38.10442\n",
      "[181/1000] train loss: 17.72330, val loss: 37.89149\n",
      "[182/1000] train loss: 17.34617, val loss: 37.75267\n",
      "[183/1000] train loss: 17.48574, val loss: 37.54773\n",
      "[184/1000] train loss: 17.41839, val loss: 37.36388\n",
      "[185/1000] train loss: 17.25684, val loss: 37.23611\n",
      "[186/1000] train loss: 17.05553, val loss: 37.02721\n",
      "[187/1000] train loss: 16.98470, val loss: 36.86789\n",
      "[188/1000] train loss: 16.66148, val loss: 36.60552\n",
      "[189/1000] train loss: 16.84660, val loss: 36.39700\n",
      "[190/1000] train loss: 16.63506, val loss: 36.25280\n",
      "[191/1000] train loss: 16.76292, val loss: 36.10757\n",
      "[192/1000] train loss: 16.69900, val loss: 35.91133\n",
      "[193/1000] train loss: 16.49074, val loss: 35.77885\n",
      "[194/1000] train loss: 16.35272, val loss: 35.66690\n",
      "[195/1000] train loss: 16.40522, val loss: 35.50103\n",
      "[196/1000] train loss: 16.26142, val loss: 35.35899\n",
      "[197/1000] train loss: 16.10258, val loss: 35.23485\n",
      "[198/1000] train loss: 15.81795, val loss: 35.05057\n",
      "[199/1000] train loss: 15.91405, val loss: 34.92786\n",
      "[200/1000] train loss: 15.91024, val loss: 34.73862\n",
      "[201/1000] train loss: 15.76436, val loss: 34.58326\n",
      "[202/1000] train loss: 15.72237, val loss: 34.42885\n",
      "[203/1000] train loss: 15.41993, val loss: 34.33546\n",
      "[204/1000] train loss: 15.52083, val loss: 34.17593\n",
      "[205/1000] train loss: 15.46128, val loss: 34.07491\n",
      "[206/1000] train loss: 15.25671, val loss: 33.93153\n",
      "[207/1000] train loss: 14.90740, val loss: 33.83921\n",
      "[208/1000] train loss: 15.24086, val loss: 33.69336\n",
      "[209/1000] train loss: 15.00228, val loss: 33.58801\n",
      "[210/1000] train loss: 15.04368, val loss: 33.42559\n",
      "[211/1000] train loss: 14.99229, val loss: 33.27293\n",
      "[212/1000] train loss: 14.85217, val loss: 33.12112\n",
      "[213/1000] train loss: 14.70611, val loss: 32.94031\n",
      "[214/1000] train loss: 14.77711, val loss: 32.79623\n",
      "[215/1000] train loss: 14.63356, val loss: 32.67551\n",
      "[216/1000] train loss: 14.45760, val loss: 32.56873\n",
      "[217/1000] train loss: 14.41726, val loss: 32.43224\n",
      "[218/1000] train loss: 14.39508, val loss: 32.33489\n",
      "[219/1000] train loss: 14.36264, val loss: 32.21963\n",
      "[220/1000] train loss: 13.13043, val loss: 32.03118\n",
      "[221/1000] train loss: 14.21217, val loss: 31.88804\n",
      "[222/1000] train loss: 14.13535, val loss: 31.73421\n",
      "[223/1000] train loss: 13.98956, val loss: 31.68440\n",
      "[224/1000] train loss: 13.93655, val loss: 31.50255\n",
      "[225/1000] train loss: 13.88980, val loss: 31.36912\n",
      "[226/1000] train loss: 13.80367, val loss: 31.21749\n",
      "[227/1000] train loss: 13.53329, val loss: 31.18565\n",
      "[228/1000] train loss: 13.62178, val loss: 31.07645\n",
      "[229/1000] train loss: 13.64359, val loss: 30.98009\n",
      "[230/1000] train loss: 13.59109, val loss: 30.89023\n",
      "[231/1000] train loss: 13.41512, val loss: 30.69064\n",
      "[232/1000] train loss: 13.44000, val loss: 30.48245\n",
      "[233/1000] train loss: 13.36641, val loss: 30.42634\n",
      "[234/1000] train loss: 13.30373, val loss: 30.28432\n",
      "[235/1000] train loss: 13.11841, val loss: 30.21382\n",
      "[236/1000] train loss: 13.15158, val loss: 30.12666\n",
      "[237/1000] train loss: 13.04469, val loss: 30.01449\n",
      "[238/1000] train loss: 12.95898, val loss: 29.87455\n",
      "[239/1000] train loss: 12.83148, val loss: 29.77220\n",
      "[240/1000] train loss: 12.86554, val loss: 29.68456\n",
      "[241/1000] train loss: 12.81067, val loss: 29.59014\n",
      "[242/1000] train loss: 12.75352, val loss: 29.46805\n",
      "[243/1000] train loss: 12.63586, val loss: 29.39883\n",
      "[244/1000] train loss: 12.60106, val loss: 29.24342\n",
      "[245/1000] train loss: 12.57588, val loss: 29.20671\n",
      "[246/1000] train loss: 12.52409, val loss: 29.13088\n",
      "[247/1000] train loss: 12.43171, val loss: 28.95954\n",
      "[248/1000] train loss: 12.32658, val loss: 28.86433\n",
      "[249/1000] train loss: 12.36735, val loss: 28.71020\n",
      "[250/1000] train loss: 12.18549, val loss: 28.59171\n",
      "[251/1000] train loss: 12.10147, val loss: 28.54326\n",
      "[252/1000] train loss: 12.09434, val loss: 28.46288\n",
      "[253/1000] train loss: 11.97324, val loss: 28.33334\n",
      "[254/1000] train loss: 11.94889, val loss: 28.25932\n",
      "[255/1000] train loss: 10.87388, val loss: 28.02065\n",
      "[256/1000] train loss: 10.92147, val loss: 27.79758\n",
      "[257/1000] train loss: 11.90904, val loss: 27.85331\n",
      "[258/1000] train loss: 11.82591, val loss: 27.82214\n",
      "[259/1000] train loss: 11.65723, val loss: 27.79890\n",
      "[260/1000] train loss: 11.42967, val loss: 27.71996\n",
      "[261/1000] train loss: 11.42588, val loss: 27.59440\n",
      "[262/1000] train loss: 11.63245, val loss: 27.56454\n",
      "[263/1000] train loss: 11.22443, val loss: 27.48722\n",
      "[264/1000] train loss: 11.30537, val loss: 27.35195\n",
      "[265/1000] train loss: 11.28954, val loss: 27.28196\n",
      "[266/1000] train loss: 11.36772, val loss: 27.13618\n",
      "[267/1000] train loss: 11.32508, val loss: 27.15167\n",
      "[268/1000] train loss: 11.23158, val loss: 27.03353\n",
      "[269/1000] train loss: 11.13563, val loss: 27.00384\n",
      "[270/1000] train loss: 11.05085, val loss: 26.93218\n",
      "[271/1000] train loss: 11.14456, val loss: 26.87433\n",
      "[272/1000] train loss: 11.07223, val loss: 26.81723\n",
      "[273/1000] train loss: 10.83050, val loss: 26.68668\n",
      "[274/1000] train loss: 11.00511, val loss: 26.56734\n",
      "[275/1000] train loss: 10.92952, val loss: 26.56787\n",
      "[276/1000] train loss: 10.68265, val loss: 26.50537\n",
      "[277/1000] train loss: 10.89484, val loss: 26.42452\n",
      "[278/1000] train loss: 10.73575, val loss: 26.31888\n",
      "[279/1000] train loss: 10.73743, val loss: 26.20589\n",
      "[280/1000] train loss: 10.69401, val loss: 26.19971\n",
      "[281/1000] train loss: 10.67587, val loss: 26.14287\n",
      "[282/1000] train loss: 10.58095, val loss: 26.04659\n",
      "[283/1000] train loss: 10.56066, val loss: 26.02695\n",
      "[284/1000] train loss: 10.48463, val loss: 25.95580\n",
      "[285/1000] train loss: 10.52500, val loss: 25.96293\n",
      "[286/1000] train loss: 10.37742, val loss: 25.94901\n",
      "[287/1000] train loss: 10.43826, val loss: 25.80091\n",
      "[288/1000] train loss: 10.36741, val loss: 25.77031\n",
      "[289/1000] train loss: 10.22455, val loss: 25.61591\n",
      "[290/1000] train loss: 10.30657, val loss: 25.52067\n",
      "[291/1000] train loss: 10.12959, val loss: 25.59430\n",
      "[292/1000] train loss: 10.25030, val loss: 25.44268\n",
      "[293/1000] train loss: 10.14108, val loss: 25.37111\n",
      "[294/1000] train loss: 10.19769, val loss: 25.45012\n",
      "[295/1000] train loss: 10.06060, val loss: 25.38258\n",
      "[296/1000] train loss: 10.04199, val loss: 25.23495\n",
      "[297/1000] train loss: 10.04554, val loss: 25.25813\n",
      "[298/1000] train loss: 8.64709, val loss: 25.39829\n",
      "[299/1000] train loss: 9.99654, val loss: 25.12274\n",
      "[300/1000] train loss: 9.74335, val loss: 24.98199\n",
      "[301/1000] train loss: 9.68316, val loss: 24.88665\n",
      "[302/1000] train loss: 9.74265, val loss: 24.77333\n",
      "[303/1000] train loss: 9.75635, val loss: 24.78770\n",
      "[304/1000] train loss: 9.82007, val loss: 24.58016\n",
      "[305/1000] train loss: 9.70946, val loss: 24.61397\n",
      "[306/1000] train loss: 9.58273, val loss: 24.60068\n",
      "[307/1000] train loss: 9.75249, val loss: 24.67313\n",
      "[308/1000] train loss: 9.61490, val loss: 24.53862\n",
      "[309/1000] train loss: 9.59222, val loss: 24.42356\n",
      "[310/1000] train loss: 9.60006, val loss: 24.39835\n",
      "[311/1000] train loss: 9.51996, val loss: 24.24418\n",
      "[312/1000] train loss: 9.50478, val loss: 24.17026\n",
      "[313/1000] train loss: 9.65864, val loss: 24.22677\n",
      "[314/1000] train loss: 9.48047, val loss: 24.13411\n",
      "[315/1000] train loss: 9.31147, val loss: 24.07666\n",
      "[316/1000] train loss: 9.37069, val loss: 24.14076\n",
      "[317/1000] train loss: 9.33000, val loss: 24.05618\n",
      "[318/1000] train loss: 9.32923, val loss: 23.99743\n",
      "[319/1000] train loss: 9.26509, val loss: 23.95517\n",
      "[320/1000] train loss: 9.19843, val loss: 23.92055\n",
      "[321/1000] train loss: 9.05595, val loss: 23.81423\n",
      "[322/1000] train loss: 9.08005, val loss: 23.78051\n",
      "[323/1000] train loss: 9.18020, val loss: 23.74916\n",
      "[324/1000] train loss: 9.11510, val loss: 23.62668\n",
      "[325/1000] train loss: 9.15113, val loss: 23.77468\n",
      "[326/1000] train loss: 9.16090, val loss: 23.61061\n",
      "[327/1000] train loss: 9.08095, val loss: 23.42966\n",
      "[328/1000] train loss: 8.93339, val loss: 23.38289\n",
      "[329/1000] train loss: 8.99498, val loss: 23.49488\n",
      "[330/1000] train loss: 8.97149, val loss: 23.42223\n",
      "[331/1000] train loss: 8.91438, val loss: 23.49167\n",
      "[332/1000] train loss: 8.89946, val loss: 23.35124\n",
      "[333/1000] train loss: 8.86839, val loss: 23.29478\n",
      "[334/1000] train loss: 8.86154, val loss: 23.23743\n",
      "[335/1000] train loss: 8.63759, val loss: 23.13924\n",
      "[336/1000] train loss: 8.83533, val loss: 23.15473\n",
      "[337/1000] train loss: 8.65414, val loss: 23.13648\n",
      "[338/1000] train loss: 8.88798, val loss: 23.13228\n",
      "[339/1000] train loss: 8.70338, val loss: 23.01290\n",
      "[340/1000] train loss: 8.63297, val loss: 23.20547\n",
      "[341/1000] train loss: 8.74582, val loss: 23.11333\n",
      "[342/1000] train loss: 8.64779, val loss: 23.04874\n",
      "[343/1000] train loss: 8.69156, val loss: 22.95648\n",
      "[344/1000] train loss: 8.60374, val loss: 22.99339\n",
      "[345/1000] train loss: 8.52688, val loss: 22.95115\n",
      "[346/1000] train loss: 7.82625, val loss: 22.66862\n",
      "[347/1000] train loss: 8.58986, val loss: 22.81741\n",
      "[348/1000] train loss: 8.55661, val loss: 22.80990\n",
      "[349/1000] train loss: 8.52793, val loss: 22.81665\n",
      "[350/1000] train loss: 8.50408, val loss: 22.81797\n",
      "[351/1000] train loss: 8.38635, val loss: 22.84435\n",
      "[352/1000] train loss: 8.42439, val loss: 22.65683\n",
      "[353/1000] train loss: 8.32316, val loss: 22.56448\n",
      "[354/1000] train loss: 8.44242, val loss: 22.44322\n",
      "[355/1000] train loss: 8.31036, val loss: 22.54186\n",
      "[356/1000] train loss: 8.38319, val loss: 22.34784\n",
      "[357/1000] train loss: 8.31166, val loss: 22.37276\n",
      "[358/1000] train loss: 8.24584, val loss: 22.31174\n",
      "[359/1000] train loss: 8.21081, val loss: 22.35802\n",
      "[360/1000] train loss: 8.19991, val loss: 22.36004\n",
      "[361/1000] train loss: 8.08965, val loss: 22.37438\n",
      "[362/1000] train loss: 8.28503, val loss: 22.13612\n",
      "[363/1000] train loss: 8.24330, val loss: 22.43588\n",
      "[364/1000] train loss: 8.16088, val loss: 22.38532\n",
      "[365/1000] train loss: 8.13207, val loss: 22.26775\n",
      "[366/1000] train loss: 8.09935, val loss: 22.27973\n",
      "[367/1000] train loss: 8.05360, val loss: 22.15418\n",
      "[368/1000] train loss: 8.16742, val loss: 22.27030\n",
      "[369/1000] train loss: 8.06328, val loss: 22.33792\n",
      "[370/1000] train loss: 8.01634, val loss: 22.18406\n",
      "[371/1000] train loss: 7.98239, val loss: 22.05412\n",
      "[372/1000] train loss: 8.02353, val loss: 22.21893\n",
      "[373/1000] train loss: 7.78118, val loss: 22.06448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[374/1000] train loss: 7.95827, val loss: 22.04703\n",
      "[375/1000] train loss: 8.02470, val loss: 22.09136\n",
      "[376/1000] train loss: 8.01566, val loss: 22.11902\n",
      "[377/1000] train loss: 7.18320, val loss: 21.82494\n",
      "[378/1000] train loss: 7.83867, val loss: 21.74149\n",
      "[379/1000] train loss: 7.72456, val loss: 21.77267\n",
      "[380/1000] train loss: 7.64978, val loss: 21.78260\n",
      "[381/1000] train loss: 7.74826, val loss: 21.73410\n",
      "[382/1000] train loss: 7.85620, val loss: 21.58802\n",
      "[383/1000] train loss: 7.79765, val loss: 21.65141\n",
      "[384/1000] train loss: 7.76491, val loss: 21.77954\n",
      "[385/1000] train loss: 7.70414, val loss: 21.69217\n",
      "[386/1000] train loss: 7.77459, val loss: 21.56166\n",
      "[387/1000] train loss: 7.83761, val loss: 21.54135\n",
      "[388/1000] train loss: 7.54037, val loss: 21.78086\n",
      "[389/1000] train loss: 7.74448, val loss: 21.57654\n",
      "[390/1000] train loss: 7.68789, val loss: 21.53693\n",
      "[391/1000] train loss: 7.68191, val loss: 21.59384\n",
      "[392/1000] train loss: 7.56543, val loss: 21.60981\n",
      "[393/1000] train loss: 7.61920, val loss: 21.51904\n",
      "[394/1000] train loss: 7.55677, val loss: 21.48983\n",
      "[395/1000] train loss: 7.62087, val loss: 21.57358\n",
      "[396/1000] train loss: 7.71364, val loss: 21.30735\n",
      "[397/1000] train loss: 7.57019, val loss: 21.37046\n",
      "[398/1000] train loss: 7.59334, val loss: 21.45402\n",
      "[399/1000] train loss: 7.56679, val loss: 21.42523\n",
      "[400/1000] train loss: 7.69861, val loss: 21.15561\n",
      "[401/1000] train loss: 7.48693, val loss: 21.32860\n",
      "[402/1000] train loss: 7.51127, val loss: 21.41430\n",
      "[403/1000] train loss: 7.49598, val loss: 21.48218\n",
      "[404/1000] train loss: 7.53870, val loss: 21.44770\n",
      "[405/1000] train loss: 7.48060, val loss: 21.55606\n",
      "[406/1000] train loss: 7.64111, val loss: 21.40367\n",
      "[407/1000] train loss: 7.42792, val loss: 21.43752\n",
      "[408/1000] train loss: 7.44197, val loss: 21.30406\n",
      "[409/1000] train loss: 7.41237, val loss: 21.20461\n",
      "[410/1000] train loss: 7.34026, val loss: 21.29030\n",
      "[411/1000] train loss: 7.25493, val loss: 21.24365\n",
      "[412/1000] train loss: 7.33536, val loss: 21.14513\n",
      "[413/1000] train loss: 7.38545, val loss: 21.35231\n",
      "[414/1000] train loss: 7.33930, val loss: 21.12709\n",
      "[415/1000] train loss: 7.35609, val loss: 21.00461\n",
      "[416/1000] train loss: 7.09548, val loss: 21.14566\n",
      "[417/1000] train loss: 7.28971, val loss: 21.13085\n",
      "[418/1000] train loss: 7.28273, val loss: 21.12155\n",
      "[419/1000] train loss: 7.25717, val loss: 21.09723\n",
      "[420/1000] train loss: 7.23518, val loss: 21.08840\n",
      "[421/1000] train loss: 7.26492, val loss: 21.12391\n",
      "[422/1000] train loss: 7.20718, val loss: 21.07808\n",
      "[423/1000] train loss: 7.20238, val loss: 20.95000\n",
      "[424/1000] train loss: 7.23525, val loss: 20.86898\n",
      "[425/1000] train loss: 7.18143, val loss: 21.07493\n",
      "[426/1000] train loss: 7.14624, val loss: 20.91517\n",
      "[427/1000] train loss: 7.05219, val loss: 20.97083\n",
      "[428/1000] train loss: 7.11384, val loss: 21.01353\n",
      "[429/1000] train loss: 7.24940, val loss: 21.11238\n",
      "[430/1000] train loss: 7.08737, val loss: 20.83715\n",
      "[431/1000] train loss: 7.19179, val loss: 20.75660\n",
      "[432/1000] train loss: 7.07805, val loss: 20.79005\n",
      "[433/1000] train loss: 7.16737, val loss: 20.98807\n",
      "[434/1000] train loss: 7.08132, val loss: 20.77421\n",
      "[435/1000] train loss: 7.00534, val loss: 20.80550\n",
      "[436/1000] train loss: 7.01505, val loss: 20.81379\n",
      "[437/1000] train loss: 7.06402, val loss: 20.73720\n",
      "[438/1000] train loss: 7.10037, val loss: 20.64588\n",
      "[439/1000] train loss: 7.02160, val loss: 20.76959\n",
      "[440/1000] train loss: 7.02387, val loss: 20.76504\n",
      "[441/1000] train loss: 7.03102, val loss: 20.76777\n",
      "[442/1000] train loss: 6.51875, val loss: 20.47208\n",
      "[443/1000] train loss: 7.02110, val loss: 20.64077\n",
      "[444/1000] train loss: 6.99954, val loss: 20.64234\n",
      "[445/1000] train loss: 6.94487, val loss: 20.55015\n",
      "[446/1000] train loss: 7.01414, val loss: 20.55992\n",
      "[447/1000] train loss: 6.75307, val loss: 20.63626\n",
      "[448/1000] train loss: 6.87981, val loss: 20.56089\n",
      "[449/1000] train loss: 6.92718, val loss: 20.49592\n",
      "[450/1000] train loss: 6.94861, val loss: 20.53025\n",
      "[451/1000] train loss: 6.93035, val loss: 20.43823\n",
      "[452/1000] train loss: 7.03315, val loss: 20.63416\n",
      "[453/1000] train loss: 6.99449, val loss: 20.61986\n",
      "[454/1000] train loss: 6.86362, val loss: 20.44907\n",
      "[455/1000] train loss: 6.82414, val loss: 20.51040\n",
      "[456/1000] train loss: 6.79825, val loss: 20.43882\n",
      "[457/1000] train loss: 6.92574, val loss: 20.49215\n",
      "[458/1000] train loss: 6.89498, val loss: 20.51998\n",
      "[459/1000] train loss: 6.79517, val loss: 20.37685\n",
      "[460/1000] train loss: 6.76766, val loss: 20.27363\n",
      "[461/1000] train loss: 6.80627, val loss: 20.43298\n",
      "[462/1000] train loss: 6.75750, val loss: 20.22869\n",
      "[463/1000] train loss: 6.80981, val loss: 20.25286\n",
      "[464/1000] train loss: 6.82572, val loss: 20.43197\n",
      "[465/1000] train loss: 6.79206, val loss: 20.22597\n",
      "[466/1000] train loss: 6.76024, val loss: 20.24675\n",
      "[467/1000] train loss: 6.76672, val loss: 20.38691\n",
      "[468/1000] train loss: 6.78471, val loss: 20.32384\n",
      "[469/1000] train loss: 6.86377, val loss: 20.27118\n",
      "[470/1000] train loss: 6.70049, val loss: 20.14027\n",
      "[471/1000] train loss: 6.78137, val loss: 20.31655\n",
      "[472/1000] train loss: 6.84324, val loss: 20.21227\n",
      "[473/1000] train loss: 6.73562, val loss: 20.09416\n",
      "[474/1000] train loss: 6.63166, val loss: 20.33137\n",
      "[475/1000] train loss: 6.74989, val loss: 20.32349\n",
      "[476/1000] train loss: 6.68687, val loss: 20.25599\n",
      "[477/1000] train loss: 6.70586, val loss: 20.05177\n",
      "[478/1000] train loss: 6.82327, val loss: 20.23440\n",
      "[479/1000] train loss: 6.71592, val loss: 20.23970\n",
      "[480/1000] train loss: 6.59615, val loss: 20.08909\n",
      "[481/1000] train loss: 6.61093, val loss: 19.95612\n",
      "[482/1000] train loss: 6.66528, val loss: 19.95542\n",
      "[483/1000] train loss: 6.62314, val loss: 20.08634\n",
      "[484/1000] train loss: 6.55417, val loss: 19.93283\n",
      "[485/1000] train loss: 6.62142, val loss: 19.99729\n",
      "[486/1000] train loss: 6.56979, val loss: 19.82521\n",
      "[487/1000] train loss: 6.58903, val loss: 19.86157\n",
      "[488/1000] train loss: 6.50837, val loss: 19.86370\n",
      "[489/1000] train loss: 6.50970, val loss: 19.88726\n",
      "[490/1000] train loss: 6.53927, val loss: 19.91702\n",
      "[491/1000] train loss: 6.34418, val loss: 19.82491\n",
      "[492/1000] train loss: 6.55746, val loss: 19.89089\n",
      "[493/1000] train loss: 5.93224, val loss: 19.67220\n",
      "[494/1000] train loss: 6.48821, val loss: 19.72357\n",
      "[495/1000] train loss: 6.43806, val loss: 19.75524\n",
      "[496/1000] train loss: 6.54858, val loss: 19.74066\n",
      "[497/1000] train loss: 6.45907, val loss: 19.76436\n",
      "[498/1000] train loss: 6.53805, val loss: 20.06496\n",
      "[499/1000] train loss: 6.49453, val loss: 19.88767\n",
      "[500/1000] train loss: 6.43629, val loss: 19.74544\n",
      "[501/1000] train loss: 6.42178, val loss: 19.59891\n",
      "[502/1000] train loss: 6.47724, val loss: 19.63479\n",
      "[503/1000] train loss: 6.44600, val loss: 19.77695\n",
      "[504/1000] train loss: 6.56720, val loss: 19.79234\n",
      "[505/1000] train loss: 6.36392, val loss: 19.50289\n",
      "[506/1000] train loss: 6.40538, val loss: 19.67219\n",
      "[507/1000] train loss: 6.40463, val loss: 19.63821\n",
      "[508/1000] train loss: 6.45401, val loss: 19.66947\n",
      "[509/1000] train loss: 6.32175, val loss: 19.76291\n",
      "[510/1000] train loss: 6.33684, val loss: 19.80612\n",
      "[511/1000] train loss: 6.38762, val loss: 19.85025\n",
      "[512/1000] train loss: 6.28983, val loss: 19.64897\n",
      "[513/1000] train loss: 6.36154, val loss: 19.80863\n",
      "[514/1000] train loss: 6.41388, val loss: 19.66102\n",
      "[515/1000] train loss: 6.22958, val loss: 19.78220\n",
      "[516/1000] train loss: 6.30729, val loss: 19.71033\n",
      "[517/1000] train loss: 6.23266, val loss: 19.77482\n",
      "[518/1000] train loss: 6.36174, val loss: 19.57683\n",
      "[519/1000] train loss: 6.29811, val loss: 19.57161\n",
      "[520/1000] train loss: 6.30226, val loss: 19.62634\n",
      "[521/1000] train loss: 6.21433, val loss: 19.69353\n",
      "[522/1000] train loss: 6.10968, val loss: 19.59368\n",
      "[523/1000] train loss: 6.10111, val loss: 19.58751\n",
      "[524/1000] train loss: 6.28135, val loss: 19.56599\n",
      "[525/1000] train loss: 6.26620, val loss: 19.57319\n",
      "[526/1000] train loss: 6.24064, val loss: 19.42123\n",
      "[527/1000] train loss: 6.19992, val loss: 19.50504\n",
      "[528/1000] train loss: 6.26010, val loss: 19.40514\n",
      "[529/1000] train loss: 6.08721, val loss: 19.51666\n",
      "[530/1000] train loss: 6.18866, val loss: 19.40606\n",
      "[531/1000] train loss: 6.27372, val loss: 19.66074\n",
      "[532/1000] train loss: 6.36724, val loss: 19.66581\n",
      "[533/1000] train loss: 5.73462, val loss: 19.34340\n",
      "[534/1000] train loss: 6.20701, val loss: 19.44943\n",
      "[535/1000] train loss: 6.06488, val loss: 19.42935\n",
      "[536/1000] train loss: 6.14308, val loss: 19.25260\n",
      "[537/1000] train loss: 6.20356, val loss: 19.31967\n",
      "[538/1000] train loss: 6.11380, val loss: 19.40110\n",
      "[539/1000] train loss: 6.15756, val loss: 19.28223\n",
      "[540/1000] train loss: 6.16171, val loss: 19.42743\n",
      "[541/1000] train loss: 6.11103, val loss: 19.41132\n",
      "[542/1000] train loss: 6.07700, val loss: 19.39110\n",
      "[543/1000] train loss: 6.18423, val loss: 19.36453\n",
      "[544/1000] train loss: 6.07439, val loss: 19.21509\n",
      "[545/1000] train loss: 6.15227, val loss: 19.29349\n",
      "[546/1000] train loss: 6.14738, val loss: 19.41784\n",
      "[547/1000] train loss: 6.06431, val loss: 19.22849\n",
      "[548/1000] train loss: 6.11933, val loss: 19.21842\n",
      "[549/1000] train loss: 6.04039, val loss: 19.32083\n",
      "[550/1000] train loss: 6.06630, val loss: 19.25401\n",
      "[551/1000] train loss: 6.02353, val loss: 19.21498\n",
      "[552/1000] train loss: 6.01163, val loss: 19.20540\n",
      "[553/1000] train loss: 6.01225, val loss: 19.15169\n",
      "[554/1000] train loss: 6.10486, val loss: 19.06063\n",
      "[555/1000] train loss: 5.99515, val loss: 19.13476\n",
      "[556/1000] train loss: 6.00595, val loss: 19.04404\n",
      "[557/1000] train loss: 6.04910, val loss: 19.18117\n",
      "[558/1000] train loss: 6.01640, val loss: 19.07043\n",
      "[559/1000] train loss: 6.05699, val loss: 19.01204\n",
      "[560/1000] train loss: 6.03128, val loss: 19.21646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[561/1000] train loss: 6.09726, val loss: 18.86405\n",
      "[562/1000] train loss: 6.02506, val loss: 19.18291\n",
      "[563/1000] train loss: 5.99416, val loss: 19.04234\n",
      "[564/1000] train loss: 5.95478, val loss: 19.19717\n",
      "[565/1000] train loss: 5.94659, val loss: 19.12503\n",
      "[566/1000] train loss: 5.98617, val loss: 18.94293\n",
      "[567/1000] train loss: 5.93993, val loss: 19.20661\n",
      "[568/1000] train loss: 5.97086, val loss: 19.01809\n",
      "[569/1000] train loss: 5.93331, val loss: 19.13983\n",
      "[570/1000] train loss: 6.00562, val loss: 18.84715\n",
      "[571/1000] train loss: 5.90841, val loss: 19.10770\n",
      "[572/1000] train loss: 5.88611, val loss: 19.14014\n",
      "[573/1000] train loss: 5.89920, val loss: 18.95596\n",
      "[574/1000] train loss: 5.77494, val loss: 19.03388\n",
      "[575/1000] train loss: 5.82245, val loss: 18.94030\n",
      "[576/1000] train loss: 5.91084, val loss: 19.19888\n",
      "[577/1000] train loss: 5.84108, val loss: 18.83499\n",
      "[578/1000] train loss: 5.90164, val loss: 18.84518\n",
      "[579/1000] train loss: 5.89197, val loss: 18.84802\n",
      "[580/1000] train loss: 5.86432, val loss: 18.92811\n",
      "[581/1000] train loss: 6.00537, val loss: 18.75954\n",
      "[582/1000] train loss: 5.78827, val loss: 18.88225\n",
      "[583/1000] train loss: 5.78117, val loss: 18.87008\n",
      "[584/1000] train loss: 5.87895, val loss: 19.04397\n",
      "[585/1000] train loss: 5.86214, val loss: 18.78293\n",
      "[586/1000] train loss: 5.66450, val loss: 19.02872\n",
      "[587/1000] train loss: 5.70155, val loss: 19.02788\n",
      "[588/1000] train loss: 5.91766, val loss: 19.20232\n",
      "[589/1000] train loss: 5.77845, val loss: 18.97907\n",
      "[590/1000] train loss: 5.80815, val loss: 18.93553\n",
      "[591/1000] train loss: 5.78510, val loss: 18.86054\n",
      "[592/1000] train loss: 5.86765, val loss: 18.67793\n",
      "[593/1000] train loss: 5.83153, val loss: 19.03586\n",
      "[594/1000] train loss: 5.75651, val loss: 18.87378\n",
      "[595/1000] train loss: 5.88465, val loss: 18.99450\n",
      "[596/1000] train loss: 5.82078, val loss: 19.06576\n",
      "[597/1000] train loss: 5.77326, val loss: 19.02978\n",
      "[598/1000] train loss: 5.75623, val loss: 18.90237\n",
      "[599/1000] train loss: 5.74720, val loss: 19.04437\n",
      "[600/1000] train loss: 5.94617, val loss: 19.04754\n",
      "[601/1000] train loss: 5.72846, val loss: 18.77955\n",
      "[602/1000] train loss: 5.73416, val loss: 18.87656\n",
      "[603/1000] train loss: 5.38514, val loss: 18.58571\n",
      "[604/1000] train loss: 5.66962, val loss: 18.71550\n",
      "[605/1000] train loss: 5.61272, val loss: 18.77214\n",
      "[606/1000] train loss: 5.79723, val loss: 18.60248\n",
      "[607/1000] train loss: 5.80053, val loss: 18.57325\n",
      "[608/1000] train loss: 5.75564, val loss: 18.63621\n",
      "[609/1000] train loss: 5.76598, val loss: 18.55237\n",
      "[610/1000] train loss: 5.66854, val loss: 18.71466\n",
      "[611/1000] train loss: 5.72196, val loss: 18.77222\n",
      "[612/1000] train loss: 5.70344, val loss: 18.77612\n",
      "[613/1000] train loss: 5.62416, val loss: 18.68504\n",
      "[614/1000] train loss: 5.68150, val loss: 18.66474\n",
      "[615/1000] train loss: 5.65796, val loss: 18.79314\n",
      "[616/1000] train loss: 5.63657, val loss: 18.78230\n",
      "[617/1000] train loss: 5.60397, val loss: 18.76671\n",
      "[618/1000] train loss: 5.63391, val loss: 18.74849\n",
      "[619/1000] train loss: 5.53119, val loss: 18.86241\n",
      "[620/1000] train loss: 5.65355, val loss: 18.53927\n",
      "[621/1000] train loss: 5.64771, val loss: 18.75475\n",
      "[622/1000] train loss: 5.64058, val loss: 18.74010\n",
      "[623/1000] train loss: 5.62142, val loss: 18.45378\n",
      "[624/1000] train loss: 5.58336, val loss: 18.70660\n",
      "[625/1000] train loss: 5.62077, val loss: 18.64913\n",
      "[626/1000] train loss: 5.52704, val loss: 18.66480\n",
      "[627/1000] train loss: 5.58278, val loss: 18.65690\n",
      "[628/1000] train loss: 5.64306, val loss: 18.76887\n",
      "[629/1000] train loss: 5.56825, val loss: 18.71112\n",
      "[630/1000] train loss: 5.62969, val loss: 18.49155\n",
      "[631/1000] train loss: 5.57368, val loss: 18.91346\n",
      "[632/1000] train loss: 5.54713, val loss: 18.56178\n",
      "[633/1000] train loss: 5.52473, val loss: 18.69676\n",
      "[634/1000] train loss: 5.50103, val loss: 18.66337\n",
      "[635/1000] train loss: 5.49494, val loss: 18.70311\n",
      "[636/1000] train loss: 5.51160, val loss: 18.64066\n",
      "[637/1000] train loss: 5.55808, val loss: 18.52353\n",
      "[638/1000] train loss: 5.56109, val loss: 18.50340\n",
      "[639/1000] train loss: 5.49752, val loss: 18.62841\n",
      "[640/1000] train loss: 5.61257, val loss: 18.66271\n",
      "[641/1000] train loss: 5.58482, val loss: 18.72128\n",
      "[642/1000] train loss: 5.54887, val loss: 18.60712\n",
      "[643/1000] train loss: 5.56284, val loss: 18.69694\n",
      "[644/1000] train loss: 5.50364, val loss: 18.65905\n",
      "[645/1000] train loss: 5.48018, val loss: 18.61151\n",
      "[646/1000] train loss: 5.47988, val loss: 18.55721\n",
      "[647/1000] train loss: 5.49859, val loss: 18.66225\n",
      "[648/1000] train loss: 5.50080, val loss: 18.68040\n",
      "[649/1000] train loss: 5.48668, val loss: 18.73176\n",
      "[650/1000] train loss: 5.47938, val loss: 18.61101\n",
      "[651/1000] train loss: 5.40884, val loss: 18.67669\n",
      "[652/1000] train loss: 5.47646, val loss: 18.67841\n",
      "[653/1000] train loss: 5.45678, val loss: 18.61476\n",
      "[654/1000] train loss: 5.39752, val loss: 18.62655\n",
      "[655/1000] train loss: 5.43003, val loss: 18.60298\n",
      "[656/1000] train loss: 5.45346, val loss: 18.56826\n",
      "[657/1000] train loss: 5.47216, val loss: 18.52459\n",
      "[658/1000] train loss: 5.50631, val loss: 18.38007\n",
      "[659/1000] train loss: 5.29838, val loss: 18.47191\n",
      "[660/1000] train loss: 5.45832, val loss: 18.66463\n",
      "[661/1000] train loss: 5.43797, val loss: 18.70318\n",
      "[662/1000] train loss: 5.42823, val loss: 18.64161\n",
      "[663/1000] train loss: 5.42276, val loss: 18.65322\n",
      "[664/1000] train loss: 5.49571, val loss: 18.61163\n",
      "[665/1000] train loss: 5.39679, val loss: 18.50065\n",
      "[666/1000] train loss: 5.46421, val loss: 18.37904\n",
      "[667/1000] train loss: 5.33719, val loss: 18.59621\n",
      "[668/1000] train loss: 5.10825, val loss: 18.51966\n",
      "[669/1000] train loss: 5.38600, val loss: 18.55721\n",
      "[670/1000] train loss: 5.42274, val loss: 18.57016\n",
      "[671/1000] train loss: 5.40583, val loss: 18.73018\n",
      "[672/1000] train loss: 5.23986, val loss: 18.48225\n",
      "[673/1000] train loss: 5.36065, val loss: 18.62271\n",
      "[674/1000] train loss: 5.40943, val loss: 18.63364\n",
      "[675/1000] train loss: 5.08438, val loss: 18.28061\n",
      "[676/1000] train loss: 5.41515, val loss: 18.46226\n",
      "[677/1000] train loss: 5.41557, val loss: 18.38363\n",
      "[678/1000] train loss: 5.45646, val loss: 18.70467\n",
      "[679/1000] train loss: 5.34007, val loss: 18.33265\n",
      "[680/1000] train loss: 5.34710, val loss: 18.40382\n",
      "[681/1000] train loss: 5.23905, val loss: 18.42729\n",
      "[682/1000] train loss: 5.31905, val loss: 18.63710\n",
      "[683/1000] train loss: 5.32636, val loss: 18.37117\n",
      "[684/1000] train loss: 5.36040, val loss: 18.43289\n",
      "[685/1000] train loss: 5.21911, val loss: 18.40249\n",
      "[686/1000] train loss: 5.29557, val loss: 18.40428\n",
      "[687/1000] train loss: 5.32452, val loss: 18.37895\n",
      "[688/1000] train loss: 5.27088, val loss: 18.47075\n",
      "[689/1000] train loss: 5.25873, val loss: 18.51509\n",
      "[690/1000] train loss: 5.36456, val loss: 18.40532\n",
      "[691/1000] train loss: 5.40036, val loss: 18.24203\n",
      "[692/1000] train loss: 5.32734, val loss: 18.60314\n",
      "[693/1000] train loss: 5.25388, val loss: 18.31271\n",
      "[694/1000] train loss: 5.27302, val loss: 18.24990\n",
      "[695/1000] train loss: 5.29824, val loss: 18.31839\n",
      "[696/1000] train loss: 5.28646, val loss: 18.28585\n",
      "[697/1000] train loss: 5.34305, val loss: 18.47043\n",
      "[698/1000] train loss: 5.38250, val loss: 18.58193\n",
      "[699/1000] train loss: 5.31898, val loss: 18.31240\n",
      "[700/1000] train loss: 5.27101, val loss: 18.45073\n",
      "[701/1000] train loss: 5.27152, val loss: 18.44715\n",
      "[702/1000] train loss: 5.29284, val loss: 18.07857\n",
      "[703/1000] train loss: 5.32527, val loss: 18.37184\n",
      "[704/1000] train loss: 5.28811, val loss: 18.16536\n",
      "[705/1000] train loss: 5.24461, val loss: 18.33526\n",
      "[706/1000] train loss: 5.23719, val loss: 18.38638\n",
      "[707/1000] train loss: 5.24998, val loss: 18.03624\n",
      "[708/1000] train loss: 5.26295, val loss: 18.17394\n",
      "[709/1000] train loss: 5.12997, val loss: 18.41737\n",
      "[710/1000] train loss: 5.22929, val loss: 18.34494\n",
      "[711/1000] train loss: 5.33237, val loss: 18.55451\n",
      "[712/1000] train loss: 5.18524, val loss: 18.19353\n",
      "[713/1000] train loss: 5.17594, val loss: 18.27527\n",
      "[714/1000] train loss: 5.17692, val loss: 18.30557\n",
      "[715/1000] train loss: 5.17200, val loss: 18.25848\n",
      "[716/1000] train loss: 5.21156, val loss: 18.30344\n",
      "[717/1000] train loss: 5.20137, val loss: 18.34435\n",
      "[718/1000] train loss: 5.17304, val loss: 18.23013\n",
      "[719/1000] train loss: 5.11968, val loss: 18.34135\n",
      "[720/1000] train loss: 5.22377, val loss: 18.14689\n",
      "[721/1000] train loss: 5.14171, val loss: 18.17096\n",
      "[722/1000] train loss: 5.19690, val loss: 18.21530\n",
      "[723/1000] train loss: 5.06302, val loss: 18.24290\n",
      "[724/1000] train loss: 5.19023, val loss: 18.32846\n",
      "[725/1000] train loss: 5.16048, val loss: 18.24253\n",
      "[726/1000] train loss: 5.15016, val loss: 18.18929\n",
      "[727/1000] train loss: 5.16122, val loss: 18.20455\n",
      "[728/1000] train loss: 5.11520, val loss: 18.10247\n",
      "[729/1000] train loss: 5.16214, val loss: 18.24722\n",
      "[730/1000] train loss: 5.12140, val loss: 18.23683\n",
      "[731/1000] train loss: 5.09339, val loss: 18.17722\n",
      "[732/1000] train loss: 5.10800, val loss: 18.14644\n",
      "[733/1000] train loss: 5.12088, val loss: 18.42216\n",
      "[734/1000] train loss: 5.13334, val loss: 18.08443\n",
      "[735/1000] train loss: 5.14621, val loss: 18.16529\n",
      "[736/1000] train loss: 5.16723, val loss: 18.08292\n",
      "[737/1000] train loss: 5.15749, val loss: 18.14158\n",
      "[738/1000] train loss: 4.86740, val loss: 18.35091\n",
      "[739/1000] train loss: 5.13394, val loss: 18.23344\n",
      "[740/1000] train loss: 5.07493, val loss: 18.13760\n",
      "[741/1000] train loss: 5.10207, val loss: 18.13667\n",
      "[742/1000] train loss: 4.89235, val loss: 18.42189\n",
      "[743/1000] train loss: 5.02841, val loss: 18.43512\n",
      "[744/1000] train loss: 5.06661, val loss: 18.28321\n",
      "[745/1000] train loss: 5.08283, val loss: 18.32205\n",
      "[746/1000] train loss: 5.06958, val loss: 18.22041\n",
      "[747/1000] train loss: 5.05483, val loss: 18.22789\n",
      "[748/1000] train loss: 5.08344, val loss: 18.13193\n",
      "[749/1000] train loss: 5.07282, val loss: 18.13861\n",
      "[750/1000] train loss: 5.10778, val loss: 18.28262\n",
      "[751/1000] train loss: 5.07290, val loss: 18.10501\n",
      "[752/1000] train loss: 5.02018, val loss: 18.15487\n",
      "[753/1000] train loss: 4.97154, val loss: 18.11050\n",
      "[754/1000] train loss: 5.12537, val loss: 18.25157\n",
      "[755/1000] train loss: 5.01457, val loss: 18.11130\n",
      "[756/1000] train loss: 5.09349, val loss: 18.33487\n",
      "[757/1000] train loss: 5.03721, val loss: 18.27619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[758/1000] train loss: 5.07018, val loss: 18.15623\n",
      "[759/1000] train loss: 5.02577, val loss: 18.12817\n",
      "[760/1000] train loss: 5.05304, val loss: 18.21564\n",
      "[761/1000] train loss: 4.99788, val loss: 18.13814\n",
      "[762/1000] train loss: 5.03778, val loss: 18.17868\n",
      "[763/1000] train loss: 5.05916, val loss: 17.97842\n",
      "[764/1000] train loss: 5.02410, val loss: 17.95360\n",
      "[765/1000] train loss: 4.98758, val loss: 18.24920\n",
      "[766/1000] train loss: 5.23430, val loss: 18.53022\n",
      "[767/1000] train loss: 5.03149, val loss: 18.03701\n",
      "[768/1000] train loss: 4.99681, val loss: 18.12316\n",
      "[769/1000] train loss: 4.99950, val loss: 18.08280\n",
      "[770/1000] train loss: 5.03949, val loss: 18.22523\n",
      "[771/1000] train loss: 5.06698, val loss: 18.36231\n",
      "[772/1000] train loss: 5.01230, val loss: 18.02382\n",
      "[773/1000] train loss: 4.96474, val loss: 18.09958\n",
      "[774/1000] train loss: 5.08864, val loss: 17.78070\n",
      "[775/1000] train loss: 5.04510, val loss: 17.83757\n",
      "[776/1000] train loss: 5.04681, val loss: 18.06652\n",
      "[777/1000] train loss: 4.93180, val loss: 18.01834\n",
      "[778/1000] train loss: 4.96981, val loss: 18.27741\n",
      "[779/1000] train loss: 4.87969, val loss: 18.06330\n",
      "[780/1000] train loss: 4.96700, val loss: 17.92847\n",
      "[781/1000] train loss: 4.94471, val loss: 18.28835\n",
      "[782/1000] train loss: 5.00810, val loss: 18.14492\n",
      "[783/1000] train loss: 4.86489, val loss: 17.97454\n",
      "[784/1000] train loss: 4.94082, val loss: 18.10229\n",
      "[785/1000] train loss: 5.02507, val loss: 18.22797\n",
      "[786/1000] train loss: 5.05742, val loss: 17.98758\n",
      "[787/1000] train loss: 4.89748, val loss: 18.04299\n",
      "[788/1000] train loss: 4.88470, val loss: 18.09443\n",
      "[789/1000] train loss: 4.94180, val loss: 17.95267\n",
      "[790/1000] train loss: 4.95205, val loss: 17.99449\n",
      "[791/1000] train loss: 5.03888, val loss: 18.03923\n",
      "[792/1000] train loss: 4.93186, val loss: 17.85554\n",
      "[793/1000] train loss: 4.95175, val loss: 18.02848\n",
      "[794/1000] train loss: 4.90089, val loss: 18.16062\n",
      "[795/1000] train loss: 4.92115, val loss: 17.99868\n",
      "[796/1000] train loss: 4.98878, val loss: 18.19190\n",
      "[797/1000] train loss: 5.06345, val loss: 18.03237\n",
      "[798/1000] train loss: 4.92778, val loss: 18.09587\n",
      "[799/1000] train loss: 4.94761, val loss: 17.80609\n",
      "[800/1000] train loss: 4.89822, val loss: 17.93826\n",
      "[801/1000] train loss: 4.91622, val loss: 17.97079\n",
      "[802/1000] train loss: 4.90635, val loss: 17.92599\n",
      "[803/1000] train loss: 4.89685, val loss: 18.03836\n",
      "[804/1000] train loss: 4.95027, val loss: 17.96454\n",
      "[805/1000] train loss: 4.93330, val loss: 17.78257\n",
      "[806/1000] train loss: 4.94285, val loss: 17.94525\n",
      "[807/1000] train loss: 4.97978, val loss: 18.14749\n",
      "[808/1000] train loss: 4.99221, val loss: 18.20586\n",
      "[809/1000] train loss: 4.67974, val loss: 17.96975\n",
      "[810/1000] train loss: 4.77923, val loss: 18.00659\n",
      "[811/1000] train loss: 4.91227, val loss: 17.89255\n",
      "[812/1000] train loss: 4.90033, val loss: 17.90071\n",
      "[813/1000] train loss: 4.85708, val loss: 18.09594\n",
      "[814/1000] train loss: 4.84568, val loss: 17.76086\n",
      "[815/1000] train loss: 4.83871, val loss: 18.00976\n",
      "[816/1000] train loss: 4.92618, val loss: 17.95177\n",
      "[817/1000] train loss: 4.91926, val loss: 17.86535\n",
      "[818/1000] train loss: 4.76643, val loss: 17.84119\n",
      "[819/1000] train loss: 4.87260, val loss: 17.77047\n",
      "[820/1000] train loss: 4.83806, val loss: 18.00007\n",
      "[821/1000] train loss: 4.81402, val loss: 17.70515\n",
      "[822/1000] train loss: 4.81222, val loss: 17.78890\n",
      "[823/1000] train loss: 4.83775, val loss: 17.99027\n",
      "[824/1000] train loss: 4.84670, val loss: 17.79764\n",
      "[825/1000] train loss: 4.86436, val loss: 18.01852\n",
      "[826/1000] train loss: 4.59827, val loss: 17.67968\n",
      "[827/1000] train loss: 4.74022, val loss: 17.84419\n",
      "[828/1000] train loss: 4.83399, val loss: 17.98763\n",
      "[829/1000] train loss: 4.79098, val loss: 18.06201\n",
      "[830/1000] train loss: 4.81469, val loss: 17.86068\n",
      "[831/1000] train loss: 4.74034, val loss: 17.91255\n",
      "[832/1000] train loss: 4.84942, val loss: 18.00713\n",
      "[833/1000] train loss: 4.83219, val loss: 17.92518\n",
      "[834/1000] train loss: 4.90286, val loss: 17.87925\n",
      "[835/1000] train loss: 4.80018, val loss: 17.83394\n",
      "[836/1000] train loss: 4.76814, val loss: 17.85404\n",
      "[837/1000] train loss: 4.78036, val loss: 17.83658\n",
      "[838/1000] train loss: 4.81219, val loss: 17.73528\n",
      "[839/1000] train loss: 4.81696, val loss: 17.80188\n",
      "[840/1000] train loss: 4.71429, val loss: 17.83692\n",
      "[841/1000] train loss: 4.74838, val loss: 17.58750\n",
      "[842/1000] train loss: 4.77147, val loss: 18.01624\n",
      "[843/1000] train loss: 4.77354, val loss: 17.86095\n",
      "[844/1000] train loss: 4.71431, val loss: 17.82939\n",
      "[845/1000] train loss: 4.81742, val loss: 17.90207\n",
      "[846/1000] train loss: 4.70543, val loss: 17.95346\n",
      "[847/1000] train loss: 4.71334, val loss: 17.90679\n",
      "[848/1000] train loss: 4.75413, val loss: 17.82156\n",
      "[849/1000] train loss: 4.73197, val loss: 17.64366\n",
      "[850/1000] train loss: 4.57304, val loss: 17.88893\n",
      "[851/1000] train loss: 4.75350, val loss: 17.94916\n",
      "[852/1000] train loss: 4.76808, val loss: 17.92765\n",
      "[853/1000] train loss: 4.87114, val loss: 17.75251\n",
      "[854/1000] train loss: 4.78133, val loss: 17.84769\n",
      "[855/1000] train loss: 4.70076, val loss: 18.02187\n",
      "[856/1000] train loss: 4.75677, val loss: 17.80512\n",
      "[857/1000] train loss: 4.70890, val loss: 17.89736\n",
      "[858/1000] train loss: 4.64431, val loss: 17.79055\n",
      "[859/1000] train loss: 4.74048, val loss: 17.67375\n",
      "[860/1000] train loss: 4.66807, val loss: 17.90661\n",
      "[861/1000] train loss: 4.66458, val loss: 17.73152\n",
      "[862/1000] train loss: 4.73066, val loss: 17.95370\n",
      "[863/1000] train loss: 4.82639, val loss: 17.73931\n",
      "[864/1000] train loss: 4.96361, val loss: 17.87479\n",
      "[865/1000] train loss: 4.70386, val loss: 17.94460\n",
      "[866/1000] train loss: 4.72820, val loss: 18.08623\n",
      "[867/1000] train loss: 4.67116, val loss: 18.00231\n",
      "[868/1000] train loss: 4.69647, val loss: 18.08942\n",
      "[869/1000] train loss: 4.70205, val loss: 17.72264\n",
      "[870/1000] train loss: 4.69107, val loss: 17.83117\n",
      "[871/1000] train loss: 4.79280, val loss: 17.73589\n",
      "[872/1000] train loss: 4.66575, val loss: 17.77654\n",
      "[873/1000] train loss: 4.65810, val loss: 17.71169\n",
      "[874/1000] train loss: 4.66736, val loss: 17.95779\n",
      "[875/1000] train loss: 4.66119, val loss: 17.70365\n",
      "[876/1000] train loss: 4.70282, val loss: 17.84130\n",
      "[877/1000] train loss: 4.57568, val loss: 17.97695\n",
      "[878/1000] train loss: 4.65290, val loss: 17.92684\n",
      "[879/1000] train loss: 4.65381, val loss: 17.78285\n",
      "[880/1000] train loss: 4.65371, val loss: 17.80987\n",
      "[881/1000] train loss: 4.72834, val loss: 17.57883\n",
      "[882/1000] train loss: 4.63084, val loss: 17.77025\n",
      "[883/1000] train loss: 4.69500, val loss: 17.67689\n",
      "[884/1000] train loss: 4.68571, val loss: 17.79378\n",
      "[885/1000] train loss: 4.67663, val loss: 17.62044\n",
      "[886/1000] train loss: 4.61966, val loss: 17.83430\n",
      "[887/1000] train loss: 4.63282, val loss: 17.73435\n",
      "[888/1000] train loss: 4.59254, val loss: 17.67752\n",
      "[889/1000] train loss: 4.63469, val loss: 17.80612\n",
      "[890/1000] train loss: 4.60261, val loss: 17.66299\n",
      "[891/1000] train loss: 4.54803, val loss: 17.64461\n",
      "[892/1000] train loss: 4.61993, val loss: 17.87382\n",
      "[893/1000] train loss: 4.63549, val loss: 17.90918\n",
      "[894/1000] train loss: 4.61816, val loss: 17.66886\n",
      "[895/1000] train loss: 4.62401, val loss: 17.94476\n",
      "[896/1000] train loss: 4.63487, val loss: 17.84357\n",
      "[897/1000] train loss: 4.66200, val loss: 17.91188\n",
      "[898/1000] train loss: 4.68272, val loss: 17.86188\n",
      "[899/1000] train loss: 4.66573, val loss: 17.91114\n",
      "[900/1000] train loss: 4.60437, val loss: 17.76733\n",
      "[901/1000] train loss: 4.65889, val loss: 18.03156\n",
      "[902/1000] train loss: 4.44986, val loss: 17.89264\n",
      "[903/1000] train loss: 4.58179, val loss: 17.75023\n",
      "[904/1000] train loss: 4.53092, val loss: 17.86024\n",
      "[905/1000] train loss: 4.56699, val loss: 17.80923\n",
      "[906/1000] train loss: 4.57205, val loss: 17.53099\n",
      "[907/1000] train loss: 4.65353, val loss: 17.55002\n",
      "[908/1000] train loss: 4.57712, val loss: 17.77505\n",
      "[909/1000] train loss: 4.58104, val loss: 17.77213\n",
      "[910/1000] train loss: 4.54855, val loss: 17.84812\n",
      "[911/1000] train loss: 4.64867, val loss: 17.51406\n",
      "[912/1000] train loss: 4.66277, val loss: 17.54095\n",
      "[913/1000] train loss: 4.47916, val loss: 17.94838\n",
      "[914/1000] train loss: 4.52115, val loss: 17.77773\n",
      "[915/1000] train loss: 4.54320, val loss: 17.68599\n",
      "[916/1000] train loss: 4.54593, val loss: 17.56277\n",
      "[917/1000] train loss: 4.49593, val loss: 17.66857\n",
      "[918/1000] train loss: 4.51303, val loss: 17.50334\n",
      "[919/1000] train loss: 4.55259, val loss: 17.53820\n",
      "[920/1000] train loss: 4.54731, val loss: 17.68788\n",
      "[921/1000] train loss: 4.55711, val loss: 17.85390\n",
      "[922/1000] train loss: 4.56937, val loss: 17.87368\n",
      "[923/1000] train loss: 4.46575, val loss: 17.69285\n",
      "[924/1000] train loss: 4.51766, val loss: 17.86618\n",
      "[925/1000] train loss: 4.45360, val loss: 17.65802\n",
      "[926/1000] train loss: 4.56018, val loss: 17.76227\n",
      "[927/1000] train loss: 4.54107, val loss: 17.81533\n",
      "[928/1000] train loss: 4.53378, val loss: 17.76216\n",
      "[929/1000] train loss: 4.61242, val loss: 17.84571\n",
      "[930/1000] train loss: 4.51060, val loss: 17.62685\n",
      "[931/1000] train loss: 4.45908, val loss: 17.79225\n",
      "[932/1000] train loss: 4.51960, val loss: 17.65020\n",
      "[933/1000] train loss: 4.42372, val loss: 17.65064\n",
      "[934/1000] train loss: 4.47140, val loss: 17.53213\n",
      "[935/1000] train loss: 4.45307, val loss: 17.78023\n",
      "[936/1000] train loss: 4.45952, val loss: 17.59338\n",
      "[937/1000] train loss: 4.49908, val loss: 17.59012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[938/1000] train loss: 4.33550, val loss: 17.80559\n",
      "[939/1000] train loss: 4.49593, val loss: 17.69877\n",
      "[940/1000] train loss: 4.45337, val loss: 17.66805\n",
      "[941/1000] train loss: 4.42290, val loss: 17.57561\n",
      "[942/1000] train loss: 4.42722, val loss: 17.79920\n",
      "[943/1000] train loss: 4.47690, val loss: 17.75064\n",
      "[944/1000] train loss: 4.53220, val loss: 18.00628\n",
      "[945/1000] train loss: 4.48450, val loss: 17.55511\n",
      "[946/1000] train loss: 4.45097, val loss: 17.66934\n",
      "[947/1000] train loss: 4.36518, val loss: 17.85658\n",
      "[948/1000] train loss: 4.46994, val loss: 17.66122\n",
      "[949/1000] train loss: 4.49441, val loss: 17.90595\n",
      "[950/1000] train loss: 4.44112, val loss: 17.86518\n",
      "[951/1000] train loss: 4.46247, val loss: 17.42081\n",
      "[952/1000] train loss: 4.43557, val loss: 17.59121\n",
      "[953/1000] train loss: 4.39266, val loss: 17.57538\n",
      "[954/1000] train loss: 4.35237, val loss: 17.53146\n",
      "[955/1000] train loss: 4.40345, val loss: 17.81173\n",
      "[956/1000] train loss: 4.46774, val loss: 17.46971\n",
      "[957/1000] train loss: 4.32827, val loss: 17.53017\n",
      "[958/1000] train loss: 4.40991, val loss: 17.73438\n",
      "[959/1000] train loss: 4.39938, val loss: 17.77587\n",
      "[960/1000] train loss: 4.35979, val loss: 17.65866\n",
      "[961/1000] train loss: 4.35559, val loss: 17.66455\n",
      "[962/1000] train loss: 4.40529, val loss: 17.62357\n",
      "[963/1000] train loss: 4.43306, val loss: 17.67722\n",
      "[964/1000] train loss: 4.38855, val loss: 17.63337\n",
      "[965/1000] train loss: 4.30465, val loss: 17.21841\n",
      "[966/1000] train loss: 4.43216, val loss: 17.57614\n",
      "[967/1000] train loss: 4.31067, val loss: 17.55068\n",
      "[968/1000] train loss: 4.24038, val loss: 17.92715\n",
      "[969/1000] train loss: 4.36476, val loss: 17.69901\n",
      "[970/1000] train loss: 4.40029, val loss: 17.63230\n",
      "[971/1000] train loss: 4.38117, val loss: 17.59880\n",
      "[972/1000] train loss: 4.29431, val loss: 17.46617\n",
      "[973/1000] train loss: 4.38256, val loss: 17.69493\n",
      "[974/1000] train loss: 4.33558, val loss: 17.71569\n",
      "[975/1000] train loss: 4.26361, val loss: 17.57738\n",
      "[976/1000] train loss: 4.31599, val loss: 17.70754\n",
      "[977/1000] train loss: 4.31696, val loss: 17.66640\n",
      "[978/1000] train loss: 4.38393, val loss: 17.79177\n",
      "[979/1000] train loss: 4.30596, val loss: 17.73247\n",
      "[980/1000] train loss: 4.33759, val loss: 17.44977\n",
      "[981/1000] train loss: 4.39972, val loss: 17.37346\n",
      "[982/1000] train loss: 4.36603, val loss: 17.75770\n",
      "[983/1000] train loss: 4.36806, val loss: 17.71558\n",
      "[984/1000] train loss: 4.35202, val loss: 17.79190\n",
      "[985/1000] train loss: 4.33961, val loss: 17.51444\n",
      "[986/1000] train loss: 4.36141, val loss: 17.52333\n",
      "[987/1000] train loss: 4.31604, val loss: 17.67650\n",
      "[988/1000] train loss: 4.31156, val loss: 17.64128\n",
      "[989/1000] train loss: 4.34210, val loss: 17.54125\n",
      "[990/1000] train loss: 4.35998, val loss: 17.55313\n",
      "[991/1000] train loss: 4.47437, val loss: 17.35430\n",
      "[992/1000] train loss: 4.35147, val loss: 17.43700\n",
      "[993/1000] train loss: 4.32830, val loss: 17.53712\n",
      "[994/1000] train loss: 4.31592, val loss: 17.55622\n",
      "[995/1000] train loss: 4.34703, val loss: 17.50860\n",
      "[996/1000] train loss: 4.33518, val loss: 17.79679\n",
      "[997/1000] train loss: 4.31828, val loss: 17.61574\n",
      "[998/1000] train loss: 4.36218, val loss: 17.46173\n",
      "[999/1000] train loss: 4.31676, val loss: 17.49702\n",
      "[1000/1000] train loss: 4.28795, val loss: 17.57704\n"
     ]
    }
   ],
   "source": [
    "# 학습: 두단계 - 학습 + 검증\n",
    "for epoch in range(N_EPOCH):\n",
    "    ################\n",
    "    # 학습 - 모델을 train 모드로 변경\n",
    "    ################\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in boston_train_loader:\n",
    "        # X, y를 device로 이동\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 1. 모델 추정\n",
    "        pred = boston_model(X) # 순전파(forward propagation)\n",
    "        # 2. loss 계산\n",
    "        loss = loss_fn(pred, y)  # 추정, 정답\n",
    "        # 3. 모델 파라미터를 업데이트\n",
    "        ## 3.1 파라미터들의 기울기를 초기화\n",
    "        optimizer.zero_grad()\n",
    "        ## 3.2 역전파(back propagration)을 해서 파라미터들의 기울기를 계산(grad속성에 저장)\n",
    "        loss.backward()\n",
    "        ## 3.3 파라미터 업데이트 처리.=> 1 step\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    #평균 loss\n",
    "    train_loss /= len(boston_train_loader)\n",
    "    # 1 epoch 학습끝\n",
    "    ###############################\n",
    "    # 검증 - 모델을 평가모드로 변경\n",
    "    ###############################\n",
    "    boston_model.eval() #evalutation mode 로 변환\n",
    "    val_loss = 0.0\n",
    "    # 역전파를 통한 gradient 계산이 필요 없기 때문에 일시적으로 grad_fn 을 구하지 않도록 처리.\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in boston_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            # 1.추정\n",
    "            pred_val = boston_model(X_val)\n",
    "            # 2. loss 계산\n",
    "            val_loss += loss_fn(pred_val, y_val).item()\n",
    "        val_loss /= len(boston_test_loader)\n",
    "    # epoch에 대한 검증 완료\n",
    "    # 결과 출력\n",
    "    print(f\"[{epoch+1}/{N_EPOCH}] train loss: {train_loss:.5f}, val loss: {val_loss:.5f}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(N_EPOCH), train_loss_list, label=\"train loss\")\n",
    "plt.plot(range(N_EPOCH), val_loss_list, label=\"validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(3, 30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "## 모델 전체 저장 및 불러오기\n",
    "- 모델구조, 파라미터 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'models/boston_model.pt'\n",
    "torch.save(boston_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_boston_model_1 = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(load_boston_model_1, (200, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = 0.0\n",
    "load_boston_model_1.to(device)\n",
    "load_boston_model_1.eval()\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in boston_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        # 1.추정\n",
    "        pred_val = load_boston_model_1(X_val)\n",
    "        # 2. loss 계산\n",
    "        val_loss += loss_fn(pred_val, y_val).item()\n",
    "    val_loss /= len(boston_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.577037811279297"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_dict 저장 및 로딩\n",
    "- 모델 파라미터만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path2 = \"models/boston_state_dict.pt\"\n",
    "model_sd = boston_model.state_dict()\n",
    "\n",
    "torch.save(model_sd, save_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#state_dict 를 로딩 \n",
    "## 1. 모델객체 생성\n",
    "load_boston_model_2 = BostonModel().to(device)\n",
    "## 2. state_dict 불러오기\n",
    "load_sd = torch.load(save_path2)\n",
    "## 3. 불러온 state_dict(파라미터들)을 모델에 덮어 씌우기\n",
    "load_boston_model_2.load_state_dict(load_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.577037811279297"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss = 0.0\n",
    "load_boston_model_2.to(device)\n",
    "load_boston_model_2.eval() #평가모드 변환\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in boston_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        # 1.추정\n",
    "        pred_val = load_boston_model_2(X_val)\n",
    "        # 2. loss 계산\n",
    "        val_loss += loss_fn(pred_val, y_val).item()\n",
    "    val_loss /= len(boston_test_loader)\n",
    "    \n",
    "val_loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 분류 (Classification)\n",
    "\n",
    "## Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋. \n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "이미지는 28x28 크기이며 Gray scale이다. *레이블*(label)은 0에서 9까지의 정수 배열이다. 아래 표는 이미지에 있는 의류의 **클래스**(class)들이다.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>레이블</th>\n",
    "    <th>클래스</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trousers</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Dress', 'Pullover', 'Sandal'], dtype='<U11'), 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_class = np.array(['T-shirt/top', 'Trousers', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'])\n",
    "class_to_index = {key:value for value, key in enumerate(index_to_class)}\n",
    "\n",
    "index_to_class[[3, 2, 5]], class_to_index['Pullover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # channel first 처리. 0 ~ 1 scaling, torch.Tensor 변환\n",
    "#     transforms.Normalize(mean=0.5, std=0.5)  # 표준화((pixcel-mean)/std). (-1 ~ 1)\n",
    "])\n",
    "# Dataset loading\n",
    "fmnist_trainset = datasets.FashionMNIST(root=\"datasets\", train=True, \n",
    "                                        download=True, transform=transform)\n",
    "fmnist_testset = datasets.FashionMNIST(root=\"datasets\", train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: datasets\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: datasets\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(fmnist_trainset)\n",
    "print(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fmnist_trainset), len(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Ankle boot\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x, y = fmnist_trainset[0]\n",
    "print(y, index_to_class[y])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader 생성\n",
    "fmnist_train_loader = DataLoader(fmnist_trainset, batch_size=128, \n",
    "                                 shuffle=True, drop_last=True)\n",
    "fmnist_test_loader = DataLoader(fmnist_testset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(28*28, 2048)\n",
    "        self.lr2 = nn.Linear(2048, 1024)\n",
    "        self.lr3 = nn.Linear(1024, 512)\n",
    "        self.lr4 = nn.Linear(512, 256)\n",
    "        self.lr5 = nn.Linear(256, 128)\n",
    "        self.lr6 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 10) #out_feature: 10 - 10개 class별 확률.\n",
    "    def forward(self, X):\n",
    "        out = nn.Flatten()(X)\n",
    "        out = nn.ReLU()(self.lr1(out))\n",
    "        out = nn.ReLU()(self.lr2(out))\n",
    "        out = nn.ReLU()(self.lr3(out))\n",
    "        out = nn.ReLU()(self.lr4(out))\n",
    "        out = nn.ReLU()(self.lr5(out))\n",
    "        out = nn.ReLU()(self.lr6(out))\n",
    "        out = self.output(out)\n",
    "#         nn.Softmax()(out)\n",
    "        # 다중분류의 output는 Softmax()함수로 계산해서 확률로 만들어서 출력해야 한다.\n",
    "        # 모델에서는 Linear를 통과한 결과를 반환.\n",
    "        # loss함수인 CrossEntropyLoss() 에서 softmax를 적용한다. \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModel(\n",
       "  (lr1): Linear(in_features=784, out_features=2048, bias=True)\n",
       "  (lr2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (lr3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (lr4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lr5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lr6): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_model = FashionMNISTModel()\n",
    "f_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNISTModel                        [128, 10]                 --\n",
       "├─Linear: 1-1                            [128, 2048]               1,607,680\n",
       "├─Linear: 1-2                            [128, 1024]               2,098,176\n",
       "├─Linear: 1-3                            [128, 512]                524,800\n",
       "├─Linear: 1-4                            [128, 256]                131,328\n",
       "├─Linear: 1-5                            [128, 128]                32,896\n",
       "├─Linear: 1-6                            [128, 64]                 8,256\n",
       "├─Linear: 1-7                            [128, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 4,403,786\n",
       "Trainable params: 4,403,786\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 563.68\n",
       "==========================================================================================\n",
       "Input size (MB): 0.40\n",
       "Forward/backward pass size (MB): 4.14\n",
       "Params size (MB): 17.62\n",
       "Estimated Total Size (MB): 22.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(f_model, (128, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n",
      "tensor([-0.0704,  0.1061, -0.0297, -0.0479, -0.0593,  0.0345,  0.0791,  0.0875,\n",
      "        -0.0969, -0.0795], grad_fn=<SelectBackward0>)\n",
      "-0.0765145793557167 1\n"
     ]
    }
   ],
   "source": [
    "# 추정\n",
    "i = torch.ones((2, 1, 28, 28), dtype=torch.float32)\n",
    "# i.shape\n",
    "y_hat = f_model(i)\n",
    "print(y_hat.shape)\n",
    "print(y_hat[0])\n",
    "print(y_hat[0].sum().item(), y_hat[0].argmax(dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0937, 0.1117, 0.0976, 0.0958, 0.0947, 0.1040, 0.1088, 0.1097, 0.0912,\n",
      "        0.0928], grad_fn=<SoftmaxBackward0>)\n",
      "0.9999999403953552 1\n"
     ]
    }
   ],
   "source": [
    "# 모델이 추정한 결과의 class를 알고 싶을 경우는 Softmax를 계산할 필요 없다.\n",
    "# 모델의 추정 확률을 알고 싶을 경우 softmax를 계산한다.\n",
    "y_hat2 = nn.Softmax(dim=-1)(y_hat[0])\n",
    "print(y_hat2)\n",
    "print(y_hat2.sum().item(), y_hat2.argmax(dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 0.61363 val loss: 0.52008 val_accuracy: 0.81630\n",
      "저장 : 1 epoch - 이전 best_score: inf, 현재 score: 0.5200847425038302\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m fmnist_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     34\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m fmnist_train_loader:\n\u001b[0;32m     36\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     38\u001b[0m     pred \u001b[38;5;241m=\u001b[39m fmnist_model(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### 학습\n",
    "import time\n",
    "device = \"cpu\"\n",
    "# 모델 생성 + device 이동\n",
    "fmnist_model = FashionMNISTModel().to(device)\n",
    "# loss -> 다중분류: CrossEntropyLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(fmnist_model.parameters(), lr=0.001)\n",
    "\n",
    "# 결과저장할 리스트\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "####################\n",
    "# 가장 성능 좋은 epoch의 모델을 학습도중 저장. -> 성능이 개선될때 마다 저장.\n",
    "# 필요한 변수들 정의\n",
    "####################\n",
    "best_score = torch.inf # 학습중 가장 좋은 평가지표(val_loss)를 저장.\n",
    "save_model_path = \"models/fashion_mnist_best_model.pt\"\n",
    "\n",
    "####################\n",
    "# 조기종료 (Eearly Stopping) - 특정 epoch동안 성능 개선이 없으면 학습을 중단.\n",
    "####################\n",
    "patience = 10   # 성능 개선 여부를 몇 epoch동안 확인 할 것인지.\n",
    "trigger_cnt = 0 # 몇 epoch째 성능개선을 기다리는지를 저장할 변수.\n",
    "\n",
    "N_EPOCH = 1000\n",
    "s = time.time()\n",
    "for epoch in range(N_EPOCH): #에폭 학습\n",
    "    ########### 학습\n",
    "    fmnist_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in fmnist_train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = fmnist_model(X)\n",
    "        loss = loss_fn(pred, y) # pred: Softmax(), y: one hot encoding 처리\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(fmnist_train_loader)   # 평균 loss 계산\n",
    "    #### 1 epoch 학습 종료\n",
    "     \n",
    "    ############ 검증\n",
    "    fmnist_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in fmnist_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            \n",
    "            pred_val = fmnist_model(X_val)  # Softmax 적용 전. -> loss는 이 값으로 계산.\n",
    "            pred_label = pred_val.argmax(dim=-1)  # accuracy  계산은 이 값으로 한다.\n",
    "            \n",
    "            # val-loss\n",
    "            loss_val = loss_fn(pred_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "            # val-accuracy\n",
    "            val_acc += torch.sum(pred_label == y_val).item()  #현 배치에서 맞은 것의 개수\n",
    "        # val_loss, val_acc의 평균\n",
    "        val_loss /= len(fmnist_test_loader)  # step 수로 나눔.\n",
    "        val_acc /= len(fmnist_test_loader.dataset) # 총 데이터 개수로 나눔.\n",
    "    \n",
    "    # 현재 Epoch에 대한 학습, 검증 종료\n",
    "    print(f\"[{epoch+1}/{N_EPOCH}] train loss: {train_loss:.5f} val loss: {val_loss:.5f} val_accuracy: {val_acc:.5f}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    ######################################## \n",
    "    #  조기종료, 모델 저장 처리\n",
    "    #   현 epoch의 val_loss가 best_score보다 개선된 경우.(작은 경우)\n",
    "    #######################################\n",
    "    if val_loss < best_score: #성능개선\n",
    "        # 저장/조기종료\n",
    "        print(f\"저장 : {epoch+1} epoch - 이전 best_score: {best_score}, 현재 score: {val_loss}\")\n",
    "        best_score = val_loss\n",
    "        torch.save(fmnist_model, save_model_path)\n",
    "        trigger_cnt = 0\n",
    "        \n",
    "    else:\n",
    "        # 저장안하기/trigger_cnt 를 증가.=>조기종료\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt:\n",
    "            print(f\"조기종료: epoch-{epoch+1}. {best_score:.5f}에서 개선이 안됨.\")\n",
    "            break\n",
    "    \n",
    "e = time.time()\n",
    "print(f'학습에 걸린시간: {e-s}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1182/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_list, label='train loss')\n",
    "plt.plot(val_loss_list, label='validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_acc_list)\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_load_model = torch.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in fmnist_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "        pred_val = fmnist_load_model(X_val)  # Softmax 적용 전. -> loss는 이 값으로 계산.\n",
    "        pred_label = pred_val.argmax(dim=-1)  # accuracy  계산은 이 값으로 한다.\n",
    "\n",
    "        # val-loss\n",
    "        loss_val = loss_fn(pred_val, y_val)\n",
    "        val_loss += loss_val.item()\n",
    "        # val-accuracy\n",
    "        val_acc += torch.sum(pred_label == y_val).item()  #현 배치에서 맞은 것의 개수\n",
    "    # val_loss, val_acc의 평균\n",
    "    val_loss /= len(fmnist_test_loader)  # step 수로 나눔.\n",
    "    val_acc /= len(fmnist_test_loader.dataset) # 총 데이터 개수로 나눔."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.31778975510144536, 0.8926)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제\n",
    "\n",
    "- **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "        \n",
    "- 위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "- Feature\n",
    "    - 종양에 대한 다양한 측정값들\n",
    "- Target의 class\n",
    "    - 0 - malignant(악성종양)\n",
    "    - 1 - benign(양성종양)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "y = y.reshape(-1, 1)  # 2차원으로 변경. ==> 모델 출력 shape과 맞춰준다. \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_tensor = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([426, 1]), torch.Size([143, 1]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape, y_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - 데이터셋이 Tensor 객체로 메모리에 loading된 경우\n",
    "wb_train_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "wb_test_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# DataLoader\n",
    "wb_train_loader = DataLoader(wb_train_set, batch_size=len(wb_train_set),\n",
    "                             shuffle=True, drop_last=True)\n",
    "wb_test_loader = DataLoader(wb_test_set, batch_size=len(wb_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([426, 30]), torch.Size([426, 1]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(wb_train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4.4773e-09,  4.4773e-09,  2.2387e-09,  8.9547e-09,  0.0000e+00,\n",
       "          2.2387e-09, -2.2387e-09,  4.4773e-09, -2.2387e-09,  4.4773e-09,\n",
       "         -8.9547e-09, -4.4773e-09,  2.2387e-09, -2.2387e-09,  4.4773e-09,\n",
       "         -2.2387e-09, -2.2387e-09,  2.2387e-09,  6.7160e-09, -2.2387e-09,\n",
       "          2.2387e-09, -2.2387e-09,  2.2387e-09,  6.7160e-09,  5.5967e-10,\n",
       "         -1.1193e-09, -2.2387e-09,  2.2387e-09,  2.2387e-09, -1.1193e-09]),\n",
       " tensor([1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012,\n",
       "         1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012,\n",
       "         1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012,\n",
       "         1.0012, 1.0012, 1.0012]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=0), x.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_class = [\"악성종양\", \"양성종양\"]\n",
    "class_to_index = {\"악성종양\":0, \"양성종양\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델정의\n",
    "class BreastCancerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(30, 32)\n",
    "        self.lr2 = nn.Linear(32, 16)\n",
    "        self.output = nn.Linear(16, 1)  # 출력- 값 1개 (positive의 확률을 출력)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = nn.ReLU()(self.lr1(X))\n",
    "        out = nn.ReLU()(self.lr2(out))\n",
    "        out = self.output(out)   \n",
    "        # 2진분류->positive 확률을 출력 -> output의 출력결과를 logistic 함수에 입력\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5580],\n",
       "        [0.5580],\n",
       "        [0.5580],\n",
       "        [0.5580],\n",
       "        [0.5580]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_model_tmp = BreastCancerModel()\n",
    "tmp_x = torch.ones(5, 30)\n",
    "y_tmp = bc_model_tmp(tmp_x)\n",
    "y_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] train loss: 0.7147631645202637, val loss: 0.7080258727073669, val accuracy: 0.2867132867132867\n",
      "1 epoch에서 저장. 이전 score: inf, 현재 score: 0.7080258727073669\n",
      "[2/500] train loss: 0.7085423469543457, val loss: 0.7021784782409668, val accuracy: 0.36363636363636365\n",
      "2 epoch에서 저장. 이전 score: 0.7080258727073669, 현재 score: 0.7021784782409668\n",
      "[3/500] train loss: 0.702426552772522, val loss: 0.6962939500808716, val accuracy: 0.43356643356643354\n",
      "3 epoch에서 저장. 이전 score: 0.7021784782409668, 현재 score: 0.6962939500808716\n",
      "[4/500] train loss: 0.6963770985603333, val loss: 0.6904833316802979, val accuracy: 0.5244755244755245\n",
      "4 epoch에서 저장. 이전 score: 0.6962939500808716, 현재 score: 0.6904833316802979\n",
      "[5/500] train loss: 0.6903742551803589, val loss: 0.6847456693649292, val accuracy: 0.6223776223776224\n",
      "5 epoch에서 저장. 이전 score: 0.6904833316802979, 현재 score: 0.6847456693649292\n",
      "[6/500] train loss: 0.6844336986541748, val loss: 0.6790831089019775, val accuracy: 0.6783216783216783\n",
      "6 epoch에서 저장. 이전 score: 0.6847456693649292, 현재 score: 0.6790831089019775\n",
      "[7/500] train loss: 0.6785277128219604, val loss: 0.6734089255332947, val accuracy: 0.7342657342657343\n",
      "7 epoch에서 저장. 이전 score: 0.6790831089019775, 현재 score: 0.6734089255332947\n",
      "[8/500] train loss: 0.6726632714271545, val loss: 0.6676297783851624, val accuracy: 0.7972027972027972\n",
      "8 epoch에서 저장. 이전 score: 0.6734089255332947, 현재 score: 0.6676297783851624\n",
      "[9/500] train loss: 0.6667669415473938, val loss: 0.6617723703384399, val accuracy: 0.8461538461538461\n",
      "9 epoch에서 저장. 이전 score: 0.6676297783851624, 현재 score: 0.6617723703384399\n",
      "[10/500] train loss: 0.6607634425163269, val loss: 0.6558197736740112, val accuracy: 0.8531468531468531\n",
      "10 epoch에서 저장. 이전 score: 0.6617723703384399, 현재 score: 0.6558197736740112\n",
      "[11/500] train loss: 0.6546813249588013, val loss: 0.6497950553894043, val accuracy: 0.8671328671328671\n",
      "11 epoch에서 저장. 이전 score: 0.6558197736740112, 현재 score: 0.6497950553894043\n",
      "[12/500] train loss: 0.6484853029251099, val loss: 0.6437558531761169, val accuracy: 0.8601398601398601\n",
      "12 epoch에서 저장. 이전 score: 0.6497950553894043, 현재 score: 0.6437558531761169\n",
      "[13/500] train loss: 0.6421413421630859, val loss: 0.6376334428787231, val accuracy: 0.8741258741258742\n",
      "13 epoch에서 저장. 이전 score: 0.6437558531761169, 현재 score: 0.6376334428787231\n",
      "[14/500] train loss: 0.6356545090675354, val loss: 0.6313732862472534, val accuracy: 0.8881118881118881\n",
      "14 epoch에서 저장. 이전 score: 0.6376334428787231, 현재 score: 0.6313732862472534\n",
      "[15/500] train loss: 0.6290503144264221, val loss: 0.6249911785125732, val accuracy: 0.8951048951048951\n",
      "15 epoch에서 저장. 이전 score: 0.6313732862472534, 현재 score: 0.6249911785125732\n",
      "[16/500] train loss: 0.6223083734512329, val loss: 0.6185026168823242, val accuracy: 0.8951048951048951\n",
      "16 epoch에서 저장. 이전 score: 0.6249911785125732, 현재 score: 0.6185026168823242\n",
      "[17/500] train loss: 0.6154334545135498, val loss: 0.6119041442871094, val accuracy: 0.9020979020979021\n",
      "17 epoch에서 저장. 이전 score: 0.6185026168823242, 현재 score: 0.6119041442871094\n",
      "[18/500] train loss: 0.6084097623825073, val loss: 0.6051412224769592, val accuracy: 0.9090909090909091\n",
      "18 epoch에서 저장. 이전 score: 0.6119041442871094, 현재 score: 0.6051412224769592\n",
      "[19/500] train loss: 0.6012277603149414, val loss: 0.5981916189193726, val accuracy: 0.9020979020979021\n",
      "19 epoch에서 저장. 이전 score: 0.6051412224769592, 현재 score: 0.5981916189193726\n",
      "[20/500] train loss: 0.5938860177993774, val loss: 0.591083824634552, val accuracy: 0.9020979020979021\n",
      "20 epoch에서 저장. 이전 score: 0.5981916189193726, 현재 score: 0.591083824634552\n",
      "[21/500] train loss: 0.5863224267959595, val loss: 0.5838263630867004, val accuracy: 0.9020979020979021\n",
      "21 epoch에서 저장. 이전 score: 0.591083824634552, 현재 score: 0.5838263630867004\n",
      "[22/500] train loss: 0.5785467028617859, val loss: 0.5763756036758423, val accuracy: 0.9020979020979021\n",
      "22 epoch에서 저장. 이전 score: 0.5838263630867004, 현재 score: 0.5763756036758423\n",
      "[23/500] train loss: 0.5705867409706116, val loss: 0.5687137246131897, val accuracy: 0.9020979020979021\n",
      "23 epoch에서 저장. 이전 score: 0.5763756036758423, 현재 score: 0.5687137246131897\n",
      "[24/500] train loss: 0.5624405145645142, val loss: 0.560872495174408, val accuracy: 0.9020979020979021\n",
      "24 epoch에서 저장. 이전 score: 0.5687137246131897, 현재 score: 0.560872495174408\n",
      "[25/500] train loss: 0.5540940761566162, val loss: 0.5528752207756042, val accuracy: 0.9020979020979021\n",
      "25 epoch에서 저장. 이전 score: 0.560872495174408, 현재 score: 0.5528752207756042\n",
      "[26/500] train loss: 0.545531153678894, val loss: 0.5446810126304626, val accuracy: 0.9020979020979021\n",
      "26 epoch에서 저장. 이전 score: 0.5528752207756042, 현재 score: 0.5446810126304626\n",
      "[27/500] train loss: 0.5367686748504639, val loss: 0.5363824963569641, val accuracy: 0.9020979020979021\n",
      "27 epoch에서 저장. 이전 score: 0.5446810126304626, 현재 score: 0.5363824963569641\n",
      "[28/500] train loss: 0.5278321504592896, val loss: 0.5279403328895569, val accuracy: 0.9090909090909091\n",
      "28 epoch에서 저장. 이전 score: 0.5363824963569641, 현재 score: 0.5279403328895569\n",
      "[29/500] train loss: 0.5187581181526184, val loss: 0.5193701982498169, val accuracy: 0.9090909090909091\n",
      "29 epoch에서 저장. 이전 score: 0.5279403328895569, 현재 score: 0.5193701982498169\n",
      "[30/500] train loss: 0.5095236897468567, val loss: 0.5106621980667114, val accuracy: 0.9090909090909091\n",
      "30 epoch에서 저장. 이전 score: 0.5193701982498169, 현재 score: 0.5106621980667114\n",
      "[31/500] train loss: 0.5001606345176697, val loss: 0.5018409490585327, val accuracy: 0.916083916083916\n",
      "31 epoch에서 저장. 이전 score: 0.5106621980667114, 현재 score: 0.5018409490585327\n",
      "[32/500] train loss: 0.49069371819496155, val loss: 0.4929061532020569, val accuracy: 0.916083916083916\n",
      "32 epoch에서 저장. 이전 score: 0.5018409490585327, 현재 score: 0.4929061532020569\n",
      "[33/500] train loss: 0.481116384267807, val loss: 0.48389407992362976, val accuracy: 0.916083916083916\n",
      "33 epoch에서 저장. 이전 score: 0.4929061532020569, 현재 score: 0.48389407992362976\n",
      "[34/500] train loss: 0.4714493751525879, val loss: 0.4748150408267975, val accuracy: 0.916083916083916\n",
      "34 epoch에서 저장. 이전 score: 0.48389407992362976, 현재 score: 0.4748150408267975\n",
      "[35/500] train loss: 0.4616855978965759, val loss: 0.46566110849380493, val accuracy: 0.916083916083916\n",
      "35 epoch에서 저장. 이전 score: 0.4748150408267975, 현재 score: 0.46566110849380493\n",
      "[36/500] train loss: 0.4518485367298126, val loss: 0.4564412832260132, val accuracy: 0.916083916083916\n",
      "36 epoch에서 저장. 이전 score: 0.46566110849380493, 현재 score: 0.4564412832260132\n",
      "[37/500] train loss: 0.4419917166233063, val loss: 0.4471825957298279, val accuracy: 0.916083916083916\n",
      "37 epoch에서 저장. 이전 score: 0.4564412832260132, 현재 score: 0.4471825957298279\n",
      "[38/500] train loss: 0.43210190534591675, val loss: 0.4378918707370758, val accuracy: 0.916083916083916\n",
      "38 epoch에서 저장. 이전 score: 0.4471825957298279, 현재 score: 0.4378918707370758\n",
      "[39/500] train loss: 0.4221939742565155, val loss: 0.42860108613967896, val accuracy: 0.9230769230769231\n",
      "39 epoch에서 저장. 이전 score: 0.4378918707370758, 현재 score: 0.42860108613967896\n",
      "[40/500] train loss: 0.41229164600372314, val loss: 0.4193340837955475, val accuracy: 0.9230769230769231\n",
      "40 epoch에서 저장. 이전 score: 0.42860108613967896, 현재 score: 0.4193340837955475\n",
      "[41/500] train loss: 0.40240800380706787, val loss: 0.41010424494743347, val accuracy: 0.9230769230769231\n",
      "41 epoch에서 저장. 이전 score: 0.4193340837955475, 현재 score: 0.41010424494743347\n",
      "[42/500] train loss: 0.39256975054740906, val loss: 0.400937020778656, val accuracy: 0.9230769230769231\n",
      "42 epoch에서 저장. 이전 score: 0.41010424494743347, 현재 score: 0.400937020778656\n",
      "[43/500] train loss: 0.3827947974205017, val loss: 0.391849160194397, val accuracy: 0.9230769230769231\n",
      "43 epoch에서 저장. 이전 score: 0.400937020778656, 현재 score: 0.391849160194397\n",
      "[44/500] train loss: 0.37310370802879333, val loss: 0.3828575611114502, val accuracy: 0.9230769230769231\n",
      "44 epoch에서 저장. 이전 score: 0.391849160194397, 현재 score: 0.3828575611114502\n",
      "[45/500] train loss: 0.3635132908821106, val loss: 0.37396085262298584, val accuracy: 0.9230769230769231\n",
      "45 epoch에서 저장. 이전 score: 0.3828575611114502, 현재 score: 0.37396085262298584\n",
      "[46/500] train loss: 0.3540191948413849, val loss: 0.3651650846004486, val accuracy: 0.9230769230769231\n",
      "46 epoch에서 저장. 이전 score: 0.37396085262298584, 현재 score: 0.3651650846004486\n",
      "[47/500] train loss: 0.34465184807777405, val loss: 0.3565065264701843, val accuracy: 0.9230769230769231\n",
      "47 epoch에서 저장. 이전 score: 0.3651650846004486, 현재 score: 0.3565065264701843\n",
      "[48/500] train loss: 0.3354305326938629, val loss: 0.347997784614563, val accuracy: 0.916083916083916\n",
      "48 epoch에서 저장. 이전 score: 0.3565065264701843, 현재 score: 0.347997784614563\n",
      "[49/500] train loss: 0.3263683021068573, val loss: 0.3396528661251068, val accuracy: 0.916083916083916\n",
      "49 epoch에서 저장. 이전 score: 0.347997784614563, 현재 score: 0.3396528661251068\n",
      "[50/500] train loss: 0.317486971616745, val loss: 0.3314795196056366, val accuracy: 0.916083916083916\n",
      "50 epoch에서 저장. 이전 score: 0.3396528661251068, 현재 score: 0.3314795196056366\n",
      "[51/500] train loss: 0.3087974190711975, val loss: 0.32348066568374634, val accuracy: 0.9230769230769231\n",
      "51 epoch에서 저장. 이전 score: 0.3314795196056366, 현재 score: 0.32348066568374634\n",
      "[52/500] train loss: 0.3002930283546448, val loss: 0.3156668543815613, val accuracy: 0.9230769230769231\n",
      "52 epoch에서 저장. 이전 score: 0.32348066568374634, 현재 score: 0.3156668543815613\n",
      "[53/500] train loss: 0.29198578000068665, val loss: 0.30804747343063354, val accuracy: 0.9230769230769231\n",
      "53 epoch에서 저장. 이전 score: 0.3156668543815613, 현재 score: 0.30804747343063354\n",
      "[54/500] train loss: 0.28387781977653503, val loss: 0.30063390731811523, val accuracy: 0.9230769230769231\n",
      "54 epoch에서 저장. 이전 score: 0.30804747343063354, 현재 score: 0.30063390731811523\n",
      "[55/500] train loss: 0.2759839594364166, val loss: 0.29343852400779724, val accuracy: 0.9300699300699301\n",
      "55 epoch에서 저장. 이전 score: 0.30063390731811523, 현재 score: 0.29343852400779724\n",
      "[56/500] train loss: 0.2682975232601166, val loss: 0.2864527404308319, val accuracy: 0.9300699300699301\n",
      "56 epoch에서 저장. 이전 score: 0.29343852400779724, 현재 score: 0.2864527404308319\n",
      "[57/500] train loss: 0.26083147525787354, val loss: 0.2796529531478882, val accuracy: 0.9300699300699301\n",
      "57 epoch에서 저장. 이전 score: 0.2864527404308319, 현재 score: 0.2796529531478882\n",
      "[58/500] train loss: 0.25359046459198, val loss: 0.2730754017829895, val accuracy: 0.9300699300699301\n",
      "58 epoch에서 저장. 이전 score: 0.2796529531478882, 현재 score: 0.2730754017829895\n",
      "[59/500] train loss: 0.24657070636749268, val loss: 0.2667159140110016, val accuracy: 0.9300699300699301\n",
      "59 epoch에서 저장. 이전 score: 0.2730754017829895, 현재 score: 0.2667159140110016\n",
      "[60/500] train loss: 0.23978805541992188, val loss: 0.2605602741241455, val accuracy: 0.9370629370629371\n",
      "60 epoch에서 저장. 이전 score: 0.2667159140110016, 현재 score: 0.2605602741241455\n",
      "[61/500] train loss: 0.2332351803779602, val loss: 0.2546081244945526, val accuracy: 0.9440559440559441\n",
      "61 epoch에서 저장. 이전 score: 0.2605602741241455, 현재 score: 0.2546081244945526\n",
      "[62/500] train loss: 0.22691315412521362, val loss: 0.2488590031862259, val accuracy: 0.9440559440559441\n",
      "62 epoch에서 저장. 이전 score: 0.2546081244945526, 현재 score: 0.2488590031862259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63/500] train loss: 0.22081272304058075, val loss: 0.24332265555858612, val accuracy: 0.9440559440559441\n",
      "63 epoch에서 저장. 이전 score: 0.2488590031862259, 현재 score: 0.24332265555858612\n",
      "[64/500] train loss: 0.2149229794740677, val loss: 0.23799341917037964, val accuracy: 0.9440559440559441\n",
      "64 epoch에서 저장. 이전 score: 0.24332265555858612, 현재 score: 0.23799341917037964\n",
      "[65/500] train loss: 0.20923563838005066, val loss: 0.2328455001115799, val accuracy: 0.9440559440559441\n",
      "65 epoch에서 저장. 이전 score: 0.23799341917037964, 현재 score: 0.2328455001115799\n",
      "[66/500] train loss: 0.2037457972764969, val loss: 0.22790281474590302, val accuracy: 0.9440559440559441\n",
      "66 epoch에서 저장. 이전 score: 0.2328455001115799, 현재 score: 0.22790281474590302\n",
      "[67/500] train loss: 0.1984550952911377, val loss: 0.22317680716514587, val accuracy: 0.9440559440559441\n",
      "67 epoch에서 저장. 이전 score: 0.22790281474590302, 현재 score: 0.22317680716514587\n",
      "[68/500] train loss: 0.19335757195949554, val loss: 0.21865706145763397, val accuracy: 0.9440559440559441\n",
      "68 epoch에서 저장. 이전 score: 0.22317680716514587, 현재 score: 0.21865706145763397\n",
      "[69/500] train loss: 0.18844930827617645, val loss: 0.2143050730228424, val accuracy: 0.9440559440559441\n",
      "69 epoch에서 저장. 이전 score: 0.21865706145763397, 현재 score: 0.2143050730228424\n",
      "[70/500] train loss: 0.18372967839241028, val loss: 0.21010129153728485, val accuracy: 0.9440559440559441\n",
      "70 epoch에서 저장. 이전 score: 0.2143050730228424, 현재 score: 0.21010129153728485\n",
      "[71/500] train loss: 0.17919275164604187, val loss: 0.20607693493366241, val accuracy: 0.9440559440559441\n",
      "71 epoch에서 저장. 이전 score: 0.21010129153728485, 현재 score: 0.20607693493366241\n",
      "[72/500] train loss: 0.17483346164226532, val loss: 0.20222187042236328, val accuracy: 0.9440559440559441\n",
      "72 epoch에서 저장. 이전 score: 0.20607693493366241, 현재 score: 0.20222187042236328\n",
      "[73/500] train loss: 0.17064720392227173, val loss: 0.19851720333099365, val accuracy: 0.9440559440559441\n",
      "73 epoch에서 저장. 이전 score: 0.20222187042236328, 현재 score: 0.19851720333099365\n",
      "[74/500] train loss: 0.16662028431892395, val loss: 0.19496962428092957, val accuracy: 0.9440559440559441\n",
      "74 epoch에서 저장. 이전 score: 0.19851720333099365, 현재 score: 0.19496962428092957\n",
      "[75/500] train loss: 0.1627332717180252, val loss: 0.19156146049499512, val accuracy: 0.9440559440559441\n",
      "75 epoch에서 저장. 이전 score: 0.19496962428092957, 현재 score: 0.19156146049499512\n",
      "[76/500] train loss: 0.15898089110851288, val loss: 0.18827813863754272, val accuracy: 0.9440559440559441\n",
      "76 epoch에서 저장. 이전 score: 0.19156146049499512, 현재 score: 0.18827813863754272\n",
      "[77/500] train loss: 0.15536466240882874, val loss: 0.1851196587085724, val accuracy: 0.9440559440559441\n",
      "77 epoch에서 저장. 이전 score: 0.18827813863754272, 현재 score: 0.1851196587085724\n",
      "[78/500] train loss: 0.15187765657901764, val loss: 0.182085320353508, val accuracy: 0.9440559440559441\n",
      "78 epoch에서 저장. 이전 score: 0.1851196587085724, 현재 score: 0.182085320353508\n",
      "[79/500] train loss: 0.14851658046245575, val loss: 0.1791691929101944, val accuracy: 0.9440559440559441\n",
      "79 epoch에서 저장. 이전 score: 0.182085320353508, 현재 score: 0.1791691929101944\n",
      "[80/500] train loss: 0.145276740193367, val loss: 0.1763722151517868, val accuracy: 0.9440559440559441\n",
      "80 epoch에서 저장. 이전 score: 0.1791691929101944, 현재 score: 0.1763722151517868\n",
      "[81/500] train loss: 0.14215366542339325, val loss: 0.17368583381175995, val accuracy: 0.9440559440559441\n",
      "81 epoch에서 저장. 이전 score: 0.1763722151517868, 현재 score: 0.17368583381175995\n",
      "[82/500] train loss: 0.13914673030376434, val loss: 0.1710880845785141, val accuracy: 0.9440559440559441\n",
      "82 epoch에서 저장. 이전 score: 0.17368583381175995, 현재 score: 0.1710880845785141\n",
      "[83/500] train loss: 0.13624493777751923, val loss: 0.1685931384563446, val accuracy: 0.9440559440559441\n",
      "83 epoch에서 저장. 이전 score: 0.1710880845785141, 현재 score: 0.1685931384563446\n",
      "[84/500] train loss: 0.13344483077526093, val loss: 0.16620242595672607, val accuracy: 0.9440559440559441\n",
      "84 epoch에서 저장. 이전 score: 0.1685931384563446, 현재 score: 0.16620242595672607\n",
      "[85/500] train loss: 0.1307411789894104, val loss: 0.16391193866729736, val accuracy: 0.9440559440559441\n",
      "85 epoch에서 저장. 이전 score: 0.16620242595672607, 현재 score: 0.16391193866729736\n",
      "[86/500] train loss: 0.128134623169899, val loss: 0.1617053896188736, val accuracy: 0.9440559440559441\n",
      "86 epoch에서 저장. 이전 score: 0.16391193866729736, 현재 score: 0.1617053896188736\n",
      "[87/500] train loss: 0.12561897933483124, val loss: 0.1595718413591385, val accuracy: 0.9440559440559441\n",
      "87 epoch에서 저장. 이전 score: 0.1617053896188736, 현재 score: 0.1595718413591385\n",
      "[88/500] train loss: 0.12318824976682663, val loss: 0.15751200914382935, val accuracy: 0.9440559440559441\n",
      "88 epoch에서 저장. 이전 score: 0.1595718413591385, 현재 score: 0.15751200914382935\n",
      "[89/500] train loss: 0.12083573639392853, val loss: 0.15552423894405365, val accuracy: 0.9440559440559441\n",
      "89 epoch에서 저장. 이전 score: 0.15751200914382935, 현재 score: 0.15552423894405365\n",
      "[90/500] train loss: 0.11856193095445633, val loss: 0.15360844135284424, val accuracy: 0.9440559440559441\n",
      "90 epoch에서 저장. 이전 score: 0.15552423894405365, 현재 score: 0.15360844135284424\n",
      "[91/500] train loss: 0.11636361479759216, val loss: 0.15176765620708466, val accuracy: 0.9440559440559441\n",
      "91 epoch에서 저장. 이전 score: 0.15360844135284424, 현재 score: 0.15176765620708466\n",
      "[92/500] train loss: 0.11423806101083755, val loss: 0.15000854432582855, val accuracy: 0.9370629370629371\n",
      "92 epoch에서 저장. 이전 score: 0.15176765620708466, 현재 score: 0.15000854432582855\n",
      "[93/500] train loss: 0.11218267679214478, val loss: 0.1483021080493927, val accuracy: 0.9370629370629371\n",
      "93 epoch에서 저장. 이전 score: 0.15000854432582855, 현재 score: 0.1483021080493927\n",
      "[94/500] train loss: 0.11018980294466019, val loss: 0.1466447114944458, val accuracy: 0.9440559440559441\n",
      "94 epoch에서 저장. 이전 score: 0.1483021080493927, 현재 score: 0.1466447114944458\n",
      "[95/500] train loss: 0.10826019197702408, val loss: 0.1450347751379013, val accuracy: 0.9440559440559441\n",
      "95 epoch에서 저장. 이전 score: 0.1466447114944458, 현재 score: 0.1450347751379013\n",
      "[96/500] train loss: 0.10639115422964096, val loss: 0.1434737741947174, val accuracy: 0.9440559440559441\n",
      "96 epoch에서 저장. 이전 score: 0.1450347751379013, 현재 score: 0.1434737741947174\n",
      "[97/500] train loss: 0.10458438843488693, val loss: 0.14196275174617767, val accuracy: 0.9440559440559441\n",
      "97 epoch에서 저장. 이전 score: 0.1434737741947174, 현재 score: 0.14196275174617767\n",
      "[98/500] train loss: 0.10283213108778, val loss: 0.14050424098968506, val accuracy: 0.9440559440559441\n",
      "98 epoch에서 저장. 이전 score: 0.14196275174617767, 현재 score: 0.14050424098968506\n",
      "[99/500] train loss: 0.1011338084936142, val loss: 0.1390976905822754, val accuracy: 0.9440559440559441\n",
      "99 epoch에서 저장. 이전 score: 0.14050424098968506, 현재 score: 0.1390976905822754\n",
      "[100/500] train loss: 0.09948774427175522, val loss: 0.13773924112319946, val accuracy: 0.9440559440559441\n",
      "100 epoch에서 저장. 이전 score: 0.1390976905822754, 현재 score: 0.13773924112319946\n",
      "[101/500] train loss: 0.09789188206195831, val loss: 0.13643132150173187, val accuracy: 0.9440559440559441\n",
      "101 epoch에서 저장. 이전 score: 0.13773924112319946, 현재 score: 0.13643132150173187\n",
      "[102/500] train loss: 0.09634680300951004, val loss: 0.13516519963741302, val accuracy: 0.9440559440559441\n",
      "102 epoch에서 저장. 이전 score: 0.13643132150173187, 현재 score: 0.13516519963741302\n",
      "[103/500] train loss: 0.09484860301017761, val loss: 0.133941650390625, val accuracy: 0.9440559440559441\n",
      "103 epoch에서 저장. 이전 score: 0.13516519963741302, 현재 score: 0.133941650390625\n",
      "[104/500] train loss: 0.09339877218008041, val loss: 0.13275903463363647, val accuracy: 0.9440559440559441\n",
      "104 epoch에서 저장. 이전 score: 0.133941650390625, 현재 score: 0.13275903463363647\n",
      "[105/500] train loss: 0.09199247509241104, val loss: 0.13161741197109222, val accuracy: 0.9440559440559441\n",
      "105 epoch에서 저장. 이전 score: 0.13275903463363647, 현재 score: 0.13161741197109222\n",
      "[106/500] train loss: 0.09062326699495316, val loss: 0.1305115818977356, val accuracy: 0.9440559440559441\n",
      "106 epoch에서 저장. 이전 score: 0.13161741197109222, 현재 score: 0.1305115818977356\n",
      "[107/500] train loss: 0.0892949178814888, val loss: 0.12944316864013672, val accuracy: 0.9440559440559441\n",
      "107 epoch에서 저장. 이전 score: 0.1305115818977356, 현재 score: 0.12944316864013672\n",
      "[108/500] train loss: 0.08800533413887024, val loss: 0.1284121721982956, val accuracy: 0.9440559440559441\n",
      "108 epoch에서 저장. 이전 score: 0.12944316864013672, 현재 score: 0.1284121721982956\n",
      "[109/500] train loss: 0.08675232529640198, val loss: 0.12741559743881226, val accuracy: 0.9440559440559441\n",
      "109 epoch에서 저장. 이전 score: 0.1284121721982956, 현재 score: 0.12741559743881226\n",
      "[110/500] train loss: 0.08553752303123474, val loss: 0.1264399141073227, val accuracy: 0.9440559440559441\n",
      "110 epoch에서 저장. 이전 score: 0.12741559743881226, 현재 score: 0.1264399141073227\n",
      "[111/500] train loss: 0.08436000347137451, val loss: 0.12548528611660004, val accuracy: 0.9440559440559441\n",
      "111 epoch에서 저장. 이전 score: 0.1264399141073227, 현재 score: 0.12548528611660004\n",
      "[112/500] train loss: 0.08321160078048706, val loss: 0.12456762045621872, val accuracy: 0.9440559440559441\n",
      "112 epoch에서 저장. 이전 score: 0.12548528611660004, 현재 score: 0.12456762045621872\n",
      "[113/500] train loss: 0.08209630846977234, val loss: 0.12367632985115051, val accuracy: 0.9440559440559441\n",
      "113 epoch에서 저장. 이전 score: 0.12456762045621872, 현재 score: 0.12367632985115051\n",
      "[114/500] train loss: 0.08101564645767212, val loss: 0.12281032651662827, val accuracy: 0.9440559440559441\n",
      "114 epoch에서 저장. 이전 score: 0.12367632985115051, 현재 score: 0.12281032651662827\n",
      "[115/500] train loss: 0.07996633648872375, val loss: 0.1219840720295906, val accuracy: 0.9440559440559441\n",
      "115 epoch에서 저장. 이전 score: 0.12281032651662827, 현재 score: 0.1219840720295906\n",
      "[116/500] train loss: 0.07894615083932877, val loss: 0.12118730694055557, val accuracy: 0.9440559440559441\n",
      "116 epoch에서 저장. 이전 score: 0.1219840720295906, 현재 score: 0.12118730694055557\n",
      "[117/500] train loss: 0.07795578241348267, val loss: 0.12042113393545151, val accuracy: 0.9440559440559441\n",
      "117 epoch에서 저장. 이전 score: 0.12118730694055557, 현재 score: 0.12042113393545151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118/500] train loss: 0.07699336111545563, val loss: 0.11968172341585159, val accuracy: 0.9440559440559441\n",
      "118 epoch에서 저장. 이전 score: 0.12042113393545151, 현재 score: 0.11968172341585159\n",
      "[119/500] train loss: 0.07605697214603424, val loss: 0.11896611005067825, val accuracy: 0.9440559440559441\n",
      "119 epoch에서 저장. 이전 score: 0.11968172341585159, 현재 score: 0.11896611005067825\n",
      "[120/500] train loss: 0.07514457404613495, val loss: 0.11827803403139114, val accuracy: 0.9440559440559441\n",
      "120 epoch에서 저장. 이전 score: 0.11896611005067825, 현재 score: 0.11827803403139114\n",
      "[121/500] train loss: 0.07425709068775177, val loss: 0.11761629581451416, val accuracy: 0.9440559440559441\n",
      "121 epoch에서 저장. 이전 score: 0.11827803403139114, 현재 score: 0.11761629581451416\n",
      "[122/500] train loss: 0.07339339703321457, val loss: 0.11697711795568466, val accuracy: 0.9440559440559441\n",
      "122 epoch에서 저장. 이전 score: 0.11761629581451416, 현재 score: 0.11697711795568466\n",
      "[123/500] train loss: 0.07255279272794724, val loss: 0.11635945737361908, val accuracy: 0.9440559440559441\n",
      "123 epoch에서 저장. 이전 score: 0.11697711795568466, 현재 score: 0.11635945737361908\n",
      "[124/500] train loss: 0.071733258664608, val loss: 0.11576249450445175, val accuracy: 0.951048951048951\n",
      "124 epoch에서 저장. 이전 score: 0.11635945737361908, 현재 score: 0.11576249450445175\n",
      "[125/500] train loss: 0.07093600928783417, val loss: 0.11518385261297226, val accuracy: 0.951048951048951\n",
      "125 epoch에서 저장. 이전 score: 0.11576249450445175, 현재 score: 0.11518385261297226\n",
      "[126/500] train loss: 0.07015950232744217, val loss: 0.11462267488241196, val accuracy: 0.951048951048951\n",
      "126 epoch에서 저장. 이전 score: 0.11518385261297226, 현재 score: 0.11462267488241196\n",
      "[127/500] train loss: 0.06940316408872604, val loss: 0.1140759065747261, val accuracy: 0.951048951048951\n",
      "127 epoch에서 저장. 이전 score: 0.11462267488241196, 현재 score: 0.1140759065747261\n",
      "[128/500] train loss: 0.06866451352834702, val loss: 0.11355184018611908, val accuracy: 0.9440559440559441\n",
      "128 epoch에서 저장. 이전 score: 0.1140759065747261, 현재 score: 0.11355184018611908\n",
      "[129/500] train loss: 0.06794299930334091, val loss: 0.1130482479929924, val accuracy: 0.9440559440559441\n",
      "129 epoch에서 저장. 이전 score: 0.11355184018611908, 현재 score: 0.1130482479929924\n",
      "[130/500] train loss: 0.06723982095718384, val loss: 0.11256296932697296, val accuracy: 0.9440559440559441\n",
      "130 epoch에서 저장. 이전 score: 0.1130482479929924, 현재 score: 0.11256296932697296\n",
      "[131/500] train loss: 0.06655228137969971, val loss: 0.1120993047952652, val accuracy: 0.951048951048951\n",
      "131 epoch에서 저장. 이전 score: 0.11256296932697296, 현재 score: 0.1120993047952652\n",
      "[132/500] train loss: 0.06588134169578552, val loss: 0.11165688186883926, val accuracy: 0.951048951048951\n",
      "132 epoch에서 저장. 이전 score: 0.1120993047952652, 현재 score: 0.11165688186883926\n",
      "[133/500] train loss: 0.06522555649280548, val loss: 0.11122743785381317, val accuracy: 0.951048951048951\n",
      "133 epoch에서 저장. 이전 score: 0.11165688186883926, 현재 score: 0.11122743785381317\n",
      "[134/500] train loss: 0.06458354741334915, val loss: 0.11080661416053772, val accuracy: 0.951048951048951\n",
      "134 epoch에서 저장. 이전 score: 0.11122743785381317, 현재 score: 0.11080661416053772\n",
      "[135/500] train loss: 0.0639556348323822, val loss: 0.11039377748966217, val accuracy: 0.951048951048951\n",
      "135 epoch에서 저장. 이전 score: 0.11080661416053772, 현재 score: 0.11039377748966217\n",
      "[136/500] train loss: 0.063341423869133, val loss: 0.1099962666630745, val accuracy: 0.951048951048951\n",
      "136 epoch에서 저장. 이전 score: 0.11039377748966217, 현재 score: 0.1099962666630745\n",
      "[137/500] train loss: 0.06273900717496872, val loss: 0.10960644483566284, val accuracy: 0.951048951048951\n",
      "137 epoch에서 저장. 이전 score: 0.1099962666630745, 현재 score: 0.10960644483566284\n",
      "[138/500] train loss: 0.06215033307671547, val loss: 0.10922961682081223, val accuracy: 0.951048951048951\n",
      "138 epoch에서 저장. 이전 score: 0.10960644483566284, 현재 score: 0.10922961682081223\n",
      "[139/500] train loss: 0.061573419719934464, val loss: 0.10886784642934799, val accuracy: 0.951048951048951\n",
      "139 epoch에서 저장. 이전 score: 0.10922961682081223, 현재 score: 0.10886784642934799\n",
      "[140/500] train loss: 0.06100555136799812, val loss: 0.10852038115262985, val accuracy: 0.951048951048951\n",
      "140 epoch에서 저장. 이전 score: 0.10886784642934799, 현재 score: 0.10852038115262985\n",
      "[141/500] train loss: 0.06045008450746536, val loss: 0.10818631947040558, val accuracy: 0.951048951048951\n",
      "141 epoch에서 저장. 이전 score: 0.10852038115262985, 현재 score: 0.10818631947040558\n",
      "[142/500] train loss: 0.05990837141871452, val loss: 0.10786397755146027, val accuracy: 0.951048951048951\n",
      "142 epoch에서 저장. 이전 score: 0.10818631947040558, 현재 score: 0.10786397755146027\n",
      "[143/500] train loss: 0.059376250952482224, val loss: 0.1075524315237999, val accuracy: 0.951048951048951\n",
      "143 epoch에서 저장. 이전 score: 0.10786397755146027, 현재 score: 0.1075524315237999\n",
      "[144/500] train loss: 0.058853209018707275, val loss: 0.10725007206201553, val accuracy: 0.951048951048951\n",
      "144 epoch에서 저장. 이전 score: 0.1075524315237999, 현재 score: 0.10725007206201553\n",
      "[145/500] train loss: 0.05834012106060982, val loss: 0.10695715993642807, val accuracy: 0.951048951048951\n",
      "145 epoch에서 저장. 이전 score: 0.10725007206201553, 현재 score: 0.10695715993642807\n",
      "[146/500] train loss: 0.05783463269472122, val loss: 0.10667209327220917, val accuracy: 0.951048951048951\n",
      "146 epoch에서 저장. 이전 score: 0.10695715993642807, 현재 score: 0.10667209327220917\n",
      "[147/500] train loss: 0.057339735329151154, val loss: 0.10639012604951859, val accuracy: 0.951048951048951\n",
      "147 epoch에서 저장. 이전 score: 0.10667209327220917, 현재 score: 0.10639012604951859\n",
      "[148/500] train loss: 0.05685665085911751, val loss: 0.1061084046959877, val accuracy: 0.951048951048951\n",
      "148 epoch에서 저장. 이전 score: 0.10639012604951859, 현재 score: 0.1061084046959877\n",
      "[149/500] train loss: 0.05638883635401726, val loss: 0.10583480447530746, val accuracy: 0.951048951048951\n",
      "149 epoch에서 저장. 이전 score: 0.1061084046959877, 현재 score: 0.10583480447530746\n",
      "[150/500] train loss: 0.05592913180589676, val loss: 0.10557250678539276, val accuracy: 0.951048951048951\n",
      "150 epoch에서 저장. 이전 score: 0.10583480447530746, 현재 score: 0.10557250678539276\n",
      "[151/500] train loss: 0.05547776818275452, val loss: 0.10531154274940491, val accuracy: 0.951048951048951\n",
      "151 epoch에서 저장. 이전 score: 0.10557250678539276, 현재 score: 0.10531154274940491\n",
      "[152/500] train loss: 0.05503770336508751, val loss: 0.10505250096321106, val accuracy: 0.951048951048951\n",
      "152 epoch에서 저장. 이전 score: 0.10531154274940491, 현재 score: 0.10505250096321106\n",
      "[153/500] train loss: 0.0546070821583271, val loss: 0.10480388253927231, val accuracy: 0.951048951048951\n",
      "153 epoch에서 저장. 이전 score: 0.10505250096321106, 현재 score: 0.10480388253927231\n",
      "[154/500] train loss: 0.054185185581445694, val loss: 0.1045646220445633, val accuracy: 0.951048951048951\n",
      "154 epoch에서 저장. 이전 score: 0.10480388253927231, 현재 score: 0.1045646220445633\n",
      "[155/500] train loss: 0.053769927471876144, val loss: 0.10433671623468399, val accuracy: 0.951048951048951\n",
      "155 epoch에서 저장. 이전 score: 0.1045646220445633, 현재 score: 0.10433671623468399\n",
      "[156/500] train loss: 0.05336131155490875, val loss: 0.10411810874938965, val accuracy: 0.951048951048951\n",
      "156 epoch에서 저장. 이전 score: 0.10433671623468399, 현재 score: 0.10411810874938965\n",
      "[157/500] train loss: 0.05296146497130394, val loss: 0.10390516370534897, val accuracy: 0.951048951048951\n",
      "157 epoch에서 저장. 이전 score: 0.10411810874938965, 현재 score: 0.10390516370534897\n",
      "[158/500] train loss: 0.05256899073719978, val loss: 0.10369815677404404, val accuracy: 0.951048951048951\n",
      "158 epoch에서 저장. 이전 score: 0.10390516370534897, 현재 score: 0.10369815677404404\n",
      "[159/500] train loss: 0.05218326300382614, val loss: 0.10349702835083008, val accuracy: 0.951048951048951\n",
      "159 epoch에서 저장. 이전 score: 0.10369815677404404, 현재 score: 0.10349702835083008\n",
      "[160/500] train loss: 0.05180436372756958, val loss: 0.1033053770661354, val accuracy: 0.951048951048951\n",
      "160 epoch에서 저장. 이전 score: 0.10349702835083008, 현재 score: 0.1033053770661354\n",
      "[161/500] train loss: 0.05143033340573311, val loss: 0.10312126576900482, val accuracy: 0.951048951048951\n",
      "161 epoch에서 저장. 이전 score: 0.1033053770661354, 현재 score: 0.10312126576900482\n",
      "[162/500] train loss: 0.051062554121017456, val loss: 0.10295376926660538, val accuracy: 0.951048951048951\n",
      "162 epoch에서 저장. 이전 score: 0.10312126576900482, 현재 score: 0.10295376926660538\n",
      "[163/500] train loss: 0.05070202797651291, val loss: 0.10278472304344177, val accuracy: 0.951048951048951\n",
      "163 epoch에서 저장. 이전 score: 0.10295376926660538, 현재 score: 0.10278472304344177\n",
      "[164/500] train loss: 0.05034857615828514, val loss: 0.10261320322751999, val accuracy: 0.951048951048951\n",
      "164 epoch에서 저장. 이전 score: 0.10278472304344177, 현재 score: 0.10261320322751999\n",
      "[165/500] train loss: 0.050003260374069214, val loss: 0.10244273394346237, val accuracy: 0.958041958041958\n",
      "165 epoch에서 저장. 이전 score: 0.10261320322751999, 현재 score: 0.10244273394346237\n",
      "[166/500] train loss: 0.049663837999105453, val loss: 0.10227140784263611, val accuracy: 0.958041958041958\n",
      "166 epoch에서 저장. 이전 score: 0.10244273394346237, 현재 score: 0.10227140784263611\n",
      "[167/500] train loss: 0.0493280254304409, val loss: 0.10210022330284119, val accuracy: 0.958041958041958\n",
      "167 epoch에서 저장. 이전 score: 0.10227140784263611, 현재 score: 0.10210022330284119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168/500] train loss: 0.048997364938259125, val loss: 0.10193483531475067, val accuracy: 0.958041958041958\n",
      "168 epoch에서 저장. 이전 score: 0.10210022330284119, 현재 score: 0.10193483531475067\n",
      "[169/500] train loss: 0.048672404140233994, val loss: 0.10177493095397949, val accuracy: 0.958041958041958\n",
      "169 epoch에서 저장. 이전 score: 0.10193483531475067, 현재 score: 0.10177493095397949\n",
      "[170/500] train loss: 0.04835193604230881, val loss: 0.10161653161048889, val accuracy: 0.958041958041958\n",
      "170 epoch에서 저장. 이전 score: 0.10177493095397949, 현재 score: 0.10161653161048889\n",
      "[171/500] train loss: 0.04803624749183655, val loss: 0.10147170722484589, val accuracy: 0.958041958041958\n",
      "171 epoch에서 저장. 이전 score: 0.10161653161048889, 현재 score: 0.10147170722484589\n",
      "[172/500] train loss: 0.04772533103823662, val loss: 0.10133832693099976, val accuracy: 0.958041958041958\n",
      "172 epoch에서 저장. 이전 score: 0.10147170722484589, 현재 score: 0.10133832693099976\n",
      "[173/500] train loss: 0.04741829261183739, val loss: 0.10121806710958481, val accuracy: 0.958041958041958\n",
      "173 epoch에서 저장. 이전 score: 0.10133832693099976, 현재 score: 0.10121806710958481\n",
      "[174/500] train loss: 0.04711589589715004, val loss: 0.1011066734790802, val accuracy: 0.958041958041958\n",
      "174 epoch에서 저장. 이전 score: 0.10121806710958481, 현재 score: 0.1011066734790802\n",
      "[175/500] train loss: 0.04681887477636337, val loss: 0.10100044310092926, val accuracy: 0.958041958041958\n",
      "175 epoch에서 저장. 이전 score: 0.1011066734790802, 현재 score: 0.10100044310092926\n",
      "[176/500] train loss: 0.04652633145451546, val loss: 0.1008962020277977, val accuracy: 0.958041958041958\n",
      "176 epoch에서 저장. 이전 score: 0.10100044310092926, 현재 score: 0.1008962020277977\n",
      "[177/500] train loss: 0.04623718932271004, val loss: 0.10079333186149597, val accuracy: 0.958041958041958\n",
      "177 epoch에서 저장. 이전 score: 0.1008962020277977, 현재 score: 0.10079333186149597\n",
      "[178/500] train loss: 0.04595308378338814, val loss: 0.1006908044219017, val accuracy: 0.958041958041958\n",
      "178 epoch에서 저장. 이전 score: 0.10079333186149597, 현재 score: 0.1006908044219017\n",
      "[179/500] train loss: 0.045673541724681854, val loss: 0.10059027373790741, val accuracy: 0.958041958041958\n",
      "179 epoch에서 저장. 이전 score: 0.1006908044219017, 현재 score: 0.10059027373790741\n",
      "[180/500] train loss: 0.04539797827601433, val loss: 0.1004970595240593, val accuracy: 0.958041958041958\n",
      "180 epoch에서 저장. 이전 score: 0.10059027373790741, 현재 score: 0.1004970595240593\n",
      "[181/500] train loss: 0.04512660577893257, val loss: 0.10040809959173203, val accuracy: 0.958041958041958\n",
      "181 epoch에서 저장. 이전 score: 0.1004970595240593, 현재 score: 0.10040809959173203\n",
      "[182/500] train loss: 0.04485907405614853, val loss: 0.10032270848751068, val accuracy: 0.958041958041958\n",
      "182 epoch에서 저장. 이전 score: 0.10040809959173203, 현재 score: 0.10032270848751068\n",
      "[183/500] train loss: 0.04459487646818161, val loss: 0.1002451628446579, val accuracy: 0.958041958041958\n",
      "183 epoch에서 저장. 이전 score: 0.10032270848751068, 현재 score: 0.1002451628446579\n",
      "[184/500] train loss: 0.04433382675051689, val loss: 0.10017507523298264, val accuracy: 0.958041958041958\n",
      "184 epoch에서 저장. 이전 score: 0.1002451628446579, 현재 score: 0.10017507523298264\n",
      "[185/500] train loss: 0.04407664015889168, val loss: 0.10010966658592224, val accuracy: 0.958041958041958\n",
      "185 epoch에서 저장. 이전 score: 0.10017507523298264, 현재 score: 0.10010966658592224\n",
      "[186/500] train loss: 0.04382426291704178, val loss: 0.1000429093837738, val accuracy: 0.958041958041958\n",
      "186 epoch에서 저장. 이전 score: 0.10010966658592224, 현재 score: 0.1000429093837738\n",
      "[187/500] train loss: 0.04357663169503212, val loss: 0.09997768700122833, val accuracy: 0.958041958041958\n",
      "187 epoch에서 저장. 이전 score: 0.1000429093837738, 현재 score: 0.09997768700122833\n",
      "[188/500] train loss: 0.04333050921559334, val loss: 0.09991782158613205, val accuracy: 0.958041958041958\n",
      "188 epoch에서 저장. 이전 score: 0.09997768700122833, 현재 score: 0.09991782158613205\n",
      "[189/500] train loss: 0.04308667406439781, val loss: 0.0998658537864685, val accuracy: 0.958041958041958\n",
      "189 epoch에서 저장. 이전 score: 0.09991782158613205, 현재 score: 0.0998658537864685\n",
      "[190/500] train loss: 0.042845748364925385, val loss: 0.09981811791658401, val accuracy: 0.958041958041958\n",
      "190 epoch에서 저장. 이전 score: 0.0998658537864685, 현재 score: 0.09981811791658401\n",
      "[191/500] train loss: 0.04260759800672531, val loss: 0.09977618604898453, val accuracy: 0.958041958041958\n",
      "191 epoch에서 저장. 이전 score: 0.09981811791658401, 현재 score: 0.09977618604898453\n",
      "[192/500] train loss: 0.04237186536192894, val loss: 0.099739208817482, val accuracy: 0.958041958041958\n",
      "192 epoch에서 저장. 이전 score: 0.09977618604898453, 현재 score: 0.099739208817482\n",
      "[193/500] train loss: 0.04213934764266014, val loss: 0.09970688819885254, val accuracy: 0.958041958041958\n",
      "193 epoch에서 저장. 이전 score: 0.099739208817482, 현재 score: 0.09970688819885254\n",
      "[194/500] train loss: 0.04190975800156593, val loss: 0.09967406839132309, val accuracy: 0.958041958041958\n",
      "194 epoch에서 저장. 이전 score: 0.09970688819885254, 현재 score: 0.09967406839132309\n",
      "[195/500] train loss: 0.04168142005801201, val loss: 0.099640853703022, val accuracy: 0.958041958041958\n",
      "195 epoch에서 저장. 이전 score: 0.09967406839132309, 현재 score: 0.099640853703022\n",
      "[196/500] train loss: 0.04145466163754463, val loss: 0.09960520267486572, val accuracy: 0.958041958041958\n",
      "196 epoch에서 저장. 이전 score: 0.099640853703022, 현재 score: 0.09960520267486572\n",
      "[197/500] train loss: 0.04123084247112274, val loss: 0.09957604855298996, val accuracy: 0.958041958041958\n",
      "197 epoch에서 저장. 이전 score: 0.09960520267486572, 현재 score: 0.09957604855298996\n",
      "[198/500] train loss: 0.04101042076945305, val loss: 0.09954949468374252, val accuracy: 0.958041958041958\n",
      "198 epoch에서 저장. 이전 score: 0.09957604855298996, 현재 score: 0.09954949468374252\n",
      "[199/500] train loss: 0.04079205170273781, val loss: 0.09952592849731445, val accuracy: 0.958041958041958\n",
      "199 epoch에서 저장. 이전 score: 0.09954949468374252, 현재 score: 0.09952592849731445\n",
      "[200/500] train loss: 0.040575042366981506, val loss: 0.09950629621744156, val accuracy: 0.958041958041958\n",
      "200 epoch에서 저장. 이전 score: 0.09952592849731445, 현재 score: 0.09950629621744156\n",
      "[201/500] train loss: 0.04035961255431175, val loss: 0.0994907096028328, val accuracy: 0.958041958041958\n",
      "201 epoch에서 저장. 이전 score: 0.09950629621744156, 현재 score: 0.0994907096028328\n",
      "[202/500] train loss: 0.04014658182859421, val loss: 0.09947171062231064, val accuracy: 0.958041958041958\n",
      "202 epoch에서 저장. 이전 score: 0.0994907096028328, 현재 score: 0.09947171062231064\n",
      "[203/500] train loss: 0.0399351567029953, val loss: 0.09945220500230789, val accuracy: 0.958041958041958\n",
      "203 epoch에서 저장. 이전 score: 0.09947171062231064, 현재 score: 0.09945220500230789\n",
      "[204/500] train loss: 0.039726100862026215, val loss: 0.09943576902151108, val accuracy: 0.958041958041958\n",
      "204 epoch에서 저장. 이전 score: 0.09945220500230789, 현재 score: 0.09943576902151108\n",
      "[205/500] train loss: 0.0395202673971653, val loss: 0.09941818565130234, val accuracy: 0.958041958041958\n",
      "205 epoch에서 저장. 이전 score: 0.09943576902151108, 현재 score: 0.09941818565130234\n",
      "[206/500] train loss: 0.03931783512234688, val loss: 0.09939177334308624, val accuracy: 0.958041958041958\n",
      "206 epoch에서 저장. 이전 score: 0.09941818565130234, 현재 score: 0.09939177334308624\n",
      "[207/500] train loss: 0.039116572588682175, val loss: 0.09936781972646713, val accuracy: 0.958041958041958\n",
      "207 epoch에서 저장. 이전 score: 0.09939177334308624, 현재 score: 0.09936781972646713\n",
      "[208/500] train loss: 0.038917362689971924, val loss: 0.09934572875499725, val accuracy: 0.958041958041958\n",
      "208 epoch에서 저장. 이전 score: 0.09936781972646713, 현재 score: 0.09934572875499725\n",
      "[209/500] train loss: 0.038720306009054184, val loss: 0.09932729601860046, val accuracy: 0.958041958041958\n",
      "209 epoch에서 저장. 이전 score: 0.09934572875499725, 현재 score: 0.09932729601860046\n",
      "[210/500] train loss: 0.03852517530322075, val loss: 0.09931431710720062, val accuracy: 0.958041958041958\n",
      "210 epoch에서 저장. 이전 score: 0.09932729601860046, 현재 score: 0.09931431710720062\n",
      "[211/500] train loss: 0.03833168372511864, val loss: 0.09930556267499924, val accuracy: 0.958041958041958\n",
      "211 epoch에서 저장. 이전 score: 0.09931431710720062, 현재 score: 0.09930556267499924\n",
      "[212/500] train loss: 0.038139693439006805, val loss: 0.0992981418967247, val accuracy: 0.958041958041958\n",
      "212 epoch에서 저장. 이전 score: 0.09930556267499924, 현재 score: 0.0992981418967247\n",
      "[213/500] train loss: 0.03795060142874718, val loss: 0.0992850810289383, val accuracy: 0.958041958041958\n",
      "213 epoch에서 저장. 이전 score: 0.0992981418967247, 현재 score: 0.0992850810289383\n",
      "[214/500] train loss: 0.037762437015771866, val loss: 0.09926775097846985, val accuracy: 0.958041958041958\n",
      "214 epoch에서 저장. 이전 score: 0.0992850810289383, 현재 score: 0.09926775097846985\n",
      "[215/500] train loss: 0.03757527470588684, val loss: 0.09925342351198196, val accuracy: 0.958041958041958\n",
      "215 epoch에서 저장. 이전 score: 0.09926775097846985, 현재 score: 0.09925342351198196\n",
      "[216/500] train loss: 0.037388790398836136, val loss: 0.09923630952835083, val accuracy: 0.958041958041958\n",
      "216 epoch에서 저장. 이전 score: 0.09925342351198196, 현재 score: 0.09923630952835083\n",
      "[217/500] train loss: 0.03720305487513542, val loss: 0.09921950846910477, val accuracy: 0.958041958041958\n",
      "217 epoch에서 저장. 이전 score: 0.09923630952835083, 현재 score: 0.09921950846910477\n",
      "[218/500] train loss: 0.03701944276690483, val loss: 0.09920169413089752, val accuracy: 0.958041958041958\n",
      "218 epoch에서 저장. 이전 score: 0.09921950846910477, 현재 score: 0.09920169413089752\n",
      "[219/500] train loss: 0.036837127059698105, val loss: 0.09918463230133057, val accuracy: 0.958041958041958\n",
      "219 epoch에서 저장. 이전 score: 0.09920169413089752, 현재 score: 0.09918463230133057\n",
      "[220/500] train loss: 0.03665569797158241, val loss: 0.09916869550943375, val accuracy: 0.958041958041958\n",
      "220 epoch에서 저장. 이전 score: 0.09918463230133057, 현재 score: 0.09916869550943375\n",
      "[221/500] train loss: 0.03647704795002937, val loss: 0.09915491938591003, val accuracy: 0.958041958041958\n",
      "221 epoch에서 저장. 이전 score: 0.09916869550943375, 현재 score: 0.09915491938591003\n",
      "[222/500] train loss: 0.036299824714660645, val loss: 0.09914375841617584, val accuracy: 0.958041958041958\n",
      "222 epoch에서 저장. 이전 score: 0.09915491938591003, 현재 score: 0.09914375841617584\n",
      "[223/500] train loss: 0.03612533211708069, val loss: 0.0991290733218193, val accuracy: 0.958041958041958\n",
      "223 epoch에서 저장. 이전 score: 0.09914375841617584, 현재 score: 0.0991290733218193\n",
      "[224/500] train loss: 0.03595144301652908, val loss: 0.09911145269870758, val accuracy: 0.958041958041958\n",
      "224 epoch에서 저장. 이전 score: 0.0991290733218193, 현재 score: 0.09911145269870758\n",
      "[225/500] train loss: 0.03577827662229538, val loss: 0.09908793866634369, val accuracy: 0.958041958041958\n",
      "225 epoch에서 저장. 이전 score: 0.09911145269870758, 현재 score: 0.09908793866634369\n",
      "[226/500] train loss: 0.03560568764805794, val loss: 0.0990624725818634, val accuracy: 0.958041958041958\n",
      "226 epoch에서 저장. 이전 score: 0.09908793866634369, 현재 score: 0.0990624725818634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[227/500] train loss: 0.035434093326330185, val loss: 0.09903992712497711, val accuracy: 0.958041958041958\n",
      "227 epoch에서 저장. 이전 score: 0.0990624725818634, 현재 score: 0.09903992712497711\n",
      "[228/500] train loss: 0.035263221710920334, val loss: 0.09902159124612808, val accuracy: 0.958041958041958\n",
      "228 epoch에서 저장. 이전 score: 0.09903992712497711, 현재 score: 0.09902159124612808\n",
      "[229/500] train loss: 0.035092633217573166, val loss: 0.09900838881731033, val accuracy: 0.958041958041958\n",
      "229 epoch에서 저장. 이전 score: 0.09902159124612808, 현재 score: 0.09900838881731033\n",
      "[230/500] train loss: 0.03492285683751106, val loss: 0.09900025278329849, val accuracy: 0.958041958041958\n",
      "230 epoch에서 저장. 이전 score: 0.09900838881731033, 현재 score: 0.09900025278329849\n",
      "[231/500] train loss: 0.03475331887602806, val loss: 0.09899198263883591, val accuracy: 0.958041958041958\n",
      "231 epoch에서 저장. 이전 score: 0.09900025278329849, 현재 score: 0.09899198263883591\n",
      "[232/500] train loss: 0.0345846526324749, val loss: 0.09897935390472412, val accuracy: 0.958041958041958\n",
      "232 epoch에서 저장. 이전 score: 0.09899198263883591, 현재 score: 0.09897935390472412\n",
      "[233/500] train loss: 0.034417349845170975, val loss: 0.09895919263362885, val accuracy: 0.958041958041958\n",
      "233 epoch에서 저장. 이전 score: 0.09897935390472412, 현재 score: 0.09895919263362885\n",
      "[234/500] train loss: 0.034250546246767044, val loss: 0.09893390536308289, val accuracy: 0.958041958041958\n",
      "234 epoch에서 저장. 이전 score: 0.09895919263362885, 현재 score: 0.09893390536308289\n",
      "[235/500] train loss: 0.03408527001738548, val loss: 0.09891526401042938, val accuracy: 0.958041958041958\n",
      "235 epoch에서 저장. 이전 score: 0.09893390536308289, 현재 score: 0.09891526401042938\n",
      "[236/500] train loss: 0.03392065688967705, val loss: 0.09889964759349823, val accuracy: 0.958041958041958\n",
      "236 epoch에서 저장. 이전 score: 0.09891526401042938, 현재 score: 0.09889964759349823\n",
      "[237/500] train loss: 0.033756669610738754, val loss: 0.0988909974694252, val accuracy: 0.958041958041958\n",
      "237 epoch에서 저장. 이전 score: 0.09889964759349823, 현재 score: 0.0988909974694252\n",
      "[238/500] train loss: 0.033597007393836975, val loss: 0.09888578951358795, val accuracy: 0.958041958041958\n",
      "238 epoch에서 저장. 이전 score: 0.0988909974694252, 현재 score: 0.09888578951358795\n",
      "[239/500] train loss: 0.033440642058849335, val loss: 0.09887468814849854, val accuracy: 0.958041958041958\n",
      "239 epoch에서 저장. 이전 score: 0.09888578951358795, 현재 score: 0.09887468814849854\n",
      "[240/500] train loss: 0.03328609839081764, val loss: 0.09885844588279724, val accuracy: 0.958041958041958\n",
      "240 epoch에서 저장. 이전 score: 0.09887468814849854, 현재 score: 0.09885844588279724\n",
      "[241/500] train loss: 0.03313115984201431, val loss: 0.09883921593427658, val accuracy: 0.958041958041958\n",
      "241 epoch에서 저장. 이전 score: 0.09885844588279724, 현재 score: 0.09883921593427658\n",
      "[242/500] train loss: 0.032977212220430374, val loss: 0.09882315993309021, val accuracy: 0.958041958041958\n",
      "242 epoch에서 저장. 이전 score: 0.09883921593427658, 현재 score: 0.09882315993309021\n",
      "[243/500] train loss: 0.032823480665683746, val loss: 0.0988115668296814, val accuracy: 0.958041958041958\n",
      "243 epoch에서 저장. 이전 score: 0.09882315993309021, 현재 score: 0.0988115668296814\n",
      "[244/500] train loss: 0.03267057240009308, val loss: 0.09880611300468445, val accuracy: 0.958041958041958\n",
      "244 epoch에서 저장. 이전 score: 0.0988115668296814, 현재 score: 0.09880611300468445\n",
      "[245/500] train loss: 0.032518547028303146, val loss: 0.09880660474300385, val accuracy: 0.958041958041958\n",
      "[246/500] train loss: 0.032366763800382614, val loss: 0.09881240129470825, val accuracy: 0.958041958041958\n",
      "[247/500] train loss: 0.032216720283031464, val loss: 0.09881485998630524, val accuracy: 0.958041958041958\n",
      "[248/500] train loss: 0.03206644952297211, val loss: 0.09881042689085007, val accuracy: 0.958041958041958\n",
      "[249/500] train loss: 0.03191607818007469, val loss: 0.09879789501428604, val accuracy: 0.958041958041958\n",
      "249 epoch에서 저장. 이전 score: 0.09880611300468445, 현재 score: 0.09879789501428604\n",
      "[250/500] train loss: 0.031766217201948166, val loss: 0.09878537803888321, val accuracy: 0.958041958041958\n",
      "250 epoch에서 저장. 이전 score: 0.09879789501428604, 현재 score: 0.09878537803888321\n",
      "[251/500] train loss: 0.03162132203578949, val loss: 0.0987917110323906, val accuracy: 0.958041958041958\n",
      "[252/500] train loss: 0.03147832676768303, val loss: 0.0988042801618576, val accuracy: 0.958041958041958\n",
      "[253/500] train loss: 0.03133644163608551, val loss: 0.09882062673568726, val accuracy: 0.958041958041958\n",
      "[254/500] train loss: 0.031195322051644325, val loss: 0.09883804619312286, val accuracy: 0.958041958041958\n",
      "[255/500] train loss: 0.031055113300681114, val loss: 0.09885214269161224, val accuracy: 0.958041958041958\n",
      "[256/500] train loss: 0.03091530315577984, val loss: 0.0988602265715599, val accuracy: 0.958041958041958\n",
      "[257/500] train loss: 0.030775494873523712, val loss: 0.09886369854211807, val accuracy: 0.958041958041958\n",
      "[258/500] train loss: 0.030635247007012367, val loss: 0.0988638624548912, val accuracy: 0.958041958041958\n",
      "[259/500] train loss: 0.030497552827000618, val loss: 0.0988711565732956, val accuracy: 0.958041958041958\n",
      "[260/500] train loss: 0.03036182001233101, val loss: 0.09888461977243423, val accuracy: 0.958041958041958\n",
      "260 epoch에서 종료. 0.09878537803888321에서 성능이 개선되지 않음.\n",
      "1.9826130867004395초 걸림\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "### 학습\n",
    "N_EPOCH = 500\n",
    "train_loss_list, val_loss_list, val_acc_list = [], [], []\n",
    "\n",
    "model = BreastCancerModel().to(device)\n",
    "loss_fn = nn.BCELoss()  # 이진분류-positive확률출력모델의 loss-Binary Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# early stopping, 모델 저장을 위한 변수\n",
    "best_score = torch.inf\n",
    "save_model_path_bc = 'models/breast_cancer_best_model.pt'\n",
    "\n",
    "patience = 10 # 개선될때까지 몇번 기다릴지.\n",
    "trigger_cnt = 0 # 몇번째 기다렸는지 \n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(N_EPOCH):\n",
    "    #### 학습\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in wb_train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 추정                         \n",
    "        pred = model(X) # positive의 확률 - loss 계산\n",
    "        loss = loss_fn(pred, y)\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    # 학습 종료 -> train loss 평균\n",
    "    train_loss /= len(wb_train_loader)\n",
    "    #### 검증\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in wb_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            \n",
    "            # 추정\n",
    "            pred_val = model(X_val)  # positive의 확률  0.XXXX -> loss계산\n",
    "                        \n",
    "            #type(타입)=>타입변환. bool=>정수. True->1, False->0\n",
    "            pred_label = (pred_val >= 0.5).type(torch.int32) # accuracy 계산\n",
    "               \n",
    "            loss_val = loss_fn(pred_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "            val_acc +=  torch.sum(pred_label == y_val).item()\n",
    "        \n",
    "        val_loss /= len(wb_test_loader)\n",
    "        val_acc /= len(wb_test_loader.dataset)\n",
    "    \n",
    "    # 현재 epoch 학습/검증 종료\n",
    "    # 로그출력\n",
    "    print(f\"[{epoch+1}/{N_EPOCH}] train loss: {train_loss}, val loss: {val_loss}, val accuracy: {val_acc}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    #### early stopping, model 저장 처리.\n",
    "    if val_loss < best_score: # 성능 개선\n",
    "        print(f\"{epoch+1} epoch에서 저장. 이전 score: {best_score}, 현재 score: {val_loss}\")\n",
    "        best_score = val_loss\n",
    "        torch.save(model, save_model_path_bc)\n",
    "        trigger_cnt = 0\n",
    "    else: # 현재 epoch에서 성능 개선이 안됨.\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt: # 조기 종료\n",
    "            print(f\"{epoch+1} epoch에서 종료. {best_score}에서 성능이 개선되지 않음.\")\n",
    "            break\n",
    "\n",
    "e = time.time()\n",
    "print(f\"{e-s}초 걸림\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BreastCancerModel                        [100, 1]                  --\n",
       "├─Linear: 1-1                            [100, 32]                 992\n",
       "├─Linear: 1-2                            [100, 16]                 528\n",
       "├─Linear: 1-3                            [100, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 1,537\n",
       "Trainable params: 1,537\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, (100, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"LOSS\")\n",
    "plt.plot(train_loss_list, label=\"Train\")\n",
    "plt.plot(val_loss_list, label=\"Validation\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(val_acc_list)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BreastCancerModel(\n",
       "  (lr1): Linear(in_features=30, out_features=32, bias=True)\n",
       "  (lr2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (output): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "load_model = torch.load(save_model_path_bc)\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in wb_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "        # 추정\n",
    "        pred_val = load_model(X_val)  # positive의 확률  0.XXXX -> loss계산\n",
    "\n",
    "        #type(타입)=>타입변환. bool=>정수. True->1, False->0\n",
    "        pred_label = (pred_val >= 0.5).type(torch.int32) # accuracy 계산\n",
    "\n",
    "        loss_val = loss_fn(pred_val, y_val)\n",
    "        val_loss += loss_val.item()\n",
    "        val_acc +=  torch.sum(pred_label == y_val).item()\n",
    "\n",
    "    val_loss /= len(wb_test_loader)\n",
    "    val_acc /= len(wb_test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09878537803888321 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()  \u001b[38;5;66;03m# 이진분류-positive확률출력모델의 loss-Binary Cross Entropy Loss\u001b[39;00m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m result\u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwb_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwb_test_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m             \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43msave_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/wb_model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m             \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m             \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m             \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\딥러닝\\module\\train.py:145\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(train_loader, val_loader, model, loss_fn, optimizer, epochs, save_best_model, save_model_path, early_stopping, patience, device, mode)\u001b[0m\n\u001b[0;32m    143\u001b[0m s \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 145\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    148\u001b[0m         val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m test_binary_classification(val_loader, model, loss_fn, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\Desktop\\딥러닝\\module\\train.py:87\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer, device, mode)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#     size = len(dataloader.dataset) #총 데이터수\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m---> 87\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     88\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m     90\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "model = BreastCancerModel()\n",
    "loss_fn = nn.BCELoss()  # 이진분류-positive확률출력모델의 loss-Binary Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "result= train.fit(wb_train_loader,wb_test_loader,\n",
    "             model,\n",
    "             loss_fn,\n",
    "             optimizer,\n",
    "             100,\n",
    "             save_model_path='models/wb_model.pt',\n",
    "             patience=5,\n",
    "            device=device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfmnist_train_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m loss_fn2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      3\u001b[0m optimizer2\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model2\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m result2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmnist_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfmnist_test_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[43mloss_fn2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m             \u001b[49m\u001b[43moptimizer2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43msave_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/fmnist_model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m             \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\딥러닝\\module\\train.py:145\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(train_loader, val_loader, model, loss_fn, optimizer, epochs, save_best_model, save_model_path, early_stopping, patience, device, mode)\u001b[0m\n\u001b[0;32m    143\u001b[0m s \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 145\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    148\u001b[0m         val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m test_binary_classification(val_loader, model, loss_fn, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\Desktop\\딥러닝\\module\\train.py:87\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer, device, mode)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#     size = len(dataloader.dataset) #총 데이터수\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m---> 87\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     88\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m     90\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "model2 = FashionMNISTModel()\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "optimizer2= torch.optim.Adam(model2.parameters(),lr=0.001)\n",
    "result2 = train.fit(fmnist_train_loader,fmnist_test_loader,\n",
    "             model2,\n",
    "             loss_fn2,\n",
    "             optimizer2,\n",
    "             5,\n",
    "             save_model_path='models/fmnist_model.pt',\n",
    "             early_stopping=False,\n",
    "             device=device,\n",
    "             mode='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
