{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f3060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e4a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences= ['원숭이 엉덩이는 빨개',\n",
    "'빨가면 사과 사과는 맛있어', \n",
    "'맛있으면 바나나 바나나는 길어', \n",
    "'길으면 기차 기차는 빨라',\n",
    "'빠르면 비행기 비행기는 높아', \n",
    "'높으면 백두산',\n",
    "'더 높은 건 달',\n",
    "'더 높은 건 별',\n",
    "'달보다 별보다 더 높은 건', \n",
    "'부모님의 사랑']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0e2b9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences=[\"울퉁불퉁 멋진 몸매에\",\n",
    "# \"빠알간 옷을 입고\",\n",
    "# \"새콤달콤 향내 풍기는\",\n",
    "# \"멋쟁이 토마토 토마토\",\n",
    "# \"나는야 주스 될 거야\",\n",
    "# \"나는야 케첩 될 거야\",\n",
    "# \"나는야 춤을 출 거야\",\n",
    "# \"뽐내는 토마토 토마토\",\n",
    "# \"울퉁불퉁 멋진 몸매에\",\n",
    "# \"빠알간 옷을 입고\",\n",
    "# \"새콤달콤 향내 풍기는\",\n",
    "# \"멋쟁이 토마토 토마토\",\n",
    "# \"나는야 주스 될 거야\",\n",
    "# \"나는야 케첩 될 거야\",\n",
    "# \"나는야 춤을 출 거야\",\n",
    "# \"뽐내는 토마토 토마토\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedd2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67a015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "729b9e48",
   "metadata": {},
   "source": [
    "# DATASET 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984fd46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# class tomato_dataset(Dataset):\n",
    "#     def __init__(self,sentences):\n",
    "#         super().__init__()\n",
    "# #         내가 tomato dataset = sentences\n",
    "#         self.tomato=sentences\n",
    "#         self.features, self.targets = self.vocab()\n",
    "#         self.features = self.padding()\n",
    "#         self.features = torch.Tensor(self.features)\n",
    "#     def vocab(self):\n",
    "# #         print(\"vocab이야\")\n",
    "#         features=[]\n",
    "#         vocab_list=[]\n",
    "#         targets=[]\n",
    "#         lst = []\n",
    "#         for sen in self.tomato:\n",
    "#             vocab_list.extend(sen.split())\n",
    "#         vocab_list=set(vocab_list)\n",
    "#         num_dict={i:idx for i,idx in enumerate(vocab_list,start=1)}\n",
    "#         word_dict={idx:i for i,idx in enumerate(vocab_list,start=1)}\n",
    "#         for sen in self.tomato:\n",
    "#             sen = sen.split()\n",
    "#             feature = [word_dict[i] for i in sen[:-1]]\n",
    "#             features.append(feature)\n",
    "#             target = word_dict[sen[-1]]\n",
    "#             targets.append(target)\n",
    "# #             num_list = [word_dict[i] for i in sen]\n",
    "# #             lst.append(num_list)\n",
    "#         return features, torch.Tensor(targets)\n",
    "#     def padding(self):\n",
    "#         max_len = max(len(l) for l in self.features)\n",
    "#         for i in self.features:\n",
    "#             i.extend((max_len-len(i))*[0])\n",
    "#         return self.features\n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "#     def __getitem__(self,idx):\n",
    "#         return self.features[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86295334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab919e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d798c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b31695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "class M_dataset(Dataset):\n",
    "    def __init__(self,sentences):\n",
    "        self.rawdata = sentences # rawdata를 받음\n",
    "        self.d_list=self.make_list()\n",
    "        self.feature,self.target,self.vocab_size,self.num_dict,self.word_dict = self.vocab()\n",
    "        self.feature=self.padding()\n",
    "        self.feature=torch.LongTensor(self.feature)\n",
    "        self.target=torch.LongTensor(self.target)\n",
    "#         self.target= self.target.unsqueeze(dim=1)\n",
    "    def make_list(self):# list로 만들어줌\n",
    "        d_list=[]\n",
    "        for i in self.rawdata:\n",
    "            d_list.append(i.split())\n",
    "        return d_list # 리스트로 나오게 함\n",
    "    def vocab(self): # 단어장 만들기  기능 1. 조회 2 숫자로 변경 ,3문자로 변경\n",
    "        features=[]\n",
    "        targets=[]\n",
    "        vocab_list=[]\n",
    "        for i in self.rawdata:\n",
    "            vocab_list.extend(i.split())\n",
    "        vocab_list=set(vocab_list)\n",
    "        num_dict={i:idx for i,idx in enumerate(vocab_list,start=1)} # 숫자를 넣으면 문자로 나옴 \n",
    "        word_dict={idx:i for i,idx in enumerate(vocab_list,start=1)} #  문자를 넣으면 숫자로나옴\n",
    "        for sen in self.d_list:\n",
    "            feature = [word_dict[i] for i in sen[:-1]]\n",
    "            features.append(feature)\n",
    "            target = word_dict[sen[-1]]\n",
    "            targets.append(target)\n",
    "        return features,targets,vocab_list,num_dict,word_dict\n",
    "    def padding(self):\n",
    "        padding_lst=[]\n",
    "        max_len=(max(len(i) for i in self.feature))\n",
    "        for row in self.feature:\n",
    "            padding_lst.append([0]*(max_len - len(row))+ row)\n",
    "        return padding_lst\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.feature[idx],self.target[idx]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a504511",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=M_dataset(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aab08faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.num_dict)+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80cdd1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 갯수 =문장 갯수\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7582a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 7, 8])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.feature[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29297f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c0a2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 타입\n",
    "type(dataset.feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aeb1882f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 7, 8])\n",
      "torch.Size([4])\n",
      "tensor(22)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 shape 맟 데이터 값 확인\n",
    "print(dataset[0][0]) # feature\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1])\n",
    "print(dataset[0][1].shape) # target unsqueez로 shape 를 맞춰줌 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805328e5",
   "metadata": {},
   "source": [
    "# datasetloader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55088b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "song_dataloader=DataLoader(dataset=dataset,batch_size=1,drop_last=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f240bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf253a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd57cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fa3dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0, 0, 7, 8]]), tensor([22])]\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(song_dataloader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ace0b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '기차',\n",
       " 2: '바나나',\n",
       " 3: '높아',\n",
       " 4: '맛있으면',\n",
       " 5: '달',\n",
       " 6: '건',\n",
       " 7: '원숭이',\n",
       " 8: '엉덩이는',\n",
       " 9: '바나나는',\n",
       " 10: '더',\n",
       " 11: '길으면',\n",
       " 12: '비행기는',\n",
       " 13: '백두산',\n",
       " 14: '빨라',\n",
       " 15: '사과는',\n",
       " 16: '사과',\n",
       " 17: '사랑',\n",
       " 18: '별',\n",
       " 19: '높은',\n",
       " 20: '달보다',\n",
       " 21: '별보다',\n",
       " 22: '빨개',\n",
       " 23: '기차는',\n",
       " 24: '맛있어',\n",
       " 25: '빨가면',\n",
       " 26: '비행기',\n",
       " 27: '빠르면',\n",
       " 28: '길어',\n",
       " 29: '높으면',\n",
       " 30: '부모님의'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ade008e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "tensor([[0, 0, 7, 8]])\n",
      "torch.Size([1])\n",
      "tensor([22])\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print(next(iter(song_dataloader))[0].shape) # batch, feature\n",
    "print(next(iter(song_dataloader))[0])# feature data\n",
    "print(next(iter(song_dataloader))[1].shape) # batch, target\n",
    "print(next(iter(song_dataloader))[1]) # target data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f099d",
   "metadata": {},
   "source": [
    "# CNN 모델 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "647d7e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터의 사전 사이즈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3c6036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "import torch.nn as nn\n",
    "class CNN_model(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size=64):\n",
    "        super().__init__()\n",
    "        self.embediing= torch.nn.Embedding(vocab_size,embed_size) # 임베딩할 사이즈\n",
    "        self.seq= torch.nn.Sequential(\n",
    "        torch.nn.Conv1d(in_channels=embed_size,out_channels=embed_size*2,kernel_size=2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool1d(kernel_size=1),\n",
    "        torch.nn.Conv1d(in_channels=embed_size*2,out_channels=embed_size*4,kernel_size=2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool1d(kernel_size=1),\n",
    "        torch.nn.AdaptiveAvgPool1d(1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(in_features=embed_size*4,out_features=vocab_size)\n",
    "         )\n",
    "    def forward(self,x):\n",
    "        emb_layout=self.embediing(x)\n",
    "        emb_per_layout=emb_layout.permute(0,2,1)\n",
    "        predict=self.seq(emb_per_layout)\n",
    "        return predict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "21700c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class CNN_model2(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_size=64):\n",
    "#         super().__init__()\n",
    "#         self.embedding = torch.nn.Embedding(vocab_size, embed_size)  # 임베딩할 사이즈\n",
    "#         self.con = torch.nn.Conv1d(in_channels=embed_size, out_channels=embed_size * 2, kernel_size=2)\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "#         self.maxpool = torch.nn.MaxPool1d(kernel_size=1)\n",
    "#         self.adapt = torch.nn.AdaptiveAvgPool1d(1)\n",
    "#         self.flatten = torch.nn.Flatten()\n",
    "#         self.linear = torch.nn.Linear(embed_size * 2, vocab_size)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         emb_layout = self.embedding(x)\n",
    "#         emb_per_layout = emb_layout.permute(0, 2, 1)\n",
    "#         con_layout = self.con(emb_per_layout)\n",
    "#         relu_layout = self.relu(con_layout)\n",
    "#         mxp_layout = self.maxpool(relu_layout)\n",
    "#         adp_layout = self.adapt(mxp_layout)\n",
    "#         flt_layout = self.flatten(adp_layout)\n",
    "#         ln_layout = self.linear(flt_layout)\n",
    "#         return ln_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "28bae78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc8a1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_tomato=CNN_model(len(dataset.vocab_size)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc228230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_model(\n",
       "  (embediing): Embedding(31, 64)\n",
       "  (seq): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(2,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(128, 256, kernel_size=(2,), stride=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): AdaptiveAvgPool1d(output_size=1)\n",
       "    (7): Flatten(start_dim=1, end_dim=-1)\n",
       "    (8): Linear(in_features=256, out_features=31, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_tomato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1d75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc11d825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "tensor([ 0,  0, 28,  6])\n",
      "torch.Size([1, 4])\n",
      "tensor([ 0, 12, 26, 19])\n",
      "torch.Size([1, 4])\n",
      "tensor([ 0, 25, 29,  5])\n",
      "torch.Size([1, 4])\n",
      "tensor([ 0, 27, 30, 24])\n",
      "torch.Size([1, 4])\n",
      "tensor([ 0,  8, 13, 15])\n",
      "torch.Size([1, 4])\n",
      "tensor([ 0,  0,  0, 14])\n",
      "torch.Size([1, 4])\n",
      "tensor([ 0, 16, 18,  3])\n",
      "torch.Size([1, 4])\n",
      "tensor([ 0, 16, 18,  3])\n",
      "torch.Size([1, 4])\n",
      "tensor([17,  1, 16, 18])\n",
      "torch.Size([1, 4])\n",
      "tensor([ 0,  0,  0, 20])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a282efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "tensor([[ 0,  0, 28,  6]])\n",
      "torch.Size([1, 4])\n",
      "tensor([[ 0, 12, 26, 19]])\n",
      "torch.Size([1, 4])\n",
      "tensor([[ 0, 25, 29,  5]])\n",
      "torch.Size([1, 4])\n",
      "tensor([[ 0, 27, 30, 24]])\n",
      "torch.Size([1, 4])\n",
      "tensor([[ 0,  8, 13, 15]])\n",
      "torch.Size([1, 4])\n",
      "tensor([[ 0,  0,  0, 14]])\n",
      "torch.Size([1, 4])\n",
      "tensor([[ 0, 16, 18,  3]])\n",
      "torch.Size([1, 4])\n",
      "tensor([[ 0, 16, 18,  3]])\n",
      "torch.Size([1, 4])\n",
      "tensor([[17,  1, 16, 18]])\n",
      "torch.Size([1, 4])\n",
      "tensor([[ 0,  0,  0, 20]])\n"
     ]
    }
   ],
   "source": [
    "for i in song_dataloader:\n",
    "    print(i[0].shape)\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c9e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8e8057b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn=CNN_model(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45437707",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test=CNN_model(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "14ca5199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_model(\n",
       "  (embediing): Embedding(31, 64)\n",
       "  (seq): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(2,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(128, 256, kernel_size=(2,), stride=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): AdaptiveAvgPool1d(output_size=1)\n",
       "    (7): Flatten(start_dim=1, end_dim=-1)\n",
       "    (8): Linear(in_features=256, out_features=31, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 모형 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb01f2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_model(\n",
       "  (embediing): Embedding(31, 64)\n",
       "  (seq): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(2,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(128, 256, kernel_size=(2,), stride=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): AdaptiveAvgPool1d(output_size=1)\n",
       "    (7): Flatten(start_dim=1, end_dim=-1)\n",
       "    (8): Linear(in_features=256, out_features=31, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c0d38033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_model2(\n",
       "  (embedding): Embedding(30, 64)\n",
       "  (con): Conv1d(64, 128, kernel_size=(2,), stride=(1,))\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (adapt): AdaptiveAvgPool1d(output_size=1)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear): Linear(in_features=128, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_tomato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ba3cb",
   "metadata": {},
   "source": [
    "# 학습 함수 생성연습!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e1eaf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_tomato.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a359ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 7, 8]])\n",
      "tensor([22])\n",
      "tensor([[ 0, 25, 16, 15]])\n",
      "tensor([24])\n",
      "tensor([[0, 4, 2, 9]])\n",
      "tensor([28])\n",
      "tensor([[ 0, 11,  1, 23]])\n",
      "tensor([14])\n",
      "tensor([[ 0, 27, 26, 12]])\n",
      "tensor([3])\n",
      "tensor([[ 0,  0,  0, 29]])\n",
      "tensor([13])\n",
      "tensor([[ 0, 10, 19,  6]])\n",
      "tensor([5])\n",
      "tensor([[ 0, 10, 19,  6]])\n",
      "tensor([18])\n",
      "tensor([[20, 21, 10, 19]])\n",
      "tensor([6])\n",
      "tensor([[ 0,  0,  0, 30]])\n",
      "tensor([17])\n"
     ]
    }
   ],
   "source": [
    "for batch in song_dataloader:\n",
    "    print(batch[0]) # batch,feature\n",
    "    print(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca4e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88ac0786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0579, -0.0105,  0.1494,  0.1559,  0.0196, -0.0507, -0.0078,  0.1317,\n",
      "          0.0235,  0.0528, -0.1550,  0.1250, -0.0038,  0.0360, -0.0297, -0.0188,\n",
      "         -0.0981, -0.1216,  0.2079,  0.1450,  0.0376, -0.0171, -0.0475, -0.2102,\n",
      "         -0.0959,  0.0643, -0.0832, -0.0305,  0.0309,  0.0529, -0.0873]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0550, -0.0285,  0.1629,  0.0671, -0.0176, -0.0135,  0.0097,  0.0506,\n",
      "         -0.0950, -0.0898, -0.0617,  0.0254, -0.0150,  0.1140,  0.0693, -0.1271,\n",
      "         -0.0286, -0.0828, -0.0151,  0.1872, -0.0222,  0.2305, -0.0958, -0.0958,\n",
      "         -0.1346,  0.0927, -0.1098, -0.0445,  0.1419,  0.0872, -0.1431]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0617,  0.0350,  0.0965,  0.0913,  0.0068, -0.0205,  0.0051,  0.0489,\n",
      "          0.0360, -0.0121, -0.0690,  0.0382,  0.0134,  0.0168, -0.0046, -0.1467,\n",
      "         -0.0602, -0.0928,  0.0326,  0.1731, -0.0411,  0.0713, -0.1201, -0.1711,\n",
      "         -0.1608,  0.0981, -0.1146, -0.0739,  0.0018,  0.0321, -0.0277]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0175,  0.0802,  0.1389,  0.0844, -0.0608,  0.0572, -0.0062,  0.0362,\n",
      "         -0.0621, -0.0054, -0.1626,  0.1488, -0.0272,  0.1052, -0.0096, -0.2469,\n",
      "         -0.1086, -0.0347,  0.0477,  0.2538,  0.0268,  0.0442, -0.1158, -0.0935,\n",
      "         -0.1332, -0.0397, -0.0814, -0.0503,  0.0788, -0.0629, -0.0582]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.8709e-02,  8.3431e-02,  1.4464e-01,  4.8989e-02, -1.0760e-01,\n",
      "         -8.4188e-03, -3.8908e-02,  7.9333e-02,  6.0478e-02, -7.8375e-02,\n",
      "         -1.5808e-01,  7.7695e-02,  6.7330e-02,  5.4098e-02,  6.9001e-03,\n",
      "         -1.8671e-01, -1.4695e-01, -1.2278e-01,  1.3100e-01,  1.7058e-01,\n",
      "          7.0568e-05,  9.1301e-02, -2.3569e-02, -1.4527e-01, -1.5656e-01,\n",
      "         -6.0548e-03, -3.4733e-02,  3.0956e-02,  7.1499e-02,  5.0879e-02,\n",
      "         -1.3158e-01]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0321,  0.0078,  0.1621,  0.1362,  0.0478, -0.0427, -0.0187,  0.1582,\n",
      "          0.1151,  0.0636, -0.2348,  0.1136,  0.0585,  0.0246, -0.0014, -0.0213,\n",
      "         -0.0487, -0.0656,  0.0963,  0.1273,  0.0808,  0.0391, -0.1245, -0.2307,\n",
      "         -0.1112,  0.0774, -0.0839, -0.0951,  0.0148,  0.0725, -0.1182]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0853,  0.1232,  0.0681,  0.1892,  0.0533,  0.0015,  0.0322,  0.0612,\n",
      "          0.0320,  0.0428, -0.1499,  0.0136,  0.0103,  0.0685,  0.0256, -0.0873,\n",
      "         -0.0908, -0.0363,  0.1136,  0.1835,  0.1297,  0.1430, -0.0421, -0.1797,\n",
      "         -0.1231,  0.0345, -0.1260,  0.1175,  0.0983,  0.0029, -0.0927]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0853,  0.1232,  0.0681,  0.1892,  0.0533,  0.0015,  0.0322,  0.0612,\n",
      "          0.0320,  0.0428, -0.1499,  0.0136,  0.0103,  0.0685,  0.0256, -0.0873,\n",
      "         -0.0908, -0.0363,  0.1136,  0.1835,  0.1297,  0.1430, -0.0421, -0.1797,\n",
      "         -0.1231,  0.0345, -0.1260,  0.1175,  0.0983,  0.0029, -0.0927]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1532,  0.0963,  0.1009,  0.1476,  0.0054,  0.0315,  0.0102,  0.0667,\n",
      "         -0.0225,  0.0953, -0.0823, -0.0398,  0.0618,  0.0251,  0.0344, -0.2267,\n",
      "         -0.1416,  0.0683,  0.0627,  0.1746,  0.1028,  0.1080, -0.0603, -0.1454,\n",
      "         -0.0372, -0.0393, -0.0781, -0.0007,  0.2063, -0.0166, -0.1007]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0502, -0.0400,  0.2023,  0.1009,  0.0030, -0.0253, -0.0255,  0.1539,\n",
      "          0.1039,  0.0200, -0.2476,  0.0862,  0.0446,  0.0125, -0.0049,  0.0473,\n",
      "          0.0039, -0.0906,  0.0917,  0.0796,  0.0713,  0.0548, -0.1737, -0.2502,\n",
      "         -0.1392,  0.0827, -0.0815, -0.0585,  0.0418,  0.1260, -0.0594]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 예측값 생성\n",
    "for batch in song_dataloader:\n",
    "    pred = cnn_tomato(batch[0])\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db008b23",
   "metadata": {},
   "source": [
    "# 학습 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25230fcb",
   "metadata": {},
   "source": [
    "# CNN_test 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b380a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.7653e-02, -8.9911e-02,  9.8459e-02,  9.5206e-02,  1.3939e-02,\n",
      "          7.9086e-02, -4.0915e-02,  1.1861e-01,  5.5216e-02, -9.4182e-02,\n",
      "          1.0842e-01,  2.2270e-02,  1.0716e-02, -1.6130e-01, -1.2937e-01,\n",
      "         -1.1837e-04, -3.3651e-02, -4.5009e-02,  3.6891e-02, -1.4616e-01,\n",
      "          9.5682e-02,  6.0766e-02,  6.0504e-02, -3.7315e-03,  5.4990e-02,\n",
      "          1.2812e-01,  1.2851e-01, -9.9503e-02, -8.7652e-02, -2.0280e-02,\n",
      "          1.5248e-01]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0336, -0.0682,  0.0912,  0.0896,  0.0196,  0.1011, -0.0373,  0.0814,\n",
      "          0.0634, -0.0105,  0.1415,  0.0724, -0.1030, -0.1128, -0.1410, -0.0771,\n",
      "         -0.0308,  0.0011,  0.0024, -0.2456,  0.0882,  0.0325,  0.0105, -0.0594,\n",
      "          0.0740,  0.1200,  0.1650, -0.0760, -0.0628,  0.0224,  0.2337]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0699, -0.0789,  0.1256,  0.0565,  0.0127,  0.1339,  0.0661, -0.0065,\n",
      "          0.0505, -0.0499,  0.1204,  0.0716, -0.0065, -0.1302, -0.1179,  0.0092,\n",
      "          0.0529,  0.0483,  0.0898, -0.2025,  0.1516,  0.1024,  0.0395, -0.0509,\n",
      "          0.0752,  0.1088,  0.0836, -0.0851, -0.1083,  0.0187,  0.1757]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0087, -0.1139,  0.0670,  0.0947,  0.0226, -0.0356, -0.0156,  0.0818,\n",
      "          0.0361, -0.0296,  0.1467,  0.0801,  0.0027, -0.1096, -0.1686, -0.0525,\n",
      "          0.0837, -0.0026,  0.0130, -0.1337,  0.0906,  0.0480, -0.0258, -0.0323,\n",
      "          0.0942,  0.0985,  0.1045, -0.1118, -0.0482, -0.0312,  0.1774]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0056, -0.0641,  0.1122,  0.0864, -0.0321,  0.0656,  0.0171,  0.0427,\n",
      "          0.0599, -0.1012,  0.0831,  0.0268, -0.0607, -0.0521, -0.1082, -0.0644,\n",
      "          0.0347, -0.0038,  0.0248, -0.2497,  0.0824,  0.0550, -0.0352, -0.1012,\n",
      "          0.1235,  0.0458,  0.1271, -0.0768, -0.0778,  0.0069,  0.2517]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0043, -0.0100,  0.0926, -0.0143,  0.0700,  0.1564,  0.0385,  0.1100,\n",
      "          0.0278, -0.1486,  0.0940, -0.0085, -0.0695, -0.1159, -0.0564,  0.0652,\n",
      "         -0.0191, -0.0683, -0.0463, -0.1429,  0.1413,  0.1147,  0.0240, -0.0598,\n",
      "          0.1228,  0.1368,  0.1554, -0.1159, -0.1083,  0.0345,  0.1160]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0818, -0.0658,  0.0034,  0.0631, -0.0694,  0.0613, -0.0149,  0.0239,\n",
      "          0.0260,  0.0411,  0.1662,  0.0901, -0.0121, -0.0365, -0.1943, -0.1689,\n",
      "          0.0645,  0.0263, -0.0234, -0.2695,  0.0190,  0.0961, -0.0020, -0.0497,\n",
      "          0.1243,  0.0363,  0.0551, -0.0537,  0.0073,  0.0083,  0.2410]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0818, -0.0658,  0.0034,  0.0631, -0.0694,  0.0613, -0.0149,  0.0239,\n",
      "          0.0260,  0.0411,  0.1662,  0.0901, -0.0121, -0.0365, -0.1943, -0.1689,\n",
      "          0.0645,  0.0263, -0.0234, -0.2695,  0.0190,  0.0961, -0.0020, -0.0497,\n",
      "          0.1243,  0.0363,  0.0551, -0.0537,  0.0073,  0.0083,  0.2410]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0707, -0.0286,  0.0737,  0.0308, -0.0573,  0.0066, -0.0868,  0.0659,\n",
      "          0.0619, -0.0329,  0.1077,  0.0657,  0.0080, -0.1044, -0.2305, -0.0966,\n",
      "          0.0323, -0.0285,  0.0158, -0.2136,  0.0387,  0.0954, -0.0840, -0.0580,\n",
      "          0.1444,  0.0593,  0.0093,  0.0308, -0.0025, -0.0278,  0.2307]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0448, -0.0640,  0.0797, -0.0287,  0.0351,  0.1522,  0.0477,  0.0851,\n",
      "          0.0411, -0.1615,  0.0827,  0.0519, -0.0725, -0.1321, -0.0676,  0.0724,\n",
      "         -0.0454, -0.0729, -0.0687, -0.1549,  0.1328,  0.0777,  0.0070, -0.1073,\n",
      "          0.1193,  0.1000,  0.1297, -0.1084, -0.0726,  0.0027,  0.1285]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 안됐을때 첫번째 가중치 값\n",
    "for batch in song_dataloader:\n",
    "    pred=cnn_test(batch[0])\n",
    "    print(pred)\n",
    "# tensor([[-0.1006,  0.2013,  0.0840, -0.0325, -0.0057, -0.0947, -0.2215, -0.0292,\n",
    "#          -0.1764, -0.1405,  0.0259,  0.0024,  0.1675,  0.1995,  0.1056,  0.0358,\n",
    "#          -0.0489,  0.0182, -0.0653,  0.0774, -0.0193, -0.0842,  0.0782, -0.2135,\n",
    "#          -0.0858, -0.0419, -0.0068, -0.1099,  0.2063,  0.2383]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-0.0162,  0.1080,  0.0934, -0.0705,  0.0156, -0.0783, -0.1610, -0.0623,\n",
    "#          -0.1395, -0.1657, -0.0897, -0.0840,  0.2818,  0.1920,  0.0999,  0.0571,\n",
    "#           0.0186,  0.0925,  0.0575,  0.1495, -0.0355, -0.0125,  0.1099, -0.0525,\n",
    "#          -0.0314, -0.0151, -0.0636, -0.0309,  0.1571,  0.2258]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-0.1054,  0.1119,  0.0276, -0.1349,  0.0264, -0.0343, -0.1587,  0.0468,\n",
    "#          -0.0795, -0.1959,  0.0434, -0.1265,  0.1626,  0.1462,  0.1562,  0.0207,\n",
    "#          -0.0510,  0.0132,  0.0072,  0.1254, -0.0243, -0.0560,  0.0494, -0.2558,\n",
    "#          -0.0777,  0.0215,  0.0452, -0.1682,  0.2300,  0.1484]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-0.0877,  0.1738,  0.1382, -0.0935,  0.0149, -0.0675, -0.2082,  0.0362,\n",
    "#          -0.1536, -0.1560, -0.0277,  0.1045,  0.0932,  0.1171,  0.0771,  0.0212,\n",
    "#           0.0094, -0.0504, -0.0671,  0.1572, -0.0743, -0.0304,  0.0242, -0.1903,\n",
    "#          -0.0943,  0.0201,  0.0561, -0.1253,  0.2441,  0.1974]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-0.1537,  0.0831,  0.1728, -0.1309,  0.1317, -0.0192, -0.1152,  0.0735,\n",
    "#          -0.1399, -0.2410, -0.0873, -0.0464,  0.1653,  0.1510,  0.1389,  0.1191,\n",
    "#           0.0731, -0.0129, -0.0532,  0.1519,  0.0109, -0.0876,  0.0070, -0.2428,\n",
    "#          -0.0755,  0.1125,  0.0120, -0.2078,  0.2550,  0.1923]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-0.0940,  0.2204,  0.0004, -0.0840, -0.0749, -0.1476, -0.1475,  0.1026,\n",
    "#          -0.1360, -0.1995,  0.0102, -0.0585,  0.1790,  0.2276,  0.2529, -0.0118,\n",
    "#          -0.0606, -0.0329, -0.0124,  0.0761, -0.0223, -0.1117,  0.0952, -0.1747,\n",
    "#          -0.1136, -0.0915,  0.1388, -0.1651,  0.2767,  0.1861]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-0.0749,  0.1332,  0.1033, -0.1811,  0.1140, -0.0176, -0.1315,  0.0848,\n",
    "#          -0.1982, -0.2357,  0.0210, -0.0237,  0.1576,  0.2189,  0.1145,  0.0340,\n",
    "#           0.0145,  0.0746, -0.0870,  0.1150,  0.0384, -0.0558,  0.0223, -0.1974,\n",
    "#          -0.0289,  0.0123, -0.0637, -0.0863,  0.1398,  0.1191]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-0.0749,  0.1332,  0.1033, -0.1811,  0.1140, -0.0176, -0.1315,  0.0848,\n",
    "#          -0.1982, -0.2357,  0.0210, -0.0237,  0.1576,  0.2189,  0.1145,  0.0340,\n",
    "#           0.0145,  0.0746, -0.0870,  0.1150,  0.0384, -0.0558,  0.0223, -0.1974,\n",
    "#          -0.0289,  0.0123, -0.0637, -0.0863,  0.1398,  0.1191]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-0.1376,  0.1419, -0.0074, -0.1412,  0.1193, -0.0338, -0.1675,  0.0452,\n",
    "#          -0.1503, -0.1819, -0.0164, -0.0429,  0.1626,  0.2050,  0.0722,  0.1273,\n",
    "#           0.0549,  0.1271, -0.0181,  0.0805, -0.0563, -0.0942,  0.0669, -0.1195,\n",
    "#          -0.0875, -0.0589, -0.0418, -0.0906,  0.0583,  0.1544]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-0.1031,  0.2248,  0.0159, -0.0690, -0.0385, -0.0974, -0.1700,  0.0919,\n",
    "#          -0.1887, -0.1862,  0.0818, -0.0572,  0.1531,  0.2132,  0.2202,  0.0023,\n",
    "#          -0.0516,  0.0270,  0.0032,  0.0020, -0.0705, -0.1498,  0.0885, -0.2283,\n",
    "#          -0.1225, -0.0916,  0.1155, -0.2187,  0.2301,  0.1568]],\n",
    "#        grad_fn=<AddmmBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "674390ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 cnn_test 손실함수,옵티마이저 생성\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_test.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2f1905ad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_10916\\184712336.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(pred,torch.tensor(batch[1], dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "34.901820\n",
      "1\n",
      "27.354216\n",
      "2\n",
      "20.213984\n",
      "3\n",
      "13.039364\n",
      "4\n",
      "7.808337\n",
      "5\n",
      "4.908771\n",
      "6\n",
      "3.404972\n",
      "7\n",
      "2.687332\n",
      "8\n",
      "2.275398\n",
      "9\n",
      "2.034171\n",
      "10\n",
      "1.888735\n",
      "11\n",
      "1.798252\n",
      "12\n",
      "1.737492\n",
      "13\n",
      "1.695799\n",
      "14\n",
      "1.665601\n",
      "15\n",
      "1.642969\n",
      "16\n",
      "1.625811\n",
      "17\n",
      "1.611308\n",
      "18\n",
      "1.599911\n",
      "19\n",
      "1.589641\n",
      "20\n",
      "1.581545\n",
      "21\n",
      "1.574062\n",
      "22\n",
      "1.567471\n",
      "23\n",
      "1.561040\n",
      "24\n",
      "1.556111\n",
      "25\n",
      "1.551044\n",
      "26\n",
      "1.546809\n",
      "27\n",
      "1.542992\n",
      "28\n",
      "1.539189\n",
      "29\n",
      "1.536236\n",
      "30\n",
      "1.532497\n",
      "31\n",
      "1.529639\n",
      "32\n",
      "1.527150\n",
      "33\n",
      "1.524597\n",
      "34\n",
      "1.521944\n",
      "35\n",
      "1.519502\n",
      "36\n",
      "1.517290\n",
      "37\n",
      "1.515331\n",
      "38\n",
      "1.513213\n",
      "39\n",
      "1.511371\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH=1\n",
    "pred_num=0\n",
    "for epoch in range(N_EPOCH):\n",
    "\n",
    "    cnn_test.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in song_dataloader:\n",
    "        pred = cnn_test(batch[0]) # feature 를 모델에 넣음\n",
    "        print(\"#\"*20)\n",
    "        print('정답'+pred)\n",
    "        print(\"실제 값\"+batch[1])\n",
    "        if pred.argmax(dim=-1)==batch[1]:\n",
    "            pred_num+=1\n",
    "        loss = loss_fn(pred,torch.tensor(batch[1], dtype=torch.long))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        print()\n",
    "#     iepoch%10==0:\n",
    "#     print(f\"{epoch}\")\n",
    "#     print(f\"{epoch_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5343f94c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -2.7031, -11.9725,  -2.4879, -11.9993, -11.9782,  -2.8094, -13.3631,\n",
      "         -13.2695, -12.6752, -11.4285,  -5.0202, -12.0012, -10.2563, -11.4767,\n",
      "         -10.7292, -12.1461,   5.8633,  -2.8378, -12.1131, -11.3112,  -2.1962,\n",
      "         -13.0080, -12.6742, -11.7996,  -1.3102, -11.5281, -12.2547,  -2.3779,\n",
      "         -11.0740,  -2.3969]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[  7.0027,  -8.7669,  -1.7697,  -9.3753,  -8.3081,  -3.3236, -10.1632,\n",
      "          -9.1857,  -9.3100,  -9.5374,  -1.9028,  -8.5034,  -8.0532,  -8.0124,\n",
      "          -9.3462,  -9.4987,  -1.4498,  -1.4598,  -8.6125,  -7.9787,  -4.6505,\n",
      "          -9.3860,  -8.9365,  -8.8678,  -1.6382,  -9.5421,  -9.7537,  -2.2215,\n",
      "          -8.0269,  -1.9990]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -4.0472,  -9.3145,  -2.3747,  -8.9656,  -8.6706,   6.8229,  -9.7370,\n",
      "          -9.2182, -10.0845, -10.0287,  -2.3350,  -9.8866,  -8.9287,  -9.0470,\n",
      "          -9.2082,  -9.1185,  -1.6935,  -2.5873,  -8.5311,  -8.7647,  -2.3406,\n",
      "         -10.5814,  -9.4350, -10.0092,  -3.8742,  -9.0763,  -9.2215,  -2.9817,\n",
      "          -9.1799,  -1.9082]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -1.8404,  -9.2478,  -2.6055,  -9.3799,  -8.9404,  -2.5274, -10.0963,\n",
      "          -9.0646, -10.6040, -10.2449,  -3.3469,  -9.6702,  -8.3254,  -9.2443,\n",
      "          -9.5489,  -9.2820,  -1.9441,   6.7993,  -9.7973,  -9.0710,  -3.2699,\n",
      "         -10.4312,  -9.8429,  -9.0070,  -2.0537,  -8.6197,  -9.1455,  -2.7977,\n",
      "          -9.1236,  -1.2247]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -2.4238, -10.3733,  -2.0900,  -9.7678,  -9.6864,  -1.8439, -10.8490,\n",
      "         -10.5772, -10.9149, -10.8132,  -3.5857,  -9.8900,  -9.2856,  -9.9737,\n",
      "          -9.9254, -10.6961,  -0.5705,  -0.8899, -10.8480,  -9.2301,  -4.9092,\n",
      "         -11.2831, -10.4872, -10.7752,  -6.0453,  -9.6741, -10.4603,  -2.1022,\n",
      "          -9.4890,   7.4161]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -1.8670, -12.1733,  -5.3224, -12.1679, -12.4552,  -4.3945, -13.2095,\n",
      "         -12.3533, -13.4372, -12.8989,  -3.7856, -13.2153, -11.0651, -11.3992,\n",
      "         -11.3764, -12.8729,  -0.1697,  -2.5661, -12.0369, -11.5464,   0.9094,\n",
      "         -13.1796, -12.8122, -11.8615,   7.0341, -11.4977, -11.6803,  -6.0019,\n",
      "         -10.7262,  -6.4504]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.3287, -6.6100,  4.6647, -8.2770, -6.8958, -2.3216, -7.3407, -6.7913,\n",
      "         -7.2196, -7.6441, -1.7404, -6.7195, -6.5516, -6.2942, -6.5720, -7.5338,\n",
      "         -1.6324, -2.7080, -6.5021, -6.5138, -2.5506, -7.3329, -7.2351, -7.5969,\n",
      "         -4.3938, -6.9907, -7.3539,  4.6763, -6.0248, -2.6286]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.3287, -6.6100,  4.6647, -8.2770, -6.8958, -2.3216, -7.3407, -6.7913,\n",
      "         -7.2196, -7.6441, -1.7404, -6.7195, -6.5516, -6.2942, -6.5720, -7.5338,\n",
      "         -1.6324, -2.7080, -6.5021, -6.5138, -2.5506, -7.3329, -7.2351, -7.5969,\n",
      "         -4.3938, -6.9907, -7.3539,  4.6763, -6.0248, -2.6286]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1731, -7.0494, -1.0025, -7.6107, -6.8558, -2.0992, -8.2629, -6.9078,\n",
      "         -7.9558, -7.5021,  6.7887, -7.0858, -7.9156, -7.0585, -6.8434, -7.4418,\n",
      "         -3.1637, -3.3134, -7.6247, -7.3392, -4.0432, -8.7068, -7.5393, -7.7065,\n",
      "         -3.4753, -7.7944, -8.0575, -1.3812, -5.8739, -2.8182]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -4.3755, -10.9972,  -2.8328, -11.3686, -11.5907,  -2.2695, -12.1595,\n",
      "         -11.7262, -12.7580, -12.0458,  -4.3373, -12.5409, -10.9212, -11.2670,\n",
      "         -11.0812, -12.5933,  -0.8282,  -2.3781, -11.2155, -11.3725,   7.1565,\n",
      "         -12.4915, -12.3089, -11.5461,   1.2338, -10.3994, -11.0288,  -3.6560,\n",
      "         -10.7484,  -4.6884]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in song_dataloader:\n",
    "    pred=cnn_test(batch[0])\n",
    "    print(pred)\n",
    "##########################\n",
    "# 40epoch 돌렸을 때 나온 결과\n",
    "# tensor([[ -2.7031, -11.9725,  -2.4879, -11.9993, -11.9782,  -2.8094, -13.3631,\n",
    "#          -13.2695, -12.6752, -11.4285,  -5.0202, -12.0012, -10.2563, -11.4767,\n",
    "#          -10.7292, -12.1461,   5.8633,  -2.8378, -12.1131, -11.3112,  -2.1962,\n",
    "#          -13.0080, -12.6742, -11.7996,  -1.3102, -11.5281, -12.2547,  -2.3779,\n",
    "#          -11.0740,  -2.3969]], grad_fn=<AddmmBackward0>)\n",
    "# tensor([[  7.0027,  -8.7669,  -1.7697,  -9.3753,  -8.3081,  -3.3236, -10.1632,\n",
    "#           -9.1857,  -9.3100,  -9.5374,  -1.9028,  -8.5034,  -8.0532,  -8.0124,\n",
    "#           -9.3462,  -9.4987,  -1.4498,  -1.4598,  -8.6125,  -7.9787,  -4.6505,\n",
    "#           -9.3860,  -8.9365,  -8.8678,  -1.6382,  -9.5421,  -9.7537,  -2.2215,\n",
    "#           -8.0269,  -1.9990]], grad_fn=<AddmmBackward0>)\n",
    "# tensor([[ -4.0472,  -9.3145,  -2.3747,  -8.9656,  -8.6706,   6.8229,  -9.7370,\n",
    "#           -9.2182, -10.0845, -10.0287,  -2.3350,  -9.8866,  -8.9287,  -9.0470,\n",
    "#           -9.2082,  -9.1185,  -1.6935,  -2.5873,  -8.5311,  -8.7647,  -2.3406,\n",
    "#          -10.5814,  -9.4350, -10.0092,  -3.8742,  -9.0763,  -9.2215,  -2.9817,\n",
    "#           -9.1799,  -1.9082]], grad_fn=<AddmmBackward0>)\n",
    "# tensor([[ -1.8404,  -9.2478,  -2.6055,  -9.3799,  -8.9404,  -2.5274, -10.0963,\n",
    "#           -9.0646, -10.6040, -10.2449,  -3.3469,  -9.6702,  -8.3254,  -9.2443,\n",
    "#           -9.5489,  -9.2820,  -1.9441,   6.7993,  -9.7973,  -9.0710,  -3.2699,\n",
    "#          -10.4312,  -9.8429,  -9.0070,  -2.0537,  -8.6197,  -9.1455,  -2.7977,\n",
    "#           -9.1236,  -1.2247]], grad_fn=<AddmmBackward0>)\n",
    "# tensor([[ -2.4238, -10.3733,  -2.0900,  -9.7678,  -9.6864,  -1.8439, -10.8490,\n",
    "#          -10.5772, -10.9149, -10.8132,  -3.5857,  -9.8900,  -9.2856,  -9.9737,\n",
    "#           -9.9254, -10.6961,  -0.5705,  -0.8899, -10.8480,  -9.2301,  -4.9092,\n",
    "#          -11.2831, -10.4872, -10.7752,  -6.0453,  -9.6741, -10.4603,  -2.1022,\n",
    "#           -9.4890,   7.4161]], grad_fn=<AddmmBackward0>)\n",
    "# tensor([[ -1.8670, -12.1733,  -5.3224, -12.1679, -12.4552,  -4.3945, -13.2095,\n",
    "#          -12.3533, -13.4372, -12.8989,  -3.7856, -13.2153, -11.0651, -11.3992,\n",
    "#          -11.3764, -12.8729,  -0.1697,  -2.5661, -12.0369, -11.5464,   0.9094,\n",
    "#          -13.1796, -12.8122, -11.8615,   7.0341, -11.4977, -11.6803,  -6.0019,\n",
    "#          -10.7262,  -6.4504]], grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-2.3287, -6.6100,  4.6647, -8.2770, -6.8958, -2.3216, -7.3407, -6.7913,\n",
    "#          -7.2196, -7.6441, -1.7404, -6.7195, -6.5516, -6.2942, -6.5720, -7.5338,\n",
    "#          -1.6324, -2.7080, -6.5021, -6.5138, -2.5506, -7.3329, -7.2351, -7.5969,\n",
    "#          -4.3938, -6.9907, -7.3539,  4.6763, -6.0248, -2.6286]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-2.3287, -6.6100,  4.6647, -8.2770, -6.8958, -2.3216, -7.3407, -6.7913,\n",
    "#          -7.2196, -7.6441, -1.7404, -6.7195, -6.5516, -6.2942, -6.5720, -7.5338,\n",
    "#          -1.6324, -2.7080, -6.5021, -6.5138, -2.5506, -7.3329, -7.2351, -7.5969,\n",
    "#          -4.3938, -6.9907, -7.3539,  4.6763, -6.0248, -2.6286]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[-2.1731, -7.0494, -1.0025, -7.6107, -6.8558, -2.0992, -8.2629, -6.9078,\n",
    "#          -7.9558, -7.5021,  6.7887, -7.0858, -7.9156, -7.0585, -6.8434, -7.4418,\n",
    "#          -3.1637, -3.3134, -7.6247, -7.3392, -4.0432, -8.7068, -7.5393, -7.7065,\n",
    "#          -3.4753, -7.7944, -8.0575, -1.3812, -5.8739, -2.8182]],\n",
    "#        grad_fn=<AddmmBackward0>)\n",
    "# tensor([[ -4.3755, -10.9972,  -2.8328, -11.3686, -11.5907,  -2.2695, -12.1595,\n",
    "#          -11.7262, -12.7580, -12.0458,  -4.3373, -12.5409, -10.9212, -11.2670,\n",
    "#          -11.0812, -12.5933,  -0.8282,  -2.3781, -11.2155, -11.3725,   7.1565,\n",
    "#          -12.4915, -12.3089, -11.5461,   1.2338, -10.3994, -11.0288,  -3.6560,\n",
    "#          -10.7484,  -4.6884]], grad_fn=<AddmmBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "922a3350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000026591\n"
     ]
    }
   ],
   "source": [
    "# 예측 값이 정말로 전부 합쳤을 때 확률 1로 나오는지 확인\n",
    "def convert_exponential_to_decimal(numbers):\n",
    "    decimal_numbers = []\n",
    "\n",
    "    for num in numbers:\n",
    "        decimal_numbers.append('{:.10f}'.format(num))\n",
    "\n",
    "    return decimal_numbers\n",
    "\n",
    "# 주어진 숫자 리스트 (지수 표기법)\n",
    "exponential_numbers = [1.8995e-04, 1.7906e-08, 2.3556e-04, 1.7432e-08, 1.7804e-08, 1.7079e-04,\n",
    "                       4.4573e-09, 4.8947e-09, 8.8675e-09, 3.0849e-08, 1.8721e-05, 1.7399e-08,\n",
    "                       9.9611e-08, 2.9397e-08, 6.2081e-08, 1.5051e-08, 9.9762e-01, 1.6601e-04,\n",
    "                       1.5558e-08, 3.4689e-08, 3.1535e-04, 6.3573e-09, 8.8762e-09, 2.1284e-08,\n",
    "                       7.6483e-04, 2.7924e-08, 1.3503e-08, 2.6295e-04, 4.3974e-08, 2.5800e-04]\n",
    "\n",
    "# 지수 표기법을 일반적인 소수 표기법으로 변환\n",
    "decimal_numbers = convert_exponential_to_decimal(exponential_numbers)\n",
    "\n",
    "# 변환된 숫자들 출력\n",
    "result=0\n",
    "for num in decimal_numbers:\n",
    "    result+=float(num) # 확률들 다 더하는거\n",
    "#     print(num)\n",
    "print(result) # 결과\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "921b6904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000026589150002"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1.8995e-04, 1.7906e-08, 2.3556e-04, 1.7432e-08, 1.7804e-08, 1.7079e-04,\n",
    "         4.4573e-09, 4.8947e-09, 8.8675e-09, 3.0849e-08, 1.8721e-05, 1.7399e-08,\n",
    "         9.9611e-08, 2.9397e-08, 6.2081e-08, 1.5051e-08, 9.9762e-01, 1.6601e-04,\n",
    "         1.5558e-08, 3.4689e-08, 3.1535e-04, 6.3573e-09, 8.8762e-09, 2.1284e-08,\n",
    "         7.6483e-04, 2.7924e-08, 1.3503e-08, 2.6295e-04, 4.3974e-08, 2.5800e-04]\n",
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80280559",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m song_dataloader:\n\u001b[1;32m----> 3\u001b[0m     pred\u001b[38;5;241m=\u001b[39m\u001b[43mcnn_test\u001b[49m(batch[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# pred는 확률값으로 나온게 아니라 그냥 30개의 가중치로 나옴 \u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# 가중치 의 합은 1이 아니다 왜냐하면 실제 정답값과 비슷할수록 그 해당되는 값이 커지고 해당되지 않은 값은 작아지게 만든다\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     softmax_pred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'cnn_test' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "for batch in song_dataloader:\n",
    "    pred=cnn_test(batch[0])\n",
    "    # pred는 확률값으로 나온게 아니라 그냥 30개의 가중치로 나옴 \n",
    "    # 가중치 의 합은 1이 아니다 왜냐하면 실제 정답값과 비슷할수록 그 해당되는 값이 커지고 해당되지 않은 값은 작아지게 만든다\n",
    "    softmax_pred = F.softmax(pred, dim=1) \n",
    "    # 그 30개의 가중치의 출력값이 softmax를 하면 0~1사이로 만들어줘서 30개 중에서 값이 큰값은 큰 확률을 가지고\n",
    "    # 그렇지 않은 값은 작은 값을 가진다\n",
    "#     print(softmax_pred)\n",
    "    \n",
    "    soft=softmax_pred.detach().numpy()\n",
    "    print(len(soft[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72382c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "정확도 표시하는법 아직 모르고 \n",
    "loss 그래프 표시하는거 아직 모르고\n",
    "    epoch_loss /= len(dataloader) # 에폭별 loss 계산 - 배치들 평균\n",
    "    total_acc /= len(dataloader)  #  에폭별 acc 계산 - 배치들 평균\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337d8c8",
   "metadata": {},
   "source": [
    "# CNN 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "07fdb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 손실함수,옵티마이저 생성\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fe005101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_21892\\3959523747.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(pred,torch.tensor(batch[1], dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "35.380383\n",
      "1\n",
      "29.008343\n",
      "2\n",
      "21.748074\n",
      "3\n",
      "13.300318\n",
      "4\n",
      "7.384314\n",
      "5\n",
      "4.794403\n",
      "6\n",
      "3.598320\n",
      "7\n",
      "2.923731\n",
      "8\n",
      "2.514438\n",
      "9\n",
      "2.235193\n",
      "10\n",
      "2.046475\n",
      "11\n",
      "1.913622\n",
      "12\n",
      "1.824149\n",
      "13\n",
      "1.763723\n",
      "14\n",
      "1.719417\n",
      "15\n",
      "1.687241\n",
      "16\n",
      "1.663323\n",
      "17\n",
      "1.648453\n",
      "18\n",
      "1.630702\n",
      "19\n",
      "1.617524\n",
      "20\n",
      "1.606404\n",
      "21\n",
      "1.596933\n",
      "22\n",
      "1.588660\n",
      "23\n",
      "1.582585\n",
      "24\n",
      "1.574249\n",
      "25\n",
      "1.568043\n",
      "26\n",
      "1.561939\n",
      "27\n",
      "1.556441\n",
      "28\n",
      "1.551731\n",
      "29\n",
      "1.547990\n",
      "30\n",
      "1.543849\n",
      "31\n",
      "1.539918\n",
      "32\n",
      "1.536497\n",
      "33\n",
      "1.534695\n",
      "34\n",
      "1.530304\n",
      "35\n",
      "1.528035\n",
      "36\n",
      "1.523918\n",
      "37\n",
      "1.520507\n",
      "38\n",
      "1.518778\n",
      "39\n",
      "1.516025\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH=40\n",
    "for epoch in range(N_EPOCH):\n",
    "\n",
    "    cnn.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in song_dataloader:\n",
    "        pred = cnn(batch[0]) # feature 를 모델에 넣음\n",
    "#         print(pred.shape)\n",
    "#         print(torch.tensor(batch[1], dtype=torch.long).shape)\n",
    "        loss = loss_fn(pred,torch.tensor(batch[1], dtype=torch.long))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "#     if epoch%100==0:\n",
    "    print(f\"{epoch}\")\n",
    "    print(f\"{epoch_loss:.6f}\")\n",
    "    \n",
    "# 39\n",
    "# Loss:1.511371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ad5c81b8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -2.7031, -11.9725,  -2.4879, -11.9993, -11.9782,  -2.8094, -13.3631,\n",
      "         -13.2695, -12.6752, -11.4285,  -5.0202, -12.0012, -10.2563, -11.4767,\n",
      "         -10.7292, -12.1461,   5.8633,  -2.8378, -12.1131, -11.3112,  -2.1962,\n",
      "         -13.0080, -12.6742, -11.7996,  -1.3102, -11.5281, -12.2547,  -2.3779,\n",
      "         -11.0740,  -2.3969]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[  7.0027,  -8.7669,  -1.7697,  -9.3753,  -8.3081,  -3.3236, -10.1632,\n",
      "          -9.1857,  -9.3100,  -9.5374,  -1.9028,  -8.5034,  -8.0532,  -8.0124,\n",
      "          -9.3462,  -9.4987,  -1.4498,  -1.4598,  -8.6125,  -7.9787,  -4.6505,\n",
      "          -9.3860,  -8.9365,  -8.8678,  -1.6382,  -9.5421,  -9.7537,  -2.2215,\n",
      "          -8.0269,  -1.9990]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -4.0472,  -9.3145,  -2.3747,  -8.9656,  -8.6706,   6.8229,  -9.7370,\n",
      "          -9.2182, -10.0845, -10.0287,  -2.3350,  -9.8866,  -8.9287,  -9.0470,\n",
      "          -9.2082,  -9.1185,  -1.6935,  -2.5873,  -8.5311,  -8.7647,  -2.3406,\n",
      "         -10.5814,  -9.4350, -10.0092,  -3.8742,  -9.0763,  -9.2215,  -2.9817,\n",
      "          -9.1799,  -1.9082]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -1.8404,  -9.2478,  -2.6055,  -9.3799,  -8.9404,  -2.5274, -10.0963,\n",
      "          -9.0646, -10.6040, -10.2449,  -3.3469,  -9.6702,  -8.3254,  -9.2443,\n",
      "          -9.5489,  -9.2820,  -1.9441,   6.7993,  -9.7973,  -9.0710,  -3.2699,\n",
      "         -10.4312,  -9.8429,  -9.0070,  -2.0537,  -8.6197,  -9.1455,  -2.7977,\n",
      "          -9.1236,  -1.2247]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -2.4238, -10.3733,  -2.0900,  -9.7678,  -9.6864,  -1.8439, -10.8490,\n",
      "         -10.5772, -10.9149, -10.8132,  -3.5857,  -9.8900,  -9.2856,  -9.9737,\n",
      "          -9.9254, -10.6961,  -0.5705,  -0.8899, -10.8480,  -9.2301,  -4.9092,\n",
      "         -11.2831, -10.4872, -10.7752,  -6.0453,  -9.6741, -10.4603,  -2.1022,\n",
      "          -9.4890,   7.4161]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -1.8670, -12.1733,  -5.3224, -12.1679, -12.4552,  -4.3945, -13.2095,\n",
      "         -12.3533, -13.4372, -12.8989,  -3.7856, -13.2153, -11.0651, -11.3992,\n",
      "         -11.3764, -12.8729,  -0.1697,  -2.5661, -12.0369, -11.5464,   0.9094,\n",
      "         -13.1796, -12.8122, -11.8615,   7.0341, -11.4977, -11.6803,  -6.0019,\n",
      "         -10.7262,  -6.4504]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.3287, -6.6100,  4.6647, -8.2770, -6.8958, -2.3216, -7.3407, -6.7913,\n",
      "         -7.2196, -7.6441, -1.7404, -6.7195, -6.5516, -6.2942, -6.5720, -7.5338,\n",
      "         -1.6324, -2.7080, -6.5021, -6.5138, -2.5506, -7.3329, -7.2351, -7.5969,\n",
      "         -4.3938, -6.9907, -7.3539,  4.6763, -6.0248, -2.6286]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.3287, -6.6100,  4.6647, -8.2770, -6.8958, -2.3216, -7.3407, -6.7913,\n",
      "         -7.2196, -7.6441, -1.7404, -6.7195, -6.5516, -6.2942, -6.5720, -7.5338,\n",
      "         -1.6324, -2.7080, -6.5021, -6.5138, -2.5506, -7.3329, -7.2351, -7.5969,\n",
      "         -4.3938, -6.9907, -7.3539,  4.6763, -6.0248, -2.6286]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1731, -7.0494, -1.0025, -7.6107, -6.8558, -2.0992, -8.2629, -6.9078,\n",
      "         -7.9558, -7.5021,  6.7887, -7.0858, -7.9156, -7.0585, -6.8434, -7.4418,\n",
      "         -3.1637, -3.3134, -7.6247, -7.3392, -4.0432, -8.7068, -7.5393, -7.7065,\n",
      "         -3.4753, -7.7944, -8.0575, -1.3812, -5.8739, -2.8182]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -4.3755, -10.9972,  -2.8328, -11.3686, -11.5907,  -2.2695, -12.1595,\n",
      "         -11.7262, -12.7580, -12.0458,  -4.3373, -12.5409, -10.9212, -11.2670,\n",
      "         -11.0812, -12.5933,  -0.8282,  -2.3781, -11.2155, -11.3725,   7.1565,\n",
      "         -12.4915, -12.3089, -11.5461,   1.2338, -10.3994, -11.0288,  -3.6560,\n",
      "         -10.7484,  -4.6884]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in song_dataloader:\n",
    "    pred=cnn_test(batch[0])\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 왜 안되는지 물어보기!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "97cbf4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(song_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b33ed053",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (190792197.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[117], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    a=[30  2]\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "a=[30  2]\n",
    "b=[30  2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de008be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_per(dataset,data_loader,model,loss_fn):\n",
    "    model.eval() # 모델을 평가모드로 전환\n",
    "    acc_=0 # 정확도 값 초기화\n",
    "    for batch in data_loader: # 데이터로더로 배치단위로 \n",
    "        eva=model(batch[0]) #  모델에 배치단위인 데이터를 넣어 예측값을 eva에 저장\n",
    "        pred_output=[dataset.num_dict[i] for i in eva.argmax(dim=-1).numpy()] # 예측값 확인 용도\n",
    "        print(pred_output)\n",
    "        batch_output=[dataset.num_dict[i] for i in batch[1].numpy()] # 실제값 확인 용도\n",
    "        print(batch_output)\n",
    "        acc_+=accuracy_score(batch[1].numpy(),eva.argmax(dim=-1).numpy()) # 정확도 계산 용도\n",
    "    \n",
    "    print(\"총 맞은 갯수: \",acc_)\n",
    "    batch_num=len(song_dataloader) # 배치수 확인 용도 \n",
    "    per=\n",
    "    print(\"배치 수: \",len(song_dataloader))\n",
    "    print(\"확률:\",(acc_/len(song_dataloader))*100)\n",
    "    print(\"총 데이터 갯수:\",len(eva)*len(song_dataloader)) # 배치 * 데이터 loader의 길이\n",
    "    return acc_,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b7da2fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['빨개', '맛있어']\n",
      "['빨개', '맛있어']\n",
      "1.0\n",
      "['길어', '빨라']\n",
      "['길어', '빨라']\n",
      "2.0\n",
      "['높아', '백두산']\n",
      "['높아', '백두산']\n",
      "3.0\n",
      "['달', '달']\n",
      "['달', '별']\n",
      "3.5\n",
      "['건', '사랑']\n",
      "['건', '사랑']\n",
      "4.5\n",
      "총 맞은 갯수:  4.5\n",
      "배치 수:  5\n",
      "확률: 90.0\n",
      "총 데이터 갯수: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "cnn.eval()\n",
    "pred_num=0\n",
    "acc_=0\n",
    "for batch in song_dataloader:\n",
    "    eva=cnn(batch[0])\n",
    "#     print(eva)\n",
    "    \n",
    "#     print(eva.argmax(dim=-1))\n",
    "    pred_output=[dataset.num_dict[i] for i in eva.argmax(dim=-1).numpy()]\n",
    "    print(pred_output)\n",
    "    batch_output=[dataset.num_dict[i] for i in batch[1].numpy()]\n",
    "    print(batch_output)\n",
    "#     print(batch[1])\n",
    "#     print(batch[1].numpy())\n",
    "#     print(len(eva))\n",
    "    acc_+=accuracy_score(batch[1].numpy(),eva.argmax(dim=-1).numpy())\n",
    "    print(acc_)\n",
    "print(\"총 맞은 갯수: \",acc_)\n",
    "print(\"배치 수: \",len(song_dataloader))\n",
    "print(\"확률:\",(acc_/len(song_dataloader))*100)\n",
    "print(\"총 데이터 갯수:\",len(eva)*len(song_dataloader)) # 배치 * 데이터 loader의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4b3107b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences= ['원숭이 엉덩이는 빨개',\n",
    "'빨가면 사과 사과는 맛있어', \n",
    "'맛있으면 바나나 바나나는 길어', \n",
    "'길으면 기차 기차는 빨라',\n",
    "'빠르면 비행기 비행기는 높아', \n",
    "'높으면 백두산',\n",
    "'더 높은 건 달',\n",
    "'더 높은 건 별',\n",
    "'달보다 별보다 더 높은 건', \n",
    "'부모님의 사랑']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "945ee475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6de94080",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(4, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnum_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: (4, 6)"
     ]
    }
   ],
   "source": [
    "num_dict[4,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "99ccfacf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'엉덩이는': 1,\n",
       " '맛있어': 2,\n",
       " '원숭이': 3,\n",
       " '부모님의': 4,\n",
       " '빨가면': 5,\n",
       " '건': 6,\n",
       " '비행기는': 7,\n",
       " '맛있으면': 8,\n",
       " '바나나': 9,\n",
       " '높으면': 10,\n",
       " '길으면': 11,\n",
       " '사랑': 12,\n",
       " '높은': 13,\n",
       " '별보다': 14,\n",
       " '기차는': 15,\n",
       " '빠르면': 16,\n",
       " '높아': 17,\n",
       " '길어': 18,\n",
       " '사과는': 19,\n",
       " '사과': 20,\n",
       " '기차': 21,\n",
       " '달보다': 22,\n",
       " '빨라': 23,\n",
       " '백두산': 24,\n",
       " '더': 25,\n",
       " '비행기': 26,\n",
       " '별': 27,\n",
       " '달': 28,\n",
       " '바나나는': 29,\n",
       " '빨개': 30}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva=cnn(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_max = pred.argmax(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
