{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeBKpny-2P2o","executionInfo":{"status":"ok","timestamp":1693211331972,"user_tz":-540,"elapsed":6144,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"f5f91d1d-a9a0-4c4b-afd6-f7f96bea7416"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["pip install accelerate -U"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YCUqajTZQR5c","executionInfo":{"status":"ok","timestamp":1693211339540,"user_tz":-540,"elapsed":7571,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"bb678f79-5e9a-4b55-b2f6-64940b2ad579"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install transformers[torch]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHcmnXHVQVBm","executionInfo":{"status":"ok","timestamp":1693211348581,"user_tz":-540,"elapsed":9046,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"cbb805ff-fad5-4f47-9d14-01c9e29438d5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.32.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n","Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.22.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n"]}]},{"cell_type":"code","source":["from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n","    DataCollatorForSeq2Seq,\n",")\n","from tokenizers import Tokenizer\n","from typing import Dict, List, Optional\n","from torch.utils.data import Dataset\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","from IPython.display import display\n","from typing import Dict"],"metadata":{"id":"JTU_qBYU3dfA","executionInfo":{"status":"ok","timestamp":1693211359112,"user_tz":-540,"elapsed":10537,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"R6vsZG822P2q","executionInfo":{"status":"ok","timestamp":1693211359112,"user_tz":-540,"elapsed":5,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"outputs":[],"source":["df = pd.read_csv(\"/content/drive/MyDrive/bible/pair.csv\")"]},{"cell_type":"code","source":["df.describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"GgjIZRUe4qEt","executionInfo":{"status":"ok","timestamp":1693211359566,"user_tz":-540,"elapsed":458,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"18c8ee51-0ec0-4732-bea5-8cb193d8a872"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 input_text         target_text\n","count                 31092               31092\n","unique                30844               30683\n","top     여호와께서 모세에게 말씀하셨습니다.  여호와께서 모세에게 일러 가라사대\n","freq                     69                  65"],"text/html":["\n","  <div id=\"df-3ce55938-e251-4bc5-893d-779ff59cae27\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input_text</th>\n","      <th>target_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>31092</td>\n","      <td>31092</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>30844</td>\n","      <td>30683</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>여호와께서 모세에게 말씀하셨습니다.</td>\n","      <td>여호와께서 모세에게 일러 가라사대</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>69</td>\n","      <td>65</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ce55938-e251-4bc5-893d-779ff59cae27')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3ce55938-e251-4bc5-893d-779ff59cae27 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3ce55938-e251-4bc5-893d-779ff59cae27');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["df.isnull().sum() # 결측치는 없음"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M79SGiIhB1R0","executionInfo":{"status":"ok","timestamp":1693211366267,"user_tz":-540,"elapsed":310,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"962890e9-d3b1-4f3c-8d82-01c38ddc28f8"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["input_text     0\n","target_text    0\n","dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"TBNI97-d2kcA","executionInfo":{"status":"ok","timestamp":1693211366750,"user_tz":-540,"elapsed":4,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkuu63WE2P2r","executionInfo":{"status":"ok","timestamp":1693211367854,"user_tz":-540,"elapsed":722,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"fbce4626-4bf7-4bcf-8005-c36800194587"},"outputs":[{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]}],"source":["model_name = \"gogamza/kobart-base-v2\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"B8IQOMk02P2r","executionInfo":{"status":"ok","timestamp":1693211367854,"user_tz":-540,"elapsed":8,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"outputs":[],"source":["style_map={\n","    'input_text':'문어체',\n","    'target_text':'성경체'\n","}"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"zxhqJget2P2r","executionInfo":{"status":"ok","timestamp":1693211367854,"user_tz":-540,"elapsed":7,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"outputs":[],"source":["class TextStyleTransferDataset(Dataset):\n","  def __init__(self,\n","               df: pd.DataFrame,\n","               tokenizer: Tokenizer\n","               ):\n","    self.df = df\n","    self.tokenizer = tokenizer\n","\n","  def __len__(self):\n","    return len(self.df)\n","\n","  def __getitem__(self, index):\n","    row = self.df.iloc[index, :].dropna().sample(2)\n","    text1 = row[0]\n","    text2 = row[1]\n","    target_style = row.index[1]\n","    target_style_name = style_map[target_style]\n","\n","    encoder_text = f\"{target_style_name} 말투로 변환:{text2}\"\n","    decoder_text = f\"{text1}{self.tokenizer.eos_token}\"\n","    model_inputs = self.tokenizer(encoder_text, max_length=64, truncation=True)\n","\n","    with self.tokenizer.as_target_tokenizer():\n","      labels = tokenizer(decoder_text, max_length=64, truncation=True)\n","    model_inputs['labels'] = labels['input_ids']\n","    del model_inputs['token_type_ids']\n","\n","    return model_inputs"]},{"cell_type":"code","source":["df.iloc[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3HOfmJXKMji","executionInfo":{"status":"ok","timestamp":1693211367855,"user_tz":-540,"elapsed":6,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"b908941e-643a-48b1-f6f4-ed9599727398"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["input_text     태초에 하나님께서 하늘과 땅을 창조하셨습니다.\n","target_text          태초에 하나님이 천지를 창조하시니라\n","Name: 0, dtype: object"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["row_test=df.iloc[0, :].dropna().sample(2)"],"metadata":{"id":"8D3EOGJELFHs","executionInfo":{"status":"ok","timestamp":1693211368360,"user_tz":-540,"elapsed":6,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["text1=row_test[0]\n","text1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"LIuyJl_MNfVu","executionInfo":{"status":"ok","timestamp":1693211368774,"user_tz":-540,"elapsed":419,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"78b6efe9-43c7-4b7e-8e7d-2be46f2874f9"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'태초에 하나님이 천지를 창조하시니라'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["text2=row_test[1]\n","text2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"RC2zuqLJOAMa","executionInfo":{"status":"ok","timestamp":1693211368775,"user_tz":-540,"elapsed":5,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"5468ff23-9103-49fc-906b-9cf5841358c3"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'태초에 하나님께서 하늘과 땅을 창조하셨습니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["target_style = row_test.index[1]"],"metadata":{"id":"R6henlCkNM_T","executionInfo":{"status":"ok","timestamp":1693211369656,"user_tz":-540,"elapsed":3,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["target_style_name = style_map[target_style]"],"metadata":{"id":"S_YN6i9OOWbw","executionInfo":{"status":"ok","timestamp":1693211369657,"user_tz":-540,"elapsed":3,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"id":"NoT3g6U_2P2r","executionInfo":{"status":"ok","timestamp":1693211370114,"user_tz":-540,"elapsed":4,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"outputs":[],"source":["dataset = TextStyleTransferDataset(df, tokenizer)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gIK1MPuo2P2r","executionInfo":{"status":"ok","timestamp":1693211370784,"user_tz":-540,"elapsed":2,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"bdce9791-a24c-4cb4-d216-8d0b9428afc4"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]}],"source":["out = dataset[0]"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crqUum8e2P2t","executionInfo":{"status":"ok","timestamp":1693211371241,"user_tz":-540,"elapsed":16,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"27cb02cf-0316-4ccf-b654-3b623608faa8"},"outputs":[{"output_type":"stream","name":"stdout","text":["[14127, 9085, 12687, 14070, 13282, 10338, 14296, 13716, 257, 13145, 12717, 11786, 14392, 16613, 14578, 14404, 17034, 18940, 26418]\n","[14438, 12717, 11786, 20806, 17429, 16487, 9120, 21665, 17034, 20032, 15170, 1]\n"]}],"source":["print(out['input_ids'])\n","print(out['labels'])"]},{"cell_type":"code","source":["print(tokenizer.decode(out['input_ids']))\n","print(tokenizer.decode(out['labels']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4dxijp55uWH","executionInfo":{"status":"ok","timestamp":1693211371242,"user_tz":-540,"elapsed":13,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"fadf0b89-6512-455c-c0a0-2417404ab269"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["성경체 말투로 변환:태초에 하나님이 천지를 창조하시니라\n","태초에 하나님께서 하늘과 땅을 창조하셨습니다.</s>\n"]}]},{"cell_type":"code","source":["out = dataset[1]\n","print(out['input_ids'])\n","print(out['labels'])\n","print(tokenizer.decode(out['input_ids']))\n","print(tokenizer.decode(out['labels']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7N1wvwTo5wTj","executionInfo":{"status":"ok","timestamp":1693211371242,"user_tz":-540,"elapsed":9,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"38c022e1-028b-4449-add0-dc67a42d22c1"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[14111, 11763, 12687, 14070, 13282, 10338, 14296, 13716, 257, 9229, 10281, 9828, 14028, 15495, 12005, 14446, 14400, 17345, 12048, 11229, 14082, 16763, 14202, 18618, 21686, 14392, 14807, 1700, 13185, 14085, 11763, 14418, 15170, 22504, 12034, 17433, 29253, 18896, 9102, 21512, 21420, 14126, 12005, 14198, 20341, 15549, 14502, 14158, 11306, 15170]\n","[15495, 12034, 14929, 9869, 14058, 14061, 13644, 14363, 16816, 11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466, 19808, 18940, 26418, 1]\n","문어체 말투로 변환:그런데 그 땅은 지금처럼 짜임새 있는 모습이 아니었고, 생물 하나 없이 텅 비어 있었습니다. 어둠이 깊은 바다를 덮고 있었고, 하나님의 영은 물 위에서 움직이고 계셨습니다.\n","땅이 혼돈하고 공허하며 흑암이 깊음 위에 있고 하나님의 신은 수면에 운행하시니라</s>\n"]}]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4gOh39_2P2t","executionInfo":{"status":"ok","timestamp":1693211371242,"user_tz":-540,"elapsed":6,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"379a3bb7-bedb-4748-aa09-741957ffda9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["27982 3110\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","# 학습을 위해 train, test set으로 나눈다.\n","df_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\n","print(len(df_train), len(df_test))"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1QmfBvlK2P2u","executionInfo":{"status":"ok","timestamp":1693211376199,"user_tz":-540,"elapsed":4477,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"47f3913f-c125-4173-d496-cbdc4a731b31"},"outputs":[{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]}],"source":["train_dataset = TextStyleTransferDataset(\n","    df_train,\n","    tokenizer\n",")\n","test_dataset = TextStyleTransferDataset(\n","    df_test,\n","    tokenizer\n",")\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=tokenizer, model=model\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"-e90oY-d2P2u","executionInfo":{"status":"ok","timestamp":1693211386028,"user_tz":-540,"elapsed":9834,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"outputs":[],"source":["model_path = \"/content/drive/MyDrive/data/text-transfer-smilegate-bart-eos/\"\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=model_path, #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=24, # number of training epochs\n","    per_device_train_batch_size=16, # batch size for training\n","    per_device_eval_batch_size=16,  # batch size for evaluation\n","    eval_steps=500, # Number of update steps between two evaluations.\n","    save_steps=1000, # after # steps model is saved\n","    warmup_steps=300,# number of warmup steps for learning rate scheduler\n","    prediction_loss_only=True,\n","    evaluation_strategy=\"steps\",\n","    save_total_limit=3\n","    )\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LcJRc76h2P2u","executionInfo":{"status":"error","timestamp":1693215565485,"user_tz":-540,"elapsed":4169153,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"4df905f9-8d58-42b3-bba4-dbfa9ef9be47"},"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12612' max='41976' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12612/41976 1:09:23 < 2:41:36, 3.03 it/s, Epoch 7.21/24]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.485100</td>\n","      <td>2.596757</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.597400</td>\n","      <td>2.368720</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>2.406500</td>\n","      <td>2.264215</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>2.253300</td>\n","      <td>2.194721</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>2.140500</td>\n","      <td>2.154789</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>2.087500</td>\n","      <td>2.099530</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>2.061400</td>\n","      <td>2.080022</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.904400</td>\n","      <td>2.047471</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.883800</td>\n","      <td>2.018365</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.877000</td>\n","      <td>1.999570</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.796400</td>\n","      <td>2.006438</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.738900</td>\n","      <td>1.986583</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.723500</td>\n","      <td>1.988618</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>1.727300</td>\n","      <td>1.968795</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>1.582200</td>\n","      <td>1.994630</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>1.590700</td>\n","      <td>1.973943</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>1.609700</td>\n","      <td>1.961252</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>1.522600</td>\n","      <td>1.975068</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>1.472100</td>\n","      <td>1.978541</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>1.472600</td>\n","      <td>1.961424</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>1.484200</td>\n","      <td>1.954670</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>1.358400</td>\n","      <td>1.984580</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>1.376800</td>\n","      <td>1.974864</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>1.373800</td>\n","      <td>1.959769</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>1.306500</td>\n","      <td>2.001991</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1837\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"CPS5F8uw2P2u","executionInfo":{"status":"ok","timestamp":1693215609829,"user_tz":-540,"elapsed":4716,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"outputs":[],"source":["trainer.save_model(\"/content/drive/MyDrive/bible/model/\")"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YuKWhATG2P2v","executionInfo":{"status":"ok","timestamp":1693215672085,"user_tz":-540,"elapsed":4615,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"98660ca6-9bab-4243-81f0-c9f6df3dc645"},"outputs":[{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n","You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]}],"source":["from transformers import pipeline\n","\n","nlg_pipeline = pipeline('text2text-generation',model=\"/content/drive/MyDrive/bible/model/\", tokenizer=model_name)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"rAZjpQwc2P2v","executionInfo":{"status":"ok","timestamp":1693215695632,"user_tz":-540,"elapsed":312,"user":{"displayName":"박장선","userId":"05657079238116333054"}}},"outputs":[],"source":["def generate_text(pipe, text, target_style, num_return_sequences=5, max_length=60):\n","  target_style_name = style_map[target_style]\n","  text = f\"{target_style_name} 말투로 변환:{text}\"\n","  out = pipe(text, num_return_sequences=num_return_sequences, max_length=max_length)\n","  return [x['generated_text'] for x in out]\n",""]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKdEjX4N2P2v","executionInfo":{"status":"ok","timestamp":1693215723524,"user_tz":-540,"elapsed":6290,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"3d041f98-3c32-4b3b-9142-a1187dfa2783"},"outputs":[{"output_type":"stream","name":"stdout","text":["입력 문장: \n","어쩌다 마주친 그대 모습에\n","내 마음을 빼앗겨 버렸네\n","어쩌다 마주친 그대 두 눈이\n","내 마음을 사로잡아 버렸네\n","그대에게 할 말이 있는데\n","왜 이리 용기가 없을까\n","음 말을 하고 싶지만 자신이 없어\n","내 가슴만 두근두근\n","답답한 이 내 마음\n","바람 속에 날려 보내리\n","\n","input_text 내가 어찌 보고 네 얼굴을 바라보았느냐 내 눈이 네 눈을 바라보았느냐 내가 네게 무슨 말을 할꼬 내가 어찌하여 네게 대답하기를 주저하였느냐 내가 어찌하여 네게 대답하기를 주저하였느냐\n","target_text 내가 언제친대 얼굴을 보고 마음이 빼앗겼구나. 내가 친히 보는 눈이 나를 사로잡았구나. 내가 무슨 말을 할 수 있겠는가? 내가 무슨 말을 할 수 있겠는가? 내가 무슨 말을 할 수 있겠는가?\n"]}],"source":["target_styles = df.columns\n","src_text = \"\"\"\n","어쩌다 마주친 그대 모습에\n","내 마음을 빼앗겨 버렸네\n","어쩌다 마주친 그대 두 눈이\n","내 마음을 사로잡아 버렸네\n","그대에게 할 말이 있는데\n","왜 이리 용기가 없을까\n","음 말을 하고 싶지만 자신이 없어\n","내 가슴만 두근두근\n","답답한 이 내 마음\n","바람 속에 날려 보내리\n","\"\"\"\n","\n","print(\"입력 문장:\", src_text)\n","for style in target_styles:\n","  print(style, generate_text(nlg_pipeline, src_text, style, num_return_sequences=1, max_length=1000)[0])"]},{"cell_type":"code","source":["df.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDJfavgDjZUo","executionInfo":{"status":"ok","timestamp":1693216258336,"user_tz":-540,"elapsed":6,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"192e4144-7c71-4b07-c47c-98a79a9b6112"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['input_text', 'target_text'], dtype='object')"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":[],"metadata":{"id":"b_OT-Qwhj77F"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_MP8umhF2P2v","executionInfo":{"status":"ok","timestamp":1693217109776,"user_tz":-540,"elapsed":1194,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"6dcd7b77-276a-4d98-8b52-b9d3586b6146"},"outputs":[{"output_type":"stream","name":"stdout","text":["입력 문장: \n","날 설득해봐. 네가 부활을 해야 할 이유를 납득시켜보라고.\n","\n","성경체:  나를 설득하라 네가 부활할 것을 내게 알게 하라\n"]}],"source":["target_styles = df.columns\n","src_text = \"\"\"\n","날 설득해봐. 네가 부활을 해야 할 이유를 납득시켜보라고.\n","\"\"\"\n","\n","print(\"입력 문장:\", src_text)\n","print( \"성경체: \", generate_text(nlg_pipeline, src_text,\"input_text\", num_return_sequences=1, max_length=1000)[0])\n","\n","#target_text 는 문어체\n","# input_text 는 성경체"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCuYzf4G2P2v","executionInfo":{"status":"ok","timestamp":1693217075207,"user_tz":-540,"elapsed":764,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"210d737a-f7e8-4ce5-b3eb-997d6f14b681"},"outputs":[{"output_type":"stream","name":"stdout","text":["입력 문장: \n","내가 한강을 갔더니 하늘에서 비가 내렸다.\n","\n","성경체:  내가 강으로 가니 하늘에서 비 내리는 것이 보였고\n"]}],"source":["target_styles = df.columns\n","src_text = \"\"\"\n","내가 한강을 갔더니 하늘에서 비가 내렸다.\n","\"\"\"\n","\n","print(\"입력 문장:\", src_text)\n","print( \"성경체: \", generate_text(nlg_pipeline, src_text,\"input_text\", num_return_sequences=1, max_length=1000)[0])\n"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3khuiRa2P2v","executionInfo":{"status":"ok","timestamp":1693217157638,"user_tz":-540,"elapsed":863,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"64fe6d56-91f7-4081-eb4b-9df8c15dd6ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["입력 문장: \n","안녕하세요 저는 오늘 무척 우울한 상태입니다\n","\n","성경체:  안녕하 저는 오늘 심히 근심하나이다\n"]}],"source":["\n","target_styles = df.columns\n","src_text = \"\"\"\n","안녕하세요 저는 오늘 무척 우울한 상태입니다\n","\"\"\"\n","\n","print(\"입력 문장:\", src_text)\n","print( \"성경체: \", generate_text(nlg_pipeline, src_text,\"input_text\", num_return_sequences=1, max_length=1000)[0])\n"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2e7tmkJO2P2v","executionInfo":{"status":"ok","timestamp":1693217162846,"user_tz":-540,"elapsed":1669,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"c9a3bc3a-e9f1-444c-c956-0684d44a039f"},"outputs":[{"output_type":"stream","name":"stdout","text":["입력 문장: \n","안녕하세요 저는 오늘 무척 우울한 상태입니다.\n","\n","성경체:  안녕하 저는 오늘 심히 근심하나이다\n"]}],"source":["target_styles = df.columns\n","src_text = \"\"\"\n","안녕하세요 저는 오늘 무척 우울한 상태입니다.\n","\"\"\"\n","\n","print(\"입력 문장:\", src_text)\n","print( \"성경체: \", generate_text(nlg_pipeline, src_text,\"input_text\", num_return_sequences=1, max_length=1000)[0])\n"]},{"cell_type":"code","source":["target_styles = df.columns\n","src_text = \"\"\"\n","안녕하세요 저는 오늘 무척 우울한 상태입니다!\n","\"\"\"\n","\n","print(\"입력 문장:\", src_text)\n","print( \"성경체: \", generate_text(nlg_pipeline, src_text,\"input_text\", num_return_sequences=1, max_length=1000)[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2YaKsXjU6lfl","executionInfo":{"status":"ok","timestamp":1693217182171,"user_tz":-540,"elapsed":1285,"user":{"displayName":"박장선","userId":"05657079238116333054"}},"outputId":"4d565c7c-419d-4679-9e88-410a38e25ade"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 문장: \n","안녕하세요 저는 오늘 무척 우울한 상태입니다!\n","\n","성경체:  안녕하 저는 오늘 심히 근심하나이다\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rByEdp-um8oW"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}