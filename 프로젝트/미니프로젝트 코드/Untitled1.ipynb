{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20472f1",
   "metadata": {},
   "source": [
    "# Kobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f4ece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gluonnlp==0.8.0\n",
      "  Using cached gluonnlp-0.8.0.tar.gz (235 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from gluonnlp==0.8.0) (1.23.5)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Building wheel for gluonnlp (setup.py): started\n",
      "  Building wheel for gluonnlp (setup.py): finished with status 'done'\n",
      "  Created wheel for gluonnlp: filename=gluonnlp-0.8.0-py3-none-any.whl size=292736 sha256=a0fe7bb130edd04c6687ff73c0852439d1a9efc1653a15afbb014e40f3df7f43\n",
      "  Stored in directory: c:\\users\\playdata\\appdata\\local\\pip\\cache\\wheels\\2d\\cc\\dc\\7ec84dced25f738b8be400101abb67e4b50c905090a51017e4\n",
      "Successfully built gluonnlp\n",
      "Installing collected packages: gluonnlp\n",
      "Successfully installed gluonnlp-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gluonnlp==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2aa5734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kobert_tokenizer\n",
      "  Cloning https://github.com/SKTBrain/KoBERT.git to c:\\users\\playdata\\appdata\\local\\temp\\pip-install-nbyv_t26\\kobert-tokenizer_cf74104e34434791b2d68be646a7dbb0\n",
      "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer: Requested kobert from git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer has inconsistent name: expected 'kobert-tokenizer', but metadata has 'kobert'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git 'C:\\Users\\Playdata\\AppData\\Local\\Temp\\pip-install-nbyv_t26\\kobert-tokenizer_cf74104e34434791b2d68be646a7dbb0'\n",
      "  WARNING: Generating metadata for package kobert_tokenizer produced metadata for project name kobert. Fix your #egg=kobert_tokenizer fragments.\n",
      "ERROR: Could not find a version that satisfies the requirement kobert-tokenizer (unavailable) (from versions: none)\n",
      "ERROR: No matching distribution found for kobert-tokenizer (unavailable)\n",
      "'subdirectory'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807b9e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kobert-transformers\n",
      "  Downloading kobert_transformers-0.5.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: torch>=1.1.0 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from kobert-transformers) (2.0.1)\n",
      "Requirement already satisfied: transformers<5,>=3 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from kobert-transformers) (4.30.2)\n",
      "Requirement already satisfied: sentencepiece>=0.1.91 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from kobert-transformers) (0.1.99)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (4.6.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (3.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers<5,>=3->kobert-transformers) (2023.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from tqdm>=4.27->transformers<5,>=3->kobert-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from jinja2->torch>=1.1.0->kobert-transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers<5,>=3->kobert-transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers<5,>=3->kobert-transformers) (2.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers<5,>=3->kobert-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers<5,>=3->kobert-transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\playdata\\anaconda3\\envs\\ml\\lib\\site-packages (from sympy->torch>=1.1.0->kobert-transformers) (1.3.0)\n",
      "Installing collected packages: kobert-transformers\n",
      "Successfully installed kobert-transformers-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kobert-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82cb528",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\anaconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████| 426/426 [00:00<00:00, 440kB/s]\n",
      "C:\\Users\\Playdata\\anaconda3\\envs\\ml\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading model.safetensors: 100%|████████████████████████████████████████████████| 369M/369M [00:19<00:00, 19.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from kobert_transformers import get_kobert_model, get_distilkobert_model\n",
    "model = get_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff30d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1d6e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.position_embeddings = nn.Embedding(1024, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f01e989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(1024, 768)\n"
     ]
    }
   ],
   "source": [
    "print(model.position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2cb1822c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BertModel.get_input_embeddings of BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (position_embeddings): Embedding(1024, 768)\n",
       ")>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b71a9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\anaconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3786c771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils.PreTrainedTokenizer"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PreTrainedTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5527dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_transformers import get_kobert_model, get_distilkobert_model\n",
    "model = get_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0dedb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, DistilBertModel\n",
    "model = BertModel.from_pretrained(\"monologg/kobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e586b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "008521d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7585777",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model.embeddings.position_embeddings(torch.tensor(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e2f3e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c346a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "sequence_output, pooled_output = model(input_ids, attention_mask, token_type_ids)\n",
    "sequence_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd2d4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from kobert_transformers import get_tokenizer\n",
    "tokenizer = get_tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e800a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'monologg/kobert': 1024,\n",
       " 'monologg/kobert-lm': 1024,\n",
       " 'monologg/distilkobert': 1024}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86b8760a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8004"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce055c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(\"우삼성전자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "for art in df['con']:\n",
    "    x=tokenizer.convert_tokens_to_ids(art)\n",
    "    list.ap\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38af072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8445ad32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfe47aa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_max_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_max_length\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_max_length' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bb240d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'monologg/kobert': 512,\n",
       " 'monologg/kobert-lm': 512,\n",
       " 'monologg/distilkobert': 512}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7ed4333",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁LG', '에너지', '솔', '루', '▁', '션']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"LG에너지솔루 션\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "094e145a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LG에너지솔루션']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"LG에너지솔루션\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2519785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8002"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"LG에너지솔루션\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5311f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8003"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"우삼성전자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc14abce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dec4f7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      VersionNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "---------------------------- ---------\n",
      "absl-py                      1.4.0\n",
      "anyio                        3.6.2\n",
      "argon2-cffi                  21.3.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.2.3\n",
      "asttokens                    2.2.1\n",
      "astunparse                   1.6.3\n",
      "async-generator              1.10\n",
      "attrs                        23.1.0\n",
      "backcall                     0.2.0\n",
      "beautifulsoup4               4.12.2\n",
      "bleach                       6.0.0\n",
      "boto3                        1.24.94\n",
      "botocore                     1.27.96\n",
      "cachetools                   5.3.1\n",
      "certifi                      2022.12.7\n",
      "cffi                         1.15.1\n",
      "chardet                      3.0.4\n",
      "charset-normalizer           3.1.0\n",
      "click                        8.1.3\n",
      "cmake                        3.26.4\n",
      "colorama                     0.4.6\n",
      "comm                         0.1.3\n",
      "contourpy                    1.0.7\n",
      "cycler                       0.11.0\n",
      "Cython                       0.29.36\n",
      "debugpy                      1.6.7\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "et-xmlfile                   1.1.0\n",
      "exceptiongroup               1.1.1\n",
      "executing                    1.2.0\n",
      "fastjsonschema               2.16.3\n",
      "filelock                     3.12.0\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.39.3\n",
      "fqdn                         1.5.1\n",
      "fsspec                       2023.6.0\n",
      "gast                         0.4.0\n",
      "gdown                        4.7.1\n",
      "gluonnlp                     0.8.0\n",
      "google-auth                  2.21.0\n",
      "google-auth-oauthlib         1.0.0\n",
      "google-pasta                 0.2.0\n",
      "graphviz                     0.20.1\n",
      "grpcio                       1.56.0\n",
      "h11                          0.14.0\n",
      "h5py                         3.9.0\n",
      "huggingface-hub              0.15.1\n",
      "idna                         2.6\n",
      "imbalanced-learn             0.10.1\n",
      "instagram-private-api        1.6.0.0\n",
      "ipykernel                    6.22.0\n",
      "ipython                      8.12.0\n",
      "ipython-genutils             0.2.0\n",
      "isoduration                  20.11.0\n",
      "jax                          0.4.13\n",
      "jedi                         0.18.2\n",
      "Jinja2                       3.1.2\n",
      "jmespath                     1.0.1\n",
      "joblib                       1.2.0\n",
      "JPype1                       1.4.1\n",
      "jsonpointer                  2.3\n",
      "jsonschema                   4.17.3\n",
      "jupyter_client               8.2.0\n",
      "jupyter_core                 5.3.0\n",
      "jupyter-events               0.6.3\n",
      "jupyter_server               2.5.0\n",
      "jupyter_server_terminals     0.4.4\n",
      "jupyterlab-pygments          0.2.2\n",
      "kaggle                       1.5.13\n",
      "keras                        2.12.0\n",
      "kiwisolver                   1.4.4\n",
      "kobert-transformers          0.5.1\n",
      "konlpy                       0.6.0\n",
      "libclang                     16.0.0\n",
      "lxml                         4.9.2\n",
      "Markdown                     3.4.3\n",
      "MarkupSafe                   2.1.2\n",
      "matplotlib                   3.7.1\n",
      "matplotlib-inline            0.1.6\n",
      "mistune                      2.0.5\n",
      "ml-dtypes                    0.2.0\n",
      "mpmath                       1.3.0\n",
      "nbclassic                    0.5.5\n",
      "nbclient                     0.7.3\n",
      "nbconvert                    7.3.1\n",
      "nbformat                     5.8.0\n",
      "nest-asyncio                 1.5.6\n",
      "networkx                     3.1\n",
      "notebook                     6.5.4\n",
      "notebook_shim                0.2.2\n",
      "numpy                        1.23.5\n",
      "oauthlib                     3.2.2\n",
      "opencv-contrib-python        4.7.0.72\n",
      "opencv-python                4.7.0.72\n",
      "opendatasets                 0.1.22\n",
      "openpyxl                     3.1.2\n",
      "opt-einsum                   3.3.0\n",
      "outcome                      1.2.0\n",
      "packaging                    23.1\n",
      "pandas                       2.0.0\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "pickleshare                  0.7.5\n",
      "Pillow                       9.5.0\n",
      "pip                          23.1.2\n",
      "platformdirs                 3.2.0\n",
      "prometheus-client            0.16.0\n",
      "prompt-toolkit               3.0.38\n",
      "protobuf                     4.23.3\n",
      "psutil                       5.9.5\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.0\n",
      "pyasn1-modules               0.3.0\n",
      "pycparser                    2.21\n",
      "Pygments                     2.15.1\n",
      "pyparsing                    3.0.9\n",
      "PyQt5                        5.15.9\n",
      "PyQt5-Qt5                    5.15.2\n",
      "PyQt5-sip                    12.12.1\n",
      "pyrsistent                   0.19.3\n",
      "PySocks                      1.7.1\n",
      "python-dateutil              2.8.2\n",
      "python-dotenv                1.0.0\n",
      "python-json-logger           2.0.7\n",
      "python-slugify               8.0.1\n",
      "pytz                         2023.3\n",
      "pywin32                      306\n",
      "pywinpty                     2.0.10\n",
      "PyYAML                       6.0\n",
      "pyzmq                        25.0.2\n",
      "regex                        2023.6.3\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rsa                          4.9\n",
      "s3transfer                   0.6.1\n",
      "safetensors                  0.3.1\n",
      "scikit-learn                 1.2.2\n",
      "scipy                        1.10.1\n",
      "seaborn                      0.12.2\n",
      "selenium                     4.9.0\n",
      "Send2Trash                   1.8.0\n",
      "sentencepiece                0.1.99\n",
      "setuptools                   68.0.0\n",
      "six                          1.16.0\n",
      "sniffio                      1.3.0\n",
      "sortedcontainers             2.4.0\n",
      "soupsieve                    2.4.1\n",
      "stack-data                   0.6.2\n",
      "sympy                        1.12\n",
      "tensorboard                  2.12.3\n",
      "tensorboard-data-server      0.7.1\n",
      "tensorflow                   2.12.0\n",
      "tensorflow-addons            0.20.0\n",
      "tensorflow-estimator         2.12.0\n",
      "tensorflow-intel             2.12.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.3.0\n",
      "terminado                    0.17.1\n",
      "text-unidecode               1.3\n",
      "threadpoolctl                3.1.0\n",
      "tinycss2                     1.2.1\n",
      "tokenizers                   0.13.3\n",
      "torch                        2.0.1\n",
      "torchaudio                   2.0.2\n",
      "torchdata                    0.6.1\n",
      "torchinfo                    1.8.0\n",
      "torchtext                    0.15.2\n",
      "torchvision                  0.15.2\n",
      "tornado                      6.3\n",
      "tqdm                         4.65.0\n",
      "traitlets                    5.9.0\n",
      "transformers                 4.30.2\n",
      "trio                         0.22.0\n",
      "trio-websocket               0.10.2\n",
      "typeguard                    2.13.3\n",
      "typing_extensions            4.6.2\n",
      "tzdata                       2023.3\n",
      "ultralytics                  8.0.117\n",
      "uri-template                 1.2.0\n",
      "urllib3                      1.26.16\n",
      "wcwidth                      0.2.6\n",
      "webcolors                    1.13\n",
      "webdriver-manager            3.8.6\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.5.1\n",
      "Werkzeug                     2.3.6\n",
      "wget                         3.2\n",
      "wheel                        0.40.0\n",
      "wrapt                        1.14.1\n",
      "wsproto                      1.2.0\n",
      "xgboost                      1.7.5\n",
      "xlrd                         2.0.1\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "153301c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BertModel.get_input_embeddings() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m re_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# # freezing\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# for param in resnet_pt.parameters():\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     param.requires_grad = False\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# resnet_pt.fc = nn.Linear(fc_in_features, len(classes))\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# resnet_pt = resnet_pt.to(device)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: BertModel.get_input_embeddings() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "re_model = model.get_input_embeddings( )\n",
    "# # freezing\n",
    "# for param in resnet_pt.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # fc layer 수정\n",
    "# fc_in_features = resnet_pt.fc.in_features\n",
    "# resnet_pt.fc = nn.Linear(fc_in_features, len(classes))\n",
    "# resnet_pt = resnet_pt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63fea713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BertModel.get_input_embeddings of BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (position_embeddings): Embedding(1024, 768)\n",
       ")>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ae366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a28b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f7aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd20d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a5f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d4662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c230972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7e0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
