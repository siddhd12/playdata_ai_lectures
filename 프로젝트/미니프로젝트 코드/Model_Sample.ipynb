{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26150,
     "status": "ok",
     "timestamp": 1688516510013,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "6BVPnGvAhXqu",
    "outputId": "2629104d-0263-4b31-972e-70f065f8f3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3_ickLgs2J7"
   },
   "source": [
    "# 1. BERT - beomi/kcbert-base\n",
    "- https://huggingface.co/beomi/kcbert-base\n",
    "- https://colab.research.google.com/drive/1dFC0FL-521m7CL_PSd8RLKq67jgTJVhL?usp=sharing\n",
    "- https://github.com/Beomi/KcBERT/releases/tag/TrainData_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47R6l7RPbJRt"
   },
   "source": [
    "### 1-1. Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1688517615340,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "rRKtx9vJswIo"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6910,
     "status": "ok",
     "timestamp": 1688517622723,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "Bjwvl_xKswGI",
    "outputId": "a2fbabe6-afda-4ae8-c6bb-e8286b94f7d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5162166357040405}]\n",
      "[2번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5739387273788452}]\n",
      "[3번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5745159387588501}]\n",
      "[4번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5371927618980408}]\n",
      "[5번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5698229074478149}]\n"
     ]
    }
   ],
   "source": [
    "# 모델과 토크나이저 불러오기\n",
    "model_name = \"beomi/kcbert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 문장 분류 함수\n",
    "def classify_sentiment(sentences):\n",
    "    # 문장을 토큰화하고 인코딩\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # 모델에 입력하여 예측 수행\n",
    "    outputs = model(**inputs)\n",
    "    # 로짓값을 확률로 변환하여 클래스 예측\n",
    "    predicted_class = outputs.logits.softmax(dim=1).argmax().item()\n",
    "    # 부정인 경우: 0, 긍정인 경우: 1로 반환\n",
    "    sentiment_label = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "    sentiment_score = outputs.logits.softmax(dim=1)[0][predicted_class].item()\n",
    "    result = [{\"label\": sentiment_label, \"score\": sentiment_score}]\n",
    "    return result\n",
    "\n",
    "# 문장 분류 예시\n",
    "sentences = [\"삼성전자가 베트남 현지 협력사의 경쟁력 제고를 위한 스마트팩토리 지원사업에 가속도를 내고 있다. 4일 외신과 업계에 따르면 삼성베트남은 베트남 산업통상부와 지난 3일 베트남 기업의 글로벌 공급망 참여 지원을 위한 스마트팩토리 개발 프로젝트에 돌입했다. 이번 프로젝트 1단계에서는 박닌, 하노이, 하남, 흥옌, 빈푹 등 베트남 북부 소재 12개 기업이 3주 교육 과정에 참여한다. 해당 프로젝트는 삼성전자와 산업통상자원부가 지난해 2월 체결한 스마트팩토리 개발협력 사업의 일환이다. 삼성전자는 올해까지 총 50개 기업의 스마트팩토리 공장 개발을 지원하고, 스마트공장 컨설팅 분야에서 베트남 전문가 100명을 양성할 예정이다. 최주호 삼성베트남단지 총괄이사는 '프로젝트를 통해 베트남 기업들은 베트남 기준에 맞춰 경쟁력을 강화할 것'이라며 '전 과정에 걸친 글로벌 스탠더드 등을 강조했다'고 말했다. 이어 '삼성과 글로벌 기업의 공급망에 참여할 수 있는 기회를 확대하는 우수한 기업들이 많이 생길 것'이라며 '스마트팩토리 구축 활동이 베트남의 4차 산업혁신 정책에 기여하는 원동력이 되기를 바란다'고 전했다.\",\n",
    "\"국내 가전업계 쌍두마차인 삼성전자와 LG전자가 오는 7일 발표할 2분기 실적에서 희비가 엇갈릴 전망이다. 증권가에서는 LG전자가 1분기에 이어 2분기에도 영업이익이 삼성전자를 넘어설 것이란 관측이 나온다. 3일 금융정보업체 에프앤가이드에 따르면 삼성전자의 2분기 영업이익 컨센서스는 전년 동기 대비 98.09% 감소한 2693억원이다. 같은 기간 LG전자의 예상 영업이익은 21.62% 증가한 9636억원이다. 삼성전자의 실적 악화 배경에는 핵심 사업인 메모리 반도체 수요 부진이 깔려있다. 앞서 삼성전자는 지난 1분기에도 DS 부문에서 4조5800억원의 영업적자를 기록했다. 증권가에서 제시한 반도체(DS) 부문 영업손실 추정치는 3조~4조원대에 달한다. SK증권과 이베스트투자증권에서는 올 2분기 DS부문 영업손실 규모를 4조4000억원, DB금융투자는 3조7000억원에 이를 것으로 전망했다. 반도체 부문의 손실을 스마트폰 부문이 메꾸는 양상이 될 것으로 예상된다. 스마트폰을 포함한 모바일·네트워크(MX·NW) 부문 영업이익 전망치는 SK증권이 3조1000억원, 이베스트투자증권은 2조8000억원, DB금융투자는 2조6000억원 등 대략 3조원 안팎일 것으로 증권사는 내다보고 있다. 반면 LG전자는 주력사업인 가전과 전장사업의 호조로 양호한 실적이 예상된다. 증권가에서는 생활가전(H&A) 사업본부가 올 2분기 6000억~7000억원의 영업이익을 거둘 것으로 내다봤다. 하이투자증권은 H&A사업의 2분기 영업이익을 6880억원, 이베스트투자증권은 7620억원으로 예측했다. 특히 LG전자의 전장사업이 주목받고 있다. 만년 적자에 시달렸던 전장사업은 지난해 처음으로 흑자전환에 성공했다. LG전자 전장사업의 수주 잔고는 지난해 말 80조원을 돌파했고 올해 100조원에 넘을 것으로 전망된다. 지난 1분기에도 LG전자는 1조5000억원에 육박하는 영업이익을 기록하면서 14년 만에 삼성전자를 추월했다. 삼성전자의 지난 1분기 영업이익은 6000억원에 그쳤다. 삼성전자는 반도체 업황 부진이 발목을 잡으면서 1분기에 이어 2분기에도 LG전자에 영업이익을 추월당할 것으로 보인다. 최근 2년간 양사의 연간 영업이익은 상당한 차이를 보였다. 삼성전자는 지난 2021년 51조6000억원대, 지난해 43조3000억원대의 영업이익을 기록했다. LG전자의 2021년 영업이익은 3조8000억원대, 지난해 3조5000억원대 수준이다. 올해 연간 실적도 삼성전자가 LG전자의 두배를 뛰어넘을 것으로 추정된다. 삼성전자의 연간 영업이익 컨센서스는 전년 동기 대비 77.99% 하락한 9조5451억원이다. 같은기간 LG전자의 예상 영업이익은 24.54% 증가한 4조4223억원이다. 반도체 업황이 살아나면서 다시 삼성전자의 이익 창출력은 본궤도에 진입할 전망이다. 한동희 SK증권 연구원은 “삼성전자는 감산 효과가 본격화되고 출하(수요)는 이미 저점을 지나고 있어 올 3분기부터 메모리 재고 하락이 본격화할 것”이라며 “가격 반등을 모색할 수 있는 구간으로 진입한다는 의미”라고 말했다.\",\n",
    "\"우리 기업의 핵심 자료를 해외로 빼돌리는 기술 유출 사건이 끊이지 않고 있습니다. 삼성전자와 SK하이닉스 등 영업기밀이 중요한 반도체 분야에서 특히 유출이 잦은데요. 국가 경쟁력과 직결되는 문제인데도 처벌은 솜방망이 수준이라는 지적이 나옵니다. 글로벌 기술경쟁이 심화하면서 해외 기술유출 수법은 점점 더 고도화되고 있습니다. 삼성전자에서는 전직 임원이 중국에 복제 공장을 설립하려는 일이 발생했습니다. 중국의 투자를 받아 삼성전자 직원들을 영입하고, 반도체 공장 설계도면과 공정 배치도 등 삼성전자의 영업 기밀을 빼돌린 겁니다. 단순한 기술 유출이 아니라 사실상 복제 공장을 지으려던 최초의 사례로, 삼성전자는 최소 3천억 원의 피해를 입은 것으로 추산됩니다. 삼성전자는 올해 초에도 자회사의 전 연구원이 반도체 세정장비 기술을 중국에 빼돌리는 피해를 입었습니다. 2021년에는 SK하이닉스 협력업체 연구소장이 반도체 제조 기술을, LG디스플레이 직원이 OLED 설계도를 유출하는 사고가 빚어졌습니다. 기술유출 건수 역시 증가 추세입니다. 산업스파이를 잡기 위한 경찰의 특별단속 결과 해외 기술 유출 사건은 최근 4달 간 8건으로 1년 사이 2배로 늘었습니다. 이렇게 국가 핵심산업 기술을 해외로 빼돌리면 3년 이상의 징역에 처해지지만, 대만이나 미국 등 다른 나라들의 처벌에 비하면 솜방망이 수준이라는 평가가 많습니다. 업계에서는 처벌을 강화해달라는 요구가 거세지고 있는데, 전국경제인연합회도 대법원에 양형기준을 높여달라는 의견서를 제출했습니다. '기술을 빼돌리는 것은 그야말로 공장을 폭파하는 겁니다. 경쟁력을 허물어뜨리는 거죠. 처벌 수위가 낮은데요. 도용된 기술 평가액의 수배에 해당하는 징벌적인 손해배상 청구를 해야 하고요.' 국회도 기술 유출에 대한 처벌 기준을 강화하는 내용의 법안을 발의하고 손해배상액을 최대 10배까지 높이는 방안을 검토 중입니다.\",\n",
    "\"삼성전자가 약 1년 2개월 만에 장중 7만원을 기록했다. 미국 반도체 기업 엔비디아가 호실적을 발표하면서 투자심리를 끌어올린 것으로 풀이된다. 25일 오전 9시 40분 현재 삼성전자는 전일 대비 700원(1.0%) 오른 6만9200원에 거래되고 있다. 개장 직후에는 7만원을 터치하면서 52주 신고가를 갈아치우기도 했다. 삼성전자가 장중 고가 기준 7만원대를 넘어선 것은 지난해 3월 31일(7만200원) 이후 약 1년 2개월 만이다. 이날 SK하이닉스도 장중 고가 기준 지난해 7월 29일(10만원) 이후 처음으로 10만원을 넘어섰다. 같은 시간 SK하이닉스는 전일 대비 5300원(5.4%) 상승한 10만3000원에 거래 중이다. 반도체주들의 강세는 전날 엔비디아가 시장 예상치를 크게 웃도는 ‘어닝서프라이즈’을 기록하면서 뉴욕 증시의 시간 외 거래에서 27%가량 폭등했기 때문으로 풀이된다. 엔비디아는 회계연도 2분기 매출이 시장 전망치를 50% 이상 웃도는 110억 달러(약 14조5310억원) 안팎에 이를 것으로 예상한다고 밝혔다. 회계연도 1분기(2∼4월) 매출도 71억9000만달러(약 9조4979억원)로 시장 전망치보다 약 10% 많았다.\",\n",
    "\"삼성전자가 ‘7만전자’ 회복을 눈앞에 두고 있다. 반도체 업황 반등에 대한 기대감으로 외국인의 매수세가 몰리면서다. 삼성전자의 주가가 7만원을 넘어 ‘9만전자’에 도달할 수 있다는 전망도 나왔다. 23일 한국거래소에 따르면 삼성전자 외국인 지분 비중은 52.19%로 집계됐다. 외국인 보유율이 52%대에 들어선 건 지난해 3월 4일 이후 처음이다. 이날 삼성전자는 전일 대비 100원(0.15%) 하락한 6만8400원에 약보합 마감했다. 연초 대비 23.24% 오른 가격이다. 전날에는 6만9000원까지 올라 52주 신고가를 기록하기도 했다. 삼성전자 주가 상승세는 외국인이 이끌고 있다. 외국인 투자자는 올 들어 삼성전자 주식을 9조1033억원 규모로 순매수했다. 이달 들어서만 1조2700억원을 사들였다. 올 하반기 메모리반도체 수급 안정화에 대한 기대감이 호재로 작용했다는 분석이다. 삼성전자는 지난달 올해 1분기 잠정 실적 발표와 함께 반도체 감산을 공식화했다. 하반기부터 반도체 재고 감소에 따라 가격이 안정화 될 것이라는 관측이 나온다. 증권사에서는 삼성전자의 목표주가를 올리고 있다. 유진투자증권과 유안타증권과 IBK투자증권은 증권사 중 최고 목표가인 9만원을 제시했다. 유진투자증권은 현재 메모리 사이클의 변곡점을 지나는 시점이라면서 목표주가를 기존 8만2000원에서 9만원으로 상향 조정했다. 이승우 유진투자증권 연구원은 이날 보고서에서 “조만간 실적도 주가 반등을 따라 최악의 시점을 통과하게 될 가능성이 높다”면서 “메모리 반도체는 감산이라는 카드로 충격을 흡수하면서 업황 반전을 꾀할 것”이라고 설명했다. 이 연구원은 “내년에 메모리 재고의 감소와 가격 반등이 진행되면 반도체 중심으로 큰 폭의 실적 개선이 가능할 전망”이라며 “내년 매출은 11% 증가한 307조원, 영업이익은 300% 급증한 40조4000억원에 이를 것”이라고 내다봤다. 업계에서는 삼성전자 실적이 2분기 바닥을 찍고 점차 회복할 것이라는 기대가 나온다. 삼성전자의 올해 실적은 매출 276조원, 영업이익 10조원으로 전년 대비 9%, 77% 감소할 것으로 전망된다. 이익 감소율은 역대 가장 큰 폭이 될 것으로 예상된다. 김동원 KB증권 연구원은 “메모리 반도체 제조사의 경우 2분기 이후 뚜렷한 재고감소 추세가 나타날 전망”이라면서 “하반기부터 반도체 업종은 재고감소, 가격하락 둔화, 감산에 따른 공급축소 등으로 분명한 수급개선이 예상된다”고 말했다.\"\n",
    "]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "  sentiment = classify_sentiment(sentences[i])\n",
    "  print(f\"[{i+1}번째 문장] 감성 분류 결과: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1688517579452,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "DsgN_f0zcfmv",
    "outputId": "cef404fd-cb14-4853-fcb2-06932affa689"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='beomi/kcbert-base', vocab_size=30000, model_max_length=300, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uNToeYRfuH-"
   },
   "source": [
    "#### 1-2. Large Fine Tuned Model\n",
    "- https://colab.research.google.com/drive/1dFC0FL-521m7CL_PSd8RLKq67jgTJVhL?usp=sharing#scrollTo=vx2mHvhGFYBp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19998,
     "status": "ok",
     "timestamp": 1688517792242,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "FDJdvx8wfbHU",
    "outputId": "4e69ad13-2243-4ef4-c5f1-f64fc430a723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.2/721.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.6/356.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers pytorch_lightning emoji soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13976,
     "status": "ok",
     "timestamp": 1688517818015,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "7ZLyZEV_fbDJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1688518148067,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "280SdY9Wg7Fo",
    "outputId": "b352e43f-8e1d-4f28-9384-4fafe028f81a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-69f72da3-8728-4d81-b156-1dce0a71d875\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>삼성전자가 베트남 현지 협력사의 경쟁력 제고를 위한 스마트팩토리 지원사업에 가속도를...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>국내 가전업계 쌍두마차인 삼성전자와 LG전자가 오는 7일 발표할 2분기 실적에서 희...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>우리 기업의 핵심 자료를 해외로 빼돌리는 기술 유출 사건이 끊이지 않고 있습니다. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>삼성전자가 약 1년 2개월 만에 장중 7만원을 기록했다. 미국 반도체 기업 엔비디아...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>삼성전자가 ‘7만전자’ 회복을 눈앞에 두고 있다. 반도체 업황 반등에 대한 기대감으...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69f72da3-8728-4d81-b156-1dce0a71d875')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-69f72da3-8728-4d81-b156-1dce0a71d875 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-69f72da3-8728-4d81-b156-1dce0a71d875');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                           sentences  target\n",
       "0  삼성전자가 베트남 현지 협력사의 경쟁력 제고를 위한 스마트팩토리 지원사업에 가속도를...       1\n",
       "1  국내 가전업계 쌍두마차인 삼성전자와 LG전자가 오는 7일 발표할 2분기 실적에서 희...       0\n",
       "2  우리 기업의 핵심 자료를 해외로 빼돌리는 기술 유출 사건이 끊이지 않고 있습니다. ...       0\n",
       "3  삼성전자가 약 1년 2개월 만에 장중 7만원을 기록했다. 미국 반도체 기업 엔비디아...       1\n",
       "4  삼성전자가 ‘7만전자’ 회복을 눈앞에 두고 있다. 반도체 업황 반등에 대한 기대감으...       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"삼성전자가 베트남 현지 협력사의 경쟁력 제고를 위한 스마트팩토리 지원사업에 가속도를 내고 있다. 4일 외신과 업계에 따르면 삼성베트남은 베트남 산업통상부와 지난 3일 베트남 기업의 글로벌 공급망 참여 지원을 위한 스마트팩토리 개발 프로젝트에 돌입했다. 이번 프로젝트 1단계에서는 박닌, 하노이, 하남, 흥옌, 빈푹 등 베트남 북부 소재 12개 기업이 3주 교육 과정에 참여한다. 해당 프로젝트는 삼성전자와 산업통상자원부가 지난해 2월 체결한 스마트팩토리 개발협력 사업의 일환이다. 삼성전자는 올해까지 총 50개 기업의 스마트팩토리 공장 개발을 지원하고, 스마트공장 컨설팅 분야에서 베트남 전문가 100명을 양성할 예정이다. 최주호 삼성베트남단지 총괄이사는 '프로젝트를 통해 베트남 기업들은 베트남 기준에 맞춰 경쟁력을 강화할 것'이라며 '전 과정에 걸친 글로벌 스탠더드 등을 강조했다'고 말했다. 이어 '삼성과 글로벌 기업의 공급망에 참여할 수 있는 기회를 확대하는 우수한 기업들이 많이 생길 것'이라며 '스마트팩토리 구축 활동이 베트남의 4차 산업혁신 정책에 기여하는 원동력이 되기를 바란다'고 전했다.\",\n",
    "\"국내 가전업계 쌍두마차인 삼성전자와 LG전자가 오는 7일 발표할 2분기 실적에서 희비가 엇갈릴 전망이다. 증권가에서는 LG전자가 1분기에 이어 2분기에도 영업이익이 삼성전자를 넘어설 것이란 관측이 나온다. 3일 금융정보업체 에프앤가이드에 따르면 삼성전자의 2분기 영업이익 컨센서스는 전년 동기 대비 98.09% 감소한 2693억원이다. 같은 기간 LG전자의 예상 영업이익은 21.62% 증가한 9636억원이다. 삼성전자의 실적 악화 배경에는 핵심 사업인 메모리 반도체 수요 부진이 깔려있다. 앞서 삼성전자는 지난 1분기에도 DS 부문에서 4조5800억원의 영업적자를 기록했다. 증권가에서 제시한 반도체(DS) 부문 영업손실 추정치는 3조~4조원대에 달한다. SK증권과 이베스트투자증권에서는 올 2분기 DS부문 영업손실 규모를 4조4000억원, DB금융투자는 3조7000억원에 이를 것으로 전망했다. 반도체 부문의 손실을 스마트폰 부문이 메꾸는 양상이 될 것으로 예상된다. 스마트폰을 포함한 모바일·네트워크(MX·NW) 부문 영업이익 전망치는 SK증권이 3조1000억원, 이베스트투자증권은 2조8000억원, DB금융투자는 2조6000억원 등 대략 3조원 안팎일 것으로 증권사는 내다보고 있다. 반면 LG전자는 주력사업인 가전과 전장사업의 호조로 양호한 실적이 예상된다. 증권가에서는 생활가전(H&A) 사업본부가 올 2분기 6000억~7000억원의 영업이익을 거둘 것으로 내다봤다. 하이투자증권은 H&A사업의 2분기 영업이익을 6880억원, 이베스트투자증권은 7620억원으로 예측했다. 특히 LG전자의 전장사업이 주목받고 있다. 만년 적자에 시달렸던 전장사업은 지난해 처음으로 흑자전환에 성공했다. LG전자 전장사업의 수주 잔고는 지난해 말 80조원을 돌파했고 올해 100조원에 넘을 것으로 전망된다. 지난 1분기에도 LG전자는 1조5000억원에 육박하는 영업이익을 기록하면서 14년 만에 삼성전자를 추월했다. 삼성전자의 지난 1분기 영업이익은 6000억원에 그쳤다. 삼성전자는 반도체 업황 부진이 발목을 잡으면서 1분기에 이어 2분기에도 LG전자에 영업이익을 추월당할 것으로 보인다. 최근 2년간 양사의 연간 영업이익은 상당한 차이를 보였다. 삼성전자는 지난 2021년 51조6000억원대, 지난해 43조3000억원대의 영업이익을 기록했다. LG전자의 2021년 영업이익은 3조8000억원대, 지난해 3조5000억원대 수준이다. 올해 연간 실적도 삼성전자가 LG전자의 두배를 뛰어넘을 것으로 추정된다. 삼성전자의 연간 영업이익 컨센서스는 전년 동기 대비 77.99% 하락한 9조5451억원이다. 같은기간 LG전자의 예상 영업이익은 24.54% 증가한 4조4223억원이다. 반도체 업황이 살아나면서 다시 삼성전자의 이익 창출력은 본궤도에 진입할 전망이다. 한동희 SK증권 연구원은 “삼성전자는 감산 효과가 본격화되고 출하(수요)는 이미 저점을 지나고 있어 올 3분기부터 메모리 재고 하락이 본격화할 것”이라며 “가격 반등을 모색할 수 있는 구간으로 진입한다는 의미”라고 말했다.\",\n",
    "\"우리 기업의 핵심 자료를 해외로 빼돌리는 기술 유출 사건이 끊이지 않고 있습니다. 삼성전자와 SK하이닉스 등 영업기밀이 중요한 반도체 분야에서 특히 유출이 잦은데요. 국가 경쟁력과 직결되는 문제인데도 처벌은 솜방망이 수준이라는 지적이 나옵니다. 글로벌 기술경쟁이 심화하면서 해외 기술유출 수법은 점점 더 고도화되고 있습니다. 삼성전자에서는 전직 임원이 중국에 복제 공장을 설립하려는 일이 발생했습니다. 중국의 투자를 받아 삼성전자 직원들을 영입하고, 반도체 공장 설계도면과 공정 배치도 등 삼성전자의 영업 기밀을 빼돌린 겁니다. 단순한 기술 유출이 아니라 사실상 복제 공장을 지으려던 최초의 사례로, 삼성전자는 최소 3천억 원의 피해를 입은 것으로 추산됩니다. 삼성전자는 올해 초에도 자회사의 전 연구원이 반도체 세정장비 기술을 중국에 빼돌리는 피해를 입었습니다. 2021년에는 SK하이닉스 협력업체 연구소장이 반도체 제조 기술을, LG디스플레이 직원이 OLED 설계도를 유출하는 사고가 빚어졌습니다. 기술유출 건수 역시 증가 추세입니다. 산업스파이를 잡기 위한 경찰의 특별단속 결과 해외 기술 유출 사건은 최근 4달 간 8건으로 1년 사이 2배로 늘었습니다. 이렇게 국가 핵심산업 기술을 해외로 빼돌리면 3년 이상의 징역에 처해지지만, 대만이나 미국 등 다른 나라들의 처벌에 비하면 솜방망이 수준이라는 평가가 많습니다. 업계에서는 처벌을 강화해달라는 요구가 거세지고 있는데, 전국경제인연합회도 대법원에 양형기준을 높여달라는 의견서를 제출했습니다. '기술을 빼돌리는 것은 그야말로 공장을 폭파하는 겁니다. 경쟁력을 허물어뜨리는 거죠. 처벌 수위가 낮은데요. 도용된 기술 평가액의 수배에 해당하는 징벌적인 손해배상 청구를 해야 하고요.' 국회도 기술 유출에 대한 처벌 기준을 강화하는 내용의 법안을 발의하고 손해배상액을 최대 10배까지 높이는 방안을 검토 중입니다.\",\n",
    "\"삼성전자가 약 1년 2개월 만에 장중 7만원을 기록했다. 미국 반도체 기업 엔비디아가 호실적을 발표하면서 투자심리를 끌어올린 것으로 풀이된다. 25일 오전 9시 40분 현재 삼성전자는 전일 대비 700원(1.0%) 오른 6만9200원에 거래되고 있다. 개장 직후에는 7만원을 터치하면서 52주 신고가를 갈아치우기도 했다. 삼성전자가 장중 고가 기준 7만원대를 넘어선 것은 지난해 3월 31일(7만200원) 이후 약 1년 2개월 만이다. 이날 SK하이닉스도 장중 고가 기준 지난해 7월 29일(10만원) 이후 처음으로 10만원을 넘어섰다. 같은 시간 SK하이닉스는 전일 대비 5300원(5.4%) 상승한 10만3000원에 거래 중이다. 반도체주들의 강세는 전날 엔비디아가 시장 예상치를 크게 웃도는 ‘어닝서프라이즈’을 기록하면서 뉴욕 증시의 시간 외 거래에서 27%가량 폭등했기 때문으로 풀이된다. 엔비디아는 회계연도 2분기 매출이 시장 전망치를 50% 이상 웃도는 110억 달러(약 14조5310억원) 안팎에 이를 것으로 예상한다고 밝혔다. 회계연도 1분기(2∼4월) 매출도 71억9000만달러(약 9조4979억원)로 시장 전망치보다 약 10% 많았다.\",\n",
    "\"삼성전자가 ‘7만전자’ 회복을 눈앞에 두고 있다. 반도체 업황 반등에 대한 기대감으로 외국인의 매수세가 몰리면서다. 삼성전자의 주가가 7만원을 넘어 ‘9만전자’에 도달할 수 있다는 전망도 나왔다. 23일 한국거래소에 따르면 삼성전자 외국인 지분 비중은 52.19%로 집계됐다. 외국인 보유율이 52%대에 들어선 건 지난해 3월 4일 이후 처음이다. 이날 삼성전자는 전일 대비 100원(0.15%) 하락한 6만8400원에 약보합 마감했다. 연초 대비 23.24% 오른 가격이다. 전날에는 6만9000원까지 올라 52주 신고가를 기록하기도 했다. 삼성전자 주가 상승세는 외국인이 이끌고 있다. 외국인 투자자는 올 들어 삼성전자 주식을 9조1033억원 규모로 순매수했다. 이달 들어서만 1조2700억원을 사들였다. 올 하반기 메모리반도체 수급 안정화에 대한 기대감이 호재로 작용했다는 분석이다. 삼성전자는 지난달 올해 1분기 잠정 실적 발표와 함께 반도체 감산을 공식화했다. 하반기부터 반도체 재고 감소에 따라 가격이 안정화 될 것이라는 관측이 나온다. 증권사에서는 삼성전자의 목표주가를 올리고 있다. 유진투자증권과 유안타증권과 IBK투자증권은 증권사 중 최고 목표가인 9만원을 제시했다. 유진투자증권은 현재 메모리 사이클의 변곡점을 지나는 시점이라면서 목표주가를 기존 8만2000원에서 9만원으로 상향 조정했다. 이승우 유진투자증권 연구원은 이날 보고서에서 “조만간 실적도 주가 반등을 따라 최악의 시점을 통과하게 될 가능성이 높다”면서 “메모리 반도체는 감산이라는 카드로 충격을 흡수하면서 업황 반전을 꾀할 것”이라고 설명했다. 이 연구원은 “내년에 메모리 재고의 감소와 가격 반등이 진행되면 반도체 중심으로 큰 폭의 실적 개선이 가능할 전망”이라며 “내년 매출은 11% 증가한 307조원, 영업이익은 300% 급증한 40조4000억원에 이를 것”이라고 내다봤다. 업계에서는 삼성전자 실적이 2분기 바닥을 찍고 점차 회복할 것이라는 기대가 나온다. 삼성전자의 올해 실적은 매출 276조원, 영업이익 10조원으로 전년 대비 9%, 77% 감소할 것으로 전망된다. 이익 감소율은 역대 가장 큰 폭이 될 것으로 예상된다. 김동원 KB증권 연구원은 “메모리 반도체 제조사의 경우 2분기 이후 뚜렷한 재고감소 추세가 나타날 전망”이라면서 “하반기부터 반도체 업종은 재고감소, 가격하락 둔화, 감산에 따른 공급축소 등으로 분명한 수급개선이 예상된다”고 말했다.\"\n",
    "]\n",
    "target = [1, 0, 0, 1, 1]\n",
    "\n",
    "sample_dataset = pd.DataFrame({'sentences': sentences, 'target': target})\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnrsNJbVgM1C"
   },
   "source": [
    "#### 기본학습 Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1688517818016,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "jGXf_0gufbAY"
   },
   "outputs": [],
   "source": [
    "class Arg:\n",
    "    random_seed: int = 42  # Random Seed\n",
    "    pretrained_model: str = 'beomi/kcbert-large'  # Transformers PLM name\n",
    "    pretrained_tokenizer: str = ''  # Optional, Transformers Tokenizer Name. Overrides `pretrained_model`\n",
    "    auto_batch_size: str = 'power'  # Let PyTorch Lightening find the best batch size\n",
    "    batch_size: int = 0  # Optional, Train/Eval Batch Size. Overrides `auto_batch_size`\n",
    "    lr: float = 5e-6  # Starting Learning Rate\n",
    "    epochs: int = 20  # Max Epochs\n",
    "    max_length: int = 150  # Max Length input size\n",
    "    report_cycle: int = 100  # Report (Train Metrics) Cycle\n",
    "    train_data_path: str = \"nsmc/ratings_train.txt\"  # Train Dataset file\n",
    "    val_data_path: str = \"nsmc/ratings_test.txt\"  # Validation Dataset file\n",
    "    cpu_workers: int = os.cpu_count()  # Multi cpu workers\n",
    "    test_mode: bool = False  # Test Mode enables `fast_dev_run`\n",
    "    optimizer: str = 'AdamW'  # AdamW vs AdamP\n",
    "    lr_scheduler: str = 'exp'  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
    "    fp16: bool = False  # Enable train on FP16\n",
    "    tpu_cores: int = 0  # Enable TPU with 1 core or 8 cores\n",
    "\n",
    "args = Arg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAZb_pP3gWuq"
   },
   "source": [
    "#### 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1688518530729,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "1-i1qITogLII"
   },
   "outputs": [],
   "source": [
    "class Model(LightningModule):\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        self.args = options\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(self.args.pretrained_model)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\n",
    "            self.args.pretrained_tokenizer\n",
    "            if self.args.pretrained_tokenizer\n",
    "            else self.args.pretrained_model\n",
    "        )\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        return self.bert(**kwargs)\n",
    "\n",
    "\n",
    "    # 모델 훈련\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        output = self(input_ids=data, labels=labels)\n",
    "\n",
    "        # Transformers 4.0.0+\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = labels.cpu().numpy()\n",
    "        y_pred = preds.cpu().numpy()\n",
    "\n",
    "        # Acc, Precision, Recall, F1\n",
    "        metrics = [\n",
    "            metric(y_true=y_true, y_pred=y_pred)\n",
    "            for metric in\n",
    "            (accuracy_score, precision_score, recall_score, f1_score)\n",
    "        ]\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'train_loss': loss.cpu().detach().numpy().tolist(),\n",
    "            'train_acc': metrics[0],\n",
    "            'train_precision': metrics[1],\n",
    "            'train_recall': metrics[2],\n",
    "            'train_f1': metrics[3],\n",
    "        }\n",
    "        if (batch_idx % self.args.report_cycle) == 0:\n",
    "            print()\n",
    "            pprint(tensorboard_logs)\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        output = self(input_ids=data, labels=labels)\n",
    "\n",
    "        # Transformers 4.0.0+\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for i in outputs:\n",
    "            loss += i['loss'].cpu().detach()\n",
    "        _loss = loss / len(outputs)\n",
    "\n",
    "        loss = float(_loss)\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for i in outputs:\n",
    "            y_true += i['y_true']\n",
    "            y_pred += i['y_pred']\n",
    "\n",
    "        # Acc, Precision, Recall, F1\n",
    "        metrics = [\n",
    "            metric(y_true=y_true, y_pred=y_pred)\n",
    "            for metric in\n",
    "            (accuracy_score, precision_score, recall_score, f1_score)\n",
    "        ]\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': metrics[0],\n",
    "            'val_precision': metrics[1],\n",
    "            'val_recall': metrics[2],\n",
    "            'val_f1': metrics[3],\n",
    "        }\n",
    "\n",
    "        print()\n",
    "        pprint(tensorboard_logs)\n",
    "        return {'loss': _loss, 'log': tensorboard_logs}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.args.optimizer == 'AdamW':\n",
    "            optimizer = AdamW(self.parameters(), lr=self.args.lr)\n",
    "        elif self.args.optimizer == 'AdamP':\n",
    "            from adamp import AdamP\n",
    "            optimizer = AdamP(self.parameters(), lr=self.args.lr)\n",
    "        else:\n",
    "            raise NotImplementedError('Only AdamW and AdamP is Supported!')\n",
    "        if self.args.lr_scheduler == 'cos':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
    "        elif self.args.lr_scheduler == 'exp':\n",
    "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "        else:\n",
    "            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "\n",
    "    # 데이터 로드 ===> 생략\n",
    "    # def read_data(self, path):\n",
    "    #     if path.endswith('xlsx'):\n",
    "    #         return pd.read_excel(path)\n",
    "    #     elif path.endswith('csv'):\n",
    "    #         return pd.read_csv(path)\n",
    "    #     elif path.endswith('tsv') or path.endswith('txt'):\n",
    "    #         return pd.read_csv(path, sep='\\t')\n",
    "    #     else:\n",
    "    #         raise NotImplementedError('Only Excel(xlsx)/Csv/Tsv(txt) are Supported')\n",
    "\n",
    "\n",
    "    # 데이터 전처리 ===> 생략\n",
    "    # def preprocess_dataframe(self, df):\n",
    "    #     emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
    "    #     pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
    "    #     url_pattern = re.compile(\n",
    "    #         r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "    #     def clean(x):\n",
    "    #         x = pattern.sub(' ', x)\n",
    "    #         x = url_pattern.sub('', x)\n",
    "    #         x = x.strip()\n",
    "    #         x = repeat_normalize(x, num_repeats=2)\n",
    "    #         return x\n",
    "\n",
    "    #     df['document'] = df['document'].map(lambda x: self.tokenizer.encode(\n",
    "    #         clean(str(x)),\n",
    "    #         padding='max_length',\n",
    "    #         max_length=self.args.max_length,\n",
    "    #         truncation=True,\n",
    "    #     ))\n",
    "    #     return df\n",
    "\n",
    "\n",
    "    # DataLoader - Train\n",
    "    def train_dataloader(self, sample_dataset):\n",
    "        df = sample_dataset\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(df['sentences'].to_list(), dtype=torch.long),\n",
    "            torch.tensor(df['target'].to_list(), dtype=torch.long),\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.args.batch_size or self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "\n",
    "    # DataLoader - val\n",
    "    # def val_dataloader(self):\n",
    "    #     df = self.read_data(self.args.val_data_path)\n",
    "    #     df = self.preprocess_dataframe(df)\n",
    "\n",
    "    #     dataset = TensorDataset(\n",
    "    #         torch.tensor(df['sentences'].to_list(), dtype=torch.long),\n",
    "    #         torch.tensor(df['target'].to_list(), dtype=torch.long),\n",
    "    #     )\n",
    "    #     return DataLoader(\n",
    "    #         dataset,\n",
    "    #         batch_size=self.args.batch_size or self.batch_size,\n",
    "    #         shuffle=False,\n",
    "    #         num_workers=self.args.cpu_workers,\n",
    "    #     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j685UCWNgbLk"
   },
   "source": [
    "#### 코드 실행부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1688518589626,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "edYkB39-gLF6"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Using PyTorch Ver\", torch.__version__)\n",
    "    print(\"Fix Seed:\", args.random_seed)\n",
    "    seed_everything(args.random_seed)\n",
    "    model = Model(args)\n",
    "\n",
    "    print(\":: Start Training ::\")\n",
    "    trainer = Trainer(\n",
    "        max_epochs=args.epochs,\n",
    "        fast_dev_run=args.test_mode,\n",
    "        num_sanity_val_steps=None if args.test_mode else 0,\n",
    "        auto_scale_batch_size=args.auto_batch_size if args.auto_batch_size and not args.batch_size else False,\n",
    "        # For GPU Setup\n",
    "        deterministic=torch.cuda.is_available(),\n",
    "        gpus=-1 if torch.cuda.is_available() else None,\n",
    "        precision=16 if args.fp16 else 32,\n",
    "        # For TPU Setup\n",
    "        # tpu_cores=args.tpu_cores if args.tpu_cores else None,\n",
    "    )\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tN59IEvCgLDh"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDcXeghygLBg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfIZ_jZTgK_W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgIoyMkdgK9C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qv-XW96SgK65"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VC2T08ccgK40"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2FeJEKOgK2q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYxBG_WwgKvZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAhGFdBGgKs3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g23sElNogKqX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDkqbmnRgKoI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwq7Tz4KgKlR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpHTpdlesCMg"
   },
   "source": [
    "# 2. BERT - sangrimlee/bert-base-multilingual-cased-nsmc\n",
    "- https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZlClA5grEby"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22034,
     "status": "ok",
     "timestamp": 1688487342279,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "hWyrZkQQg0Go",
    "outputId": "e946182f-f7f7-4263-a035-8a64bc57e0b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.6459546089172363}]\n",
      "[2번째 문장] 감성 분류 결과: [{'label': 'negative', 'score': 0.7543824911117554}]\n",
      "[3번째 문장] 감성 분류 결과: [{'label': 'negative', 'score': 0.7558901906013489}]\n",
      "[4번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5188146233558655}]\n",
      "[5번째 문장] 감성 분류 결과: [{'label': 'negative', 'score': 0.7167875170707703}]\n"
     ]
    }
   ],
   "source": [
    "# 모델과 토크나이저 불러오기\n",
    "model_name = \"sangrimlee/bert-base-multilingual-cased-nsmc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 문장 분류 함수\n",
    "def classify_sentiment(sentences):\n",
    "    # 문장을 토큰화하고 인코딩\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # 모델에 입력하여 예측 수행\n",
    "    outputs = model(**inputs)\n",
    "    # 로짓값을 확률로 변환하여 클래스 예측\n",
    "    predicted_class = outputs.logits.softmax(dim=1).argmax().item()\n",
    "    # 부정인 경우: 0, 긍정인 경우: 1로 반환\n",
    "    sentiment_label = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "    sentiment_score = outputs.logits.softmax(dim=1)[0][predicted_class].item()\n",
    "    result = [{\"label\": sentiment_label, \"score\": sentiment_score}]\n",
    "    return result\n",
    "\n",
    "# 문장 분류 예시\n",
    "sentences = [\"삼성전자가 베트남 현지 협력사의 경쟁력 제고를 위한 스마트팩토리 지원사업에 가속도를 내고 있다. 4일 외신과 업계에 따르면 삼성베트남은 베트남 산업통상부와 지난 3일 베트남 기업의 글로벌 공급망 참여 지원을 위한 스마트팩토리 개발 프로젝트에 돌입했다. 이번 프로젝트 1단계에서는 박닌, 하노이, 하남, 흥옌, 빈푹 등 베트남 북부 소재 12개 기업이 3주 교육 과정에 참여한다. 해당 프로젝트는 삼성전자와 산업통상자원부가 지난해 2월 체결한 스마트팩토리 개발협력 사업의 일환이다. 삼성전자는 올해까지 총 50개 기업의 스마트팩토리 공장 개발을 지원하고, 스마트공장 컨설팅 분야에서 베트남 전문가 100명을 양성할 예정이다. 최주호 삼성베트남단지 총괄이사는 '프로젝트를 통해 베트남 기업들은 베트남 기준에 맞춰 경쟁력을 강화할 것'이라며 '전 과정에 걸친 글로벌 스탠더드 등을 강조했다'고 말했다. 이어 '삼성과 글로벌 기업의 공급망에 참여할 수 있는 기회를 확대하는 우수한 기업들이 많이 생길 것'이라며 '스마트팩토리 구축 활동이 베트남의 4차 산업혁신 정책에 기여하는 원동력이 되기를 바란다'고 전했다.\",\n",
    "\"국내 가전업계 쌍두마차인 삼성전자와 LG전자가 오는 7일 발표할 2분기 실적에서 희비가 엇갈릴 전망이다. 증권가에서는 LG전자가 1분기에 이어 2분기에도 영업이익이 삼성전자를 넘어설 것이란 관측이 나온다. 3일 금융정보업체 에프앤가이드에 따르면 삼성전자의 2분기 영업이익 컨센서스는 전년 동기 대비 98.09% 감소한 2693억원이다. 같은 기간 LG전자의 예상 영업이익은 21.62% 증가한 9636억원이다. 삼성전자의 실적 악화 배경에는 핵심 사업인 메모리 반도체 수요 부진이 깔려있다. 앞서 삼성전자는 지난 1분기에도 DS 부문에서 4조5800억원의 영업적자를 기록했다. 증권가에서 제시한 반도체(DS) 부문 영업손실 추정치는 3조~4조원대에 달한다. SK증권과 이베스트투자증권에서는 올 2분기 DS부문 영업손실 규모를 4조4000억원, DB금융투자는 3조7000억원에 이를 것으로 전망했다. 반도체 부문의 손실을 스마트폰 부문이 메꾸는 양상이 될 것으로 예상된다. 스마트폰을 포함한 모바일·네트워크(MX·NW) 부문 영업이익 전망치는 SK증권이 3조1000억원, 이베스트투자증권은 2조8000억원, DB금융투자는 2조6000억원 등 대략 3조원 안팎일 것으로 증권사는 내다보고 있다. 반면 LG전자는 주력사업인 가전과 전장사업의 호조로 양호한 실적이 예상된다. 증권가에서는 생활가전(H&A) 사업본부가 올 2분기 6000억~7000억원의 영업이익을 거둘 것으로 내다봤다. 하이투자증권은 H&A사업의 2분기 영업이익을 6880억원, 이베스트투자증권은 7620억원으로 예측했다. 특히 LG전자의 전장사업이 주목받고 있다. 만년 적자에 시달렸던 전장사업은 지난해 처음으로 흑자전환에 성공했다. LG전자 전장사업의 수주 잔고는 지난해 말 80조원을 돌파했고 올해 100조원에 넘을 것으로 전망된다. 지난 1분기에도 LG전자는 1조5000억원에 육박하는 영업이익을 기록하면서 14년 만에 삼성전자를 추월했다. 삼성전자의 지난 1분기 영업이익은 6000억원에 그쳤다. 삼성전자는 반도체 업황 부진이 발목을 잡으면서 1분기에 이어 2분기에도 LG전자에 영업이익을 추월당할 것으로 보인다. 최근 2년간 양사의 연간 영업이익은 상당한 차이를 보였다. 삼성전자는 지난 2021년 51조6000억원대, 지난해 43조3000억원대의 영업이익을 기록했다. LG전자의 2021년 영업이익은 3조8000억원대, 지난해 3조5000억원대 수준이다. 올해 연간 실적도 삼성전자가 LG전자의 두배를 뛰어넘을 것으로 추정된다. 삼성전자의 연간 영업이익 컨센서스는 전년 동기 대비 77.99% 하락한 9조5451억원이다. 같은기간 LG전자의 예상 영업이익은 24.54% 증가한 4조4223억원이다. 반도체 업황이 살아나면서 다시 삼성전자의 이익 창출력은 본궤도에 진입할 전망이다. 한동희 SK증권 연구원은 “삼성전자는 감산 효과가 본격화되고 출하(수요)는 이미 저점을 지나고 있어 올 3분기부터 메모리 재고 하락이 본격화할 것”이라며 “가격 반등을 모색할 수 있는 구간으로 진입한다는 의미”라고 말했다.\",\n",
    "\"우리 기업의 핵심 자료를 해외로 빼돌리는 기술 유출 사건이 끊이지 않고 있습니다. 삼성전자와 SK하이닉스 등 영업기밀이 중요한 반도체 분야에서 특히 유출이 잦은데요. 국가 경쟁력과 직결되는 문제인데도 처벌은 솜방망이 수준이라는 지적이 나옵니다. 글로벌 기술경쟁이 심화하면서 해외 기술유출 수법은 점점 더 고도화되고 있습니다. 삼성전자에서는 전직 임원이 중국에 복제 공장을 설립하려는 일이 발생했습니다. 중국의 투자를 받아 삼성전자 직원들을 영입하고, 반도체 공장 설계도면과 공정 배치도 등 삼성전자의 영업 기밀을 빼돌린 겁니다. 단순한 기술 유출이 아니라 사실상 복제 공장을 지으려던 최초의 사례로, 삼성전자는 최소 3천억 원의 피해를 입은 것으로 추산됩니다. 삼성전자는 올해 초에도 자회사의 전 연구원이 반도체 세정장비 기술을 중국에 빼돌리는 피해를 입었습니다. 2021년에는 SK하이닉스 협력업체 연구소장이 반도체 제조 기술을, LG디스플레이 직원이 OLED 설계도를 유출하는 사고가 빚어졌습니다. 기술유출 건수 역시 증가 추세입니다. 산업스파이를 잡기 위한 경찰의 특별단속 결과 해외 기술 유출 사건은 최근 4달 간 8건으로 1년 사이 2배로 늘었습니다. 이렇게 국가 핵심산업 기술을 해외로 빼돌리면 3년 이상의 징역에 처해지지만, 대만이나 미국 등 다른 나라들의 처벌에 비하면 솜방망이 수준이라는 평가가 많습니다. 업계에서는 처벌을 강화해달라는 요구가 거세지고 있는데, 전국경제인연합회도 대법원에 양형기준을 높여달라는 의견서를 제출했습니다. '기술을 빼돌리는 것은 그야말로 공장을 폭파하는 겁니다. 경쟁력을 허물어뜨리는 거죠. 처벌 수위가 낮은데요. 도용된 기술 평가액의 수배에 해당하는 징벌적인 손해배상 청구를 해야 하고요.' 국회도 기술 유출에 대한 처벌 기준을 강화하는 내용의 법안을 발의하고 손해배상액을 최대 10배까지 높이는 방안을 검토 중입니다.\",\n",
    "\"삼성전자가 약 1년 2개월 만에 장중 7만원을 기록했다. 미국 반도체 기업 엔비디아가 호실적을 발표하면서 투자심리를 끌어올린 것으로 풀이된다. 25일 오전 9시 40분 현재 삼성전자는 전일 대비 700원(1.0%) 오른 6만9200원에 거래되고 있다. 개장 직후에는 7만원을 터치하면서 52주 신고가를 갈아치우기도 했다. 삼성전자가 장중 고가 기준 7만원대를 넘어선 것은 지난해 3월 31일(7만200원) 이후 약 1년 2개월 만이다. 이날 SK하이닉스도 장중 고가 기준 지난해 7월 29일(10만원) 이후 처음으로 10만원을 넘어섰다. 같은 시간 SK하이닉스는 전일 대비 5300원(5.4%) 상승한 10만3000원에 거래 중이다. 반도체주들의 강세는 전날 엔비디아가 시장 예상치를 크게 웃도는 ‘어닝서프라이즈’을 기록하면서 뉴욕 증시의 시간 외 거래에서 27%가량 폭등했기 때문으로 풀이된다. 엔비디아는 회계연도 2분기 매출이 시장 전망치를 50% 이상 웃도는 110억 달러(약 14조5310억원) 안팎에 이를 것으로 예상한다고 밝혔다. 회계연도 1분기(2∼4월) 매출도 71억9000만달러(약 9조4979억원)로 시장 전망치보다 약 10% 많았다.\",\n",
    "\"삼성전자가 ‘7만전자’ 회복을 눈앞에 두고 있다. 반도체 업황 반등에 대한 기대감으로 외국인의 매수세가 몰리면서다. 삼성전자의 주가가 7만원을 넘어 ‘9만전자’에 도달할 수 있다는 전망도 나왔다. 23일 한국거래소에 따르면 삼성전자 외국인 지분 비중은 52.19%로 집계됐다. 외국인 보유율이 52%대에 들어선 건 지난해 3월 4일 이후 처음이다. 이날 삼성전자는 전일 대비 100원(0.15%) 하락한 6만8400원에 약보합 마감했다. 연초 대비 23.24% 오른 가격이다. 전날에는 6만9000원까지 올라 52주 신고가를 기록하기도 했다. 삼성전자 주가 상승세는 외국인이 이끌고 있다. 외국인 투자자는 올 들어 삼성전자 주식을 9조1033억원 규모로 순매수했다. 이달 들어서만 1조2700억원을 사들였다. 올 하반기 메모리반도체 수급 안정화에 대한 기대감이 호재로 작용했다는 분석이다. 삼성전자는 지난달 올해 1분기 잠정 실적 발표와 함께 반도체 감산을 공식화했다. 하반기부터 반도체 재고 감소에 따라 가격이 안정화 될 것이라는 관측이 나온다. 증권사에서는 삼성전자의 목표주가를 올리고 있다. 유진투자증권과 유안타증권과 IBK투자증권은 증권사 중 최고 목표가인 9만원을 제시했다. 유진투자증권은 현재 메모리 사이클의 변곡점을 지나는 시점이라면서 목표주가를 기존 8만2000원에서 9만원으로 상향 조정했다. 이승우 유진투자증권 연구원은 이날 보고서에서 “조만간 실적도 주가 반등을 따라 최악의 시점을 통과하게 될 가능성이 높다”면서 “메모리 반도체는 감산이라는 카드로 충격을 흡수하면서 업황 반전을 꾀할 것”이라고 설명했다. 이 연구원은 “내년에 메모리 재고의 감소와 가격 반등이 진행되면 반도체 중심으로 큰 폭의 실적 개선이 가능할 전망”이라며 “내년 매출은 11% 증가한 307조원, 영업이익은 300% 급증한 40조4000억원에 이를 것”이라고 내다봤다. 업계에서는 삼성전자 실적이 2분기 바닥을 찍고 점차 회복할 것이라는 기대가 나온다. 삼성전자의 올해 실적은 매출 276조원, 영업이익 10조원으로 전년 대비 9%, 77% 감소할 것으로 전망된다. 이익 감소율은 역대 가장 큰 폭이 될 것으로 예상된다. 김동원 KB증권 연구원은 “메모리 반도체 제조사의 경우 2분기 이후 뚜렷한 재고감소 추세가 나타날 전망”이라면서 “하반기부터 반도체 업종은 재고감소, 가격하락 둔화, 감산에 따른 공급축소 등으로 분명한 수급개선이 예상된다”고 말했다.\"\n",
    "]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "  sentiment = classify_sentiment(sentences[i])\n",
    "  print(f\"[{i+1}번째 문장] 감성 분류 결과: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1xW-6uLwOXd"
   },
   "source": [
    "# 3. BERT - kykim/bert-kor-base\n",
    "- https://huggingface.co/kykim/bert-kor-base\n",
    "- https://github.com/kiyoungkim1/LMkor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1688517123373,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "MPD7OvehwfuS"
   },
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizerFast, BertModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14883,
     "status": "ok",
     "timestamp": 1688517277673,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "jHFpBKoBu3_M",
    "outputId": "3063ffae-5d09-41e1-cdf0-aa5886a29bdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.508989155292511}]\n",
      "[2번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5565291047096252}]\n",
      "[3번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.530027449131012}]\n",
      "[4번째 문장] 감성 분류 결과: [{'label': 'negative', 'score': 0.5343771576881409}]\n",
      "[5번째 문장] 감성 분류 결과: [{'label': 'negative', 'score': 0.5061792731285095}]\n"
     ]
    }
   ],
   "source": [
    "# 모델과 토크나이저 불러오기\n",
    "model_name = \"kykim/bert-kor-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 문장 분류 함수\n",
    "def classify_sentiment(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    predicted_class = outputs.logits.argmax(dim=1).item()\n",
    "    sentiment_label = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "    sentiment_score = outputs.logits.softmax(dim=1)[0][predicted_class].item()\n",
    "    result = [{\"label\": sentiment_label, \"score\": sentiment_score}]\n",
    "    return result\n",
    "\n",
    "# 문장 분류 예시\n",
    "sentences = [\"삼성전자가 베트남 현지 협력사의 경쟁력 제고를 위한 스마트팩토리 지원사업에 가속도를 내고 있다. 4일 외신과 업계에 따르면 삼성베트남은 베트남 산업통상부와 지난 3일 베트남 기업의 글로벌 공급망 참여 지원을 위한 스마트팩토리 개발 프로젝트에 돌입했다. 이번 프로젝트 1단계에서는 박닌, 하노이, 하남, 흥옌, 빈푹 등 베트남 북부 소재 12개 기업이 3주 교육 과정에 참여한다. 해당 프로젝트는 삼성전자와 산업통상자원부가 지난해 2월 체결한 스마트팩토리 개발협력 사업의 일환이다. 삼성전자는 올해까지 총 50개 기업의 스마트팩토리 공장 개발을 지원하고, 스마트공장 컨설팅 분야에서 베트남 전문가 100명을 양성할 예정이다. 최주호 삼성베트남단지 총괄이사는 '프로젝트를 통해 베트남 기업들은 베트남 기준에 맞춰 경쟁력을 강화할 것'이라며 '전 과정에 걸친 글로벌 스탠더드 등을 강조했다'고 말했다. 이어 '삼성과 글로벌 기업의 공급망에 참여할 수 있는 기회를 확대하는 우수한 기업들이 많이 생길 것'이라며 '스마트팩토리 구축 활동이 베트남의 4차 산업혁신 정책에 기여하는 원동력이 되기를 바란다'고 전했다.\",\n",
    "\"국내 가전업계 쌍두마차인 삼성전자와 LG전자가 오는 7일 발표할 2분기 실적에서 희비가 엇갈릴 전망이다. 증권가에서는 LG전자가 1분기에 이어 2분기에도 영업이익이 삼성전자를 넘어설 것이란 관측이 나온다. 3일 금융정보업체 에프앤가이드에 따르면 삼성전자의 2분기 영업이익 컨센서스는 전년 동기 대비 98.09% 감소한 2693억원이다. 같은 기간 LG전자의 예상 영업이익은 21.62% 증가한 9636억원이다. 삼성전자의 실적 악화 배경에는 핵심 사업인 메모리 반도체 수요 부진이 깔려있다. 앞서 삼성전자는 지난 1분기에도 DS 부문에서 4조5800억원의 영업적자를 기록했다. 증권가에서 제시한 반도체(DS) 부문 영업손실 추정치는 3조~4조원대에 달한다. SK증권과 이베스트투자증권에서는 올 2분기 DS부문 영업손실 규모를 4조4000억원, DB금융투자는 3조7000억원에 이를 것으로 전망했다. 반도체 부문의 손실을 스마트폰 부문이 메꾸는 양상이 될 것으로 예상된다. 스마트폰을 포함한 모바일·네트워크(MX·NW) 부문 영업이익 전망치는 SK증권이 3조1000억원, 이베스트투자증권은 2조8000억원, DB금융투자는 2조6000억원 등 대략 3조원 안팎일 것으로 증권사는 내다보고 있다. 반면 LG전자는 주력사업인 가전과 전장사업의 호조로 양호한 실적이 예상된다. 증권가에서는 생활가전(H&A) 사업본부가 올 2분기 6000억~7000억원의 영업이익을 거둘 것으로 내다봤다. 하이투자증권은 H&A사업의 2분기 영업이익을 6880억원, 이베스트투자증권은 7620억원으로 예측했다. 특히 LG전자의 전장사업이 주목받고 있다. 만년 적자에 시달렸던 전장사업은 지난해 처음으로 흑자전환에 성공했다. LG전자 전장사업의 수주 잔고는 지난해 말 80조원을 돌파했고 올해 100조원에 넘을 것으로 전망된다. 지난 1분기에도 LG전자는 1조5000억원에 육박하는 영업이익을 기록하면서 14년 만에 삼성전자를 추월했다. 삼성전자의 지난 1분기 영업이익은 6000억원에 그쳤다. 삼성전자는 반도체 업황 부진이 발목을 잡으면서 1분기에 이어 2분기에도 LG전자에 영업이익을 추월당할 것으로 보인다. 최근 2년간 양사의 연간 영업이익은 상당한 차이를 보였다. 삼성전자는 지난 2021년 51조6000억원대, 지난해 43조3000억원대의 영업이익을 기록했다. LG전자의 2021년 영업이익은 3조8000억원대, 지난해 3조5000억원대 수준이다. 올해 연간 실적도 삼성전자가 LG전자의 두배를 뛰어넘을 것으로 추정된다. 삼성전자의 연간 영업이익 컨센서스는 전년 동기 대비 77.99% 하락한 9조5451억원이다. 같은기간 LG전자의 예상 영업이익은 24.54% 증가한 4조4223억원이다. 반도체 업황이 살아나면서 다시 삼성전자의 이익 창출력은 본궤도에 진입할 전망이다. 한동희 SK증권 연구원은 “삼성전자는 감산 효과가 본격화되고 출하(수요)는 이미 저점을 지나고 있어 올 3분기부터 메모리 재고 하락이 본격화할 것”이라며 “가격 반등을 모색할 수 있는 구간으로 진입한다는 의미”라고 말했다.\",\n",
    "\"우리 기업의 핵심 자료를 해외로 빼돌리는 기술 유출 사건이 끊이지 않고 있습니다. 삼성전자와 SK하이닉스 등 영업기밀이 중요한 반도체 분야에서 특히 유출이 잦은데요. 국가 경쟁력과 직결되는 문제인데도 처벌은 솜방망이 수준이라는 지적이 나옵니다. 글로벌 기술경쟁이 심화하면서 해외 기술유출 수법은 점점 더 고도화되고 있습니다. 삼성전자에서는 전직 임원이 중국에 복제 공장을 설립하려는 일이 발생했습니다. 중국의 투자를 받아 삼성전자 직원들을 영입하고, 반도체 공장 설계도면과 공정 배치도 등 삼성전자의 영업 기밀을 빼돌린 겁니다. 단순한 기술 유출이 아니라 사실상 복제 공장을 지으려던 최초의 사례로, 삼성전자는 최소 3천억 원의 피해를 입은 것으로 추산됩니다. 삼성전자는 올해 초에도 자회사의 전 연구원이 반도체 세정장비 기술을 중국에 빼돌리는 피해를 입었습니다. 2021년에는 SK하이닉스 협력업체 연구소장이 반도체 제조 기술을, LG디스플레이 직원이 OLED 설계도를 유출하는 사고가 빚어졌습니다. 기술유출 건수 역시 증가 추세입니다. 산업스파이를 잡기 위한 경찰의 특별단속 결과 해외 기술 유출 사건은 최근 4달 간 8건으로 1년 사이 2배로 늘었습니다. 이렇게 국가 핵심산업 기술을 해외로 빼돌리면 3년 이상의 징역에 처해지지만, 대만이나 미국 등 다른 나라들의 처벌에 비하면 솜방망이 수준이라는 평가가 많습니다. 업계에서는 처벌을 강화해달라는 요구가 거세지고 있는데, 전국경제인연합회도 대법원에 양형기준을 높여달라는 의견서를 제출했습니다. '기술을 빼돌리는 것은 그야말로 공장을 폭파하는 겁니다. 경쟁력을 허물어뜨리는 거죠. 처벌 수위가 낮은데요. 도용된 기술 평가액의 수배에 해당하는 징벌적인 손해배상 청구를 해야 하고요.' 국회도 기술 유출에 대한 처벌 기준을 강화하는 내용의 법안을 발의하고 손해배상액을 최대 10배까지 높이는 방안을 검토 중입니다.\",\n",
    "\"삼성전자가 약 1년 2개월 만에 장중 7만원을 기록했다. 미국 반도체 기업 엔비디아가 호실적을 발표하면서 투자심리를 끌어올린 것으로 풀이된다. 25일 오전 9시 40분 현재 삼성전자는 전일 대비 700원(1.0%) 오른 6만9200원에 거래되고 있다. 개장 직후에는 7만원을 터치하면서 52주 신고가를 갈아치우기도 했다. 삼성전자가 장중 고가 기준 7만원대를 넘어선 것은 지난해 3월 31일(7만200원) 이후 약 1년 2개월 만이다. 이날 SK하이닉스도 장중 고가 기준 지난해 7월 29일(10만원) 이후 처음으로 10만원을 넘어섰다. 같은 시간 SK하이닉스는 전일 대비 5300원(5.4%) 상승한 10만3000원에 거래 중이다. 반도체주들의 강세는 전날 엔비디아가 시장 예상치를 크게 웃도는 ‘어닝서프라이즈’을 기록하면서 뉴욕 증시의 시간 외 거래에서 27%가량 폭등했기 때문으로 풀이된다. 엔비디아는 회계연도 2분기 매출이 시장 전망치를 50% 이상 웃도는 110억 달러(약 14조5310억원) 안팎에 이를 것으로 예상한다고 밝혔다. 회계연도 1분기(2∼4월) 매출도 71억9000만달러(약 9조4979억원)로 시장 전망치보다 약 10% 많았다.\",\n",
    "\"삼성전자가 ‘7만전자’ 회복을 눈앞에 두고 있다. 반도체 업황 반등에 대한 기대감으로 외국인의 매수세가 몰리면서다. 삼성전자의 주가가 7만원을 넘어 ‘9만전자’에 도달할 수 있다는 전망도 나왔다. 23일 한국거래소에 따르면 삼성전자 외국인 지분 비중은 52.19%로 집계됐다. 외국인 보유율이 52%대에 들어선 건 지난해 3월 4일 이후 처음이다. 이날 삼성전자는 전일 대비 100원(0.15%) 하락한 6만8400원에 약보합 마감했다. 연초 대비 23.24% 오른 가격이다. 전날에는 6만9000원까지 올라 52주 신고가를 기록하기도 했다. 삼성전자 주가 상승세는 외국인이 이끌고 있다. 외국인 투자자는 올 들어 삼성전자 주식을 9조1033억원 규모로 순매수했다. 이달 들어서만 1조2700억원을 사들였다. 올 하반기 메모리반도체 수급 안정화에 대한 기대감이 호재로 작용했다는 분석이다. 삼성전자는 지난달 올해 1분기 잠정 실적 발표와 함께 반도체 감산을 공식화했다. 하반기부터 반도체 재고 감소에 따라 가격이 안정화 될 것이라는 관측이 나온다. 증권사에서는 삼성전자의 목표주가를 올리고 있다. 유진투자증권과 유안타증권과 IBK투자증권은 증권사 중 최고 목표가인 9만원을 제시했다. 유진투자증권은 현재 메모리 사이클의 변곡점을 지나는 시점이라면서 목표주가를 기존 8만2000원에서 9만원으로 상향 조정했다. 이승우 유진투자증권 연구원은 이날 보고서에서 “조만간 실적도 주가 반등을 따라 최악의 시점을 통과하게 될 가능성이 높다”면서 “메모리 반도체는 감산이라는 카드로 충격을 흡수하면서 업황 반전을 꾀할 것”이라고 설명했다. 이 연구원은 “내년에 메모리 재고의 감소와 가격 반등이 진행되면 반도체 중심으로 큰 폭의 실적 개선이 가능할 전망”이라며 “내년 매출은 11% 증가한 307조원, 영업이익은 300% 급증한 40조4000억원에 이를 것”이라고 내다봤다. 업계에서는 삼성전자 실적이 2분기 바닥을 찍고 점차 회복할 것이라는 기대가 나온다. 삼성전자의 올해 실적은 매출 276조원, 영업이익 10조원으로 전년 대비 9%, 77% 감소할 것으로 전망된다. 이익 감소율은 역대 가장 큰 폭이 될 것으로 예상된다. 김동원 KB증권 연구원은 “메모리 반도체 제조사의 경우 2분기 이후 뚜렷한 재고감소 추세가 나타날 전망”이라면서 “하반기부터 반도체 업종은 재고감소, 가격하락 둔화, 감산에 따른 공급축소 등으로 분명한 수급개선이 예상된다”고 말했다.\"\n",
    "]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "  sentiment = classify_sentiment(sentences[i])\n",
    "  print(f\"[{i+1}번째 문장] 감성 분류 결과: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZYxm9gceV4a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdQPplH0eV2B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHFJMni_aI-0"
   },
   "source": [
    "# 4. GPT - kykim/gpt3-kor-small_based_on_gpt2\n",
    "- https://huggingface.co/kykim/gpt3-kor-small_based_on_gpt2\n",
    "- https://github.com/kiyoungkim1/LMkor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BchSeOayDZr"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, GPT2LMHeadModel\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11144,
     "status": "ok",
     "timestamp": 1688486428688,
     "user": {
      "displayName": "David Choi",
      "userId": "14870253108450991755"
     },
     "user_tz": -540
    },
    "id": "pSNvPP43yJ6g",
    "outputId": "d1721d33-f128-478d-9f23-d4bbb32f0900"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at kykim/gpt3-kor-small_based_on_gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5016430020332336}]\n",
      "[2번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5222628712654114}]\n",
      "[3번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5000121593475342}]\n",
      "[4번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5151134729385376}]\n",
      "[5번째 문장] 감성 분류 결과: [{'label': 'positive', 'score': 0.5302659273147583}]\n"
     ]
    }
   ],
   "source": [
    "# 모델과 토크나이저 불러오기\n",
    "model_name = \"kykim/gpt3-kor-small_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 문장 분류 함수\n",
    "def classify_sentiment(sentences):\n",
    "    # 문장을 토큰화하고 인코딩\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # 모델에 입력하여 예측 수행\n",
    "    outputs = model(**inputs)\n",
    "    # 로짓값을 확률로 변환하여 클래스 예측\n",
    "    predicted_class = outputs.logits.softmax(dim=1).argmax().item()\n",
    "    # 부정인 경우: 0, 긍정인 경우: 1로 반환\n",
    "    sentiment_label = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "    sentiment_score = outputs.logits.softmax(dim=1)[0][predicted_class].item()\n",
    "    result = [{\"label\": sentiment_label, \"score\": sentiment_score}]\n",
    "    return result\n",
    "\n",
    "# 문장 분류 예시\n",
    "sentences = [\"삼성전자가 베트남 현지 협력사의 경쟁력 제고를 위한 스마트팩토리 지원사업에 가속도를 내고 있다. 4일 외신과 업계에 따르면 삼성베트남은 베트남 산업통상부와 지난 3일 베트남 기업의 글로벌 공급망 참여 지원을 위한 스마트팩토리 개발 프로젝트에 돌입했다. 이번 프로젝트 1단계에서는 박닌, 하노이, 하남, 흥옌, 빈푹 등 베트남 북부 소재 12개 기업이 3주 교육 과정에 참여한다. 해당 프로젝트는 삼성전자와 산업통상자원부가 지난해 2월 체결한 스마트팩토리 개발협력 사업의 일환이다. 삼성전자는 올해까지 총 50개 기업의 스마트팩토리 공장 개발을 지원하고, 스마트공장 컨설팅 분야에서 베트남 전문가 100명을 양성할 예정이다. 최주호 삼성베트남단지 총괄이사는 '프로젝트를 통해 베트남 기업들은 베트남 기준에 맞춰 경쟁력을 강화할 것'이라며 '전 과정에 걸친 글로벌 스탠더드 등을 강조했다'고 말했다. 이어 '삼성과 글로벌 기업의 공급망에 참여할 수 있는 기회를 확대하는 우수한 기업들이 많이 생길 것'이라며 '스마트팩토리 구축 활동이 베트남의 4차 산업혁신 정책에 기여하는 원동력이 되기를 바란다'고 전했다.\",\n",
    "\"국내 가전업계 쌍두마차인 삼성전자와 LG전자가 오는 7일 발표할 2분기 실적에서 희비가 엇갈릴 전망이다. 증권가에서는 LG전자가 1분기에 이어 2분기에도 영업이익이 삼성전자를 넘어설 것이란 관측이 나온다. 3일 금융정보업체 에프앤가이드에 따르면 삼성전자의 2분기 영업이익 컨센서스는 전년 동기 대비 98.09% 감소한 2693억원이다. 같은 기간 LG전자의 예상 영업이익은 21.62% 증가한 9636억원이다. 삼성전자의 실적 악화 배경에는 핵심 사업인 메모리 반도체 수요 부진이 깔려있다. 앞서 삼성전자는 지난 1분기에도 DS 부문에서 4조5800억원의 영업적자를 기록했다. 증권가에서 제시한 반도체(DS) 부문 영업손실 추정치는 3조~4조원대에 달한다. SK증권과 이베스트투자증권에서는 올 2분기 DS부문 영업손실 규모를 4조4000억원, DB금융투자는 3조7000억원에 이를 것으로 전망했다. 반도체 부문의 손실을 스마트폰 부문이 메꾸는 양상이 될 것으로 예상된다. 스마트폰을 포함한 모바일·네트워크(MX·NW) 부문 영업이익 전망치는 SK증권이 3조1000억원, 이베스트투자증권은 2조8000억원, DB금융투자는 2조6000억원 등 대략 3조원 안팎일 것으로 증권사는 내다보고 있다. 반면 LG전자는 주력사업인 가전과 전장사업의 호조로 양호한 실적이 예상된다. 증권가에서는 생활가전(H&A) 사업본부가 올 2분기 6000억~7000억원의 영업이익을 거둘 것으로 내다봤다. 하이투자증권은 H&A사업의 2분기 영업이익을 6880억원, 이베스트투자증권은 7620억원으로 예측했다. 특히 LG전자의 전장사업이 주목받고 있다. 만년 적자에 시달렸던 전장사업은 지난해 처음으로 흑자전환에 성공했다. LG전자 전장사업의 수주 잔고는 지난해 말 80조원을 돌파했고 올해 100조원에 넘을 것으로 전망된다. 지난 1분기에도 LG전자는 1조5000억원에 육박하는 영업이익을 기록하면서 14년 만에 삼성전자를 추월했다. 삼성전자의 지난 1분기 영업이익은 6000억원에 그쳤다. 삼성전자는 반도체 업황 부진이 발목을 잡으면서 1분기에 이어 2분기에도 LG전자에 영업이익을 추월당할 것으로 보인다. 최근 2년간 양사의 연간 영업이익은 상당한 차이를 보였다. 삼성전자는 지난 2021년 51조6000억원대, 지난해 43조3000억원대의 영업이익을 기록했다. LG전자의 2021년 영업이익은 3조8000억원대, 지난해 3조5000억원대 수준이다. 올해 연간 실적도 삼성전자가 LG전자의 두배를 뛰어넘을 것으로 추정된다. 삼성전자의 연간 영업이익 컨센서스는 전년 동기 대비 77.99% 하락한 9조5451억원이다. 같은기간 LG전자의 예상 영업이익은 24.54% 증가한 4조4223억원이다. 반도체 업황이 살아나면서 다시 삼성전자의 이익 창출력은 본궤도에 진입할 전망이다. 한동희 SK증권 연구원은 “삼성전자는 감산 효과가 본격화되고 출하(수요)는 이미 저점을 지나고 있어 올 3분기부터 메모리 재고 하락이 본격화할 것”이라며 “가격 반등을 모색할 수 있는 구간으로 진입한다는 의미”라고 말했다.\",\n",
    "\"우리 기업의 핵심 자료를 해외로 빼돌리는 기술 유출 사건이 끊이지 않고 있습니다. 삼성전자와 SK하이닉스 등 영업기밀이 중요한 반도체 분야에서 특히 유출이 잦은데요. 국가 경쟁력과 직결되는 문제인데도 처벌은 솜방망이 수준이라는 지적이 나옵니다. 글로벌 기술경쟁이 심화하면서 해외 기술유출 수법은 점점 더 고도화되고 있습니다. 삼성전자에서는 전직 임원이 중국에 복제 공장을 설립하려는 일이 발생했습니다. 중국의 투자를 받아 삼성전자 직원들을 영입하고, 반도체 공장 설계도면과 공정 배치도 등 삼성전자의 영업 기밀을 빼돌린 겁니다. 단순한 기술 유출이 아니라 사실상 복제 공장을 지으려던 최초의 사례로, 삼성전자는 최소 3천억 원의 피해를 입은 것으로 추산됩니다. 삼성전자는 올해 초에도 자회사의 전 연구원이 반도체 세정장비 기술을 중국에 빼돌리는 피해를 입었습니다. 2021년에는 SK하이닉스 협력업체 연구소장이 반도체 제조 기술을, LG디스플레이 직원이 OLED 설계도를 유출하는 사고가 빚어졌습니다. 기술유출 건수 역시 증가 추세입니다. 산업스파이를 잡기 위한 경찰의 특별단속 결과 해외 기술 유출 사건은 최근 4달 간 8건으로 1년 사이 2배로 늘었습니다. 이렇게 국가 핵심산업 기술을 해외로 빼돌리면 3년 이상의 징역에 처해지지만, 대만이나 미국 등 다른 나라들의 처벌에 비하면 솜방망이 수준이라는 평가가 많습니다. 업계에서는 처벌을 강화해달라는 요구가 거세지고 있는데, 전국경제인연합회도 대법원에 양형기준을 높여달라는 의견서를 제출했습니다. '기술을 빼돌리는 것은 그야말로 공장을 폭파하는 겁니다. 경쟁력을 허물어뜨리는 거죠. 처벌 수위가 낮은데요. 도용된 기술 평가액의 수배에 해당하는 징벌적인 손해배상 청구를 해야 하고요.' 국회도 기술 유출에 대한 처벌 기준을 강화하는 내용의 법안을 발의하고 손해배상액을 최대 10배까지 높이는 방안을 검토 중입니다.\",\n",
    "\"삼성전자가 약 1년 2개월 만에 장중 7만원을 기록했다. 미국 반도체 기업 엔비디아가 호실적을 발표하면서 투자심리를 끌어올린 것으로 풀이된다. 25일 오전 9시 40분 현재 삼성전자는 전일 대비 700원(1.0%) 오른 6만9200원에 거래되고 있다. 개장 직후에는 7만원을 터치하면서 52주 신고가를 갈아치우기도 했다. 삼성전자가 장중 고가 기준 7만원대를 넘어선 것은 지난해 3월 31일(7만200원) 이후 약 1년 2개월 만이다. 이날 SK하이닉스도 장중 고가 기준 지난해 7월 29일(10만원) 이후 처음으로 10만원을 넘어섰다. 같은 시간 SK하이닉스는 전일 대비 5300원(5.4%) 상승한 10만3000원에 거래 중이다. 반도체주들의 강세는 전날 엔비디아가 시장 예상치를 크게 웃도는 ‘어닝서프라이즈’을 기록하면서 뉴욕 증시의 시간 외 거래에서 27%가량 폭등했기 때문으로 풀이된다. 엔비디아는 회계연도 2분기 매출이 시장 전망치를 50% 이상 웃도는 110억 달러(약 14조5310억원) 안팎에 이를 것으로 예상한다고 밝혔다. 회계연도 1분기(2∼4월) 매출도 71억9000만달러(약 9조4979억원)로 시장 전망치보다 약 10% 많았다.\",\n",
    "\"삼성전자가 ‘7만전자’ 회복을 눈앞에 두고 있다. 반도체 업황 반등에 대한 기대감으로 외국인의 매수세가 몰리면서다. 삼성전자의 주가가 7만원을 넘어 ‘9만전자’에 도달할 수 있다는 전망도 나왔다. 23일 한국거래소에 따르면 삼성전자 외국인 지분 비중은 52.19%로 집계됐다. 외국인 보유율이 52%대에 들어선 건 지난해 3월 4일 이후 처음이다. 이날 삼성전자는 전일 대비 100원(0.15%) 하락한 6만8400원에 약보합 마감했다. 연초 대비 23.24% 오른 가격이다. 전날에는 6만9000원까지 올라 52주 신고가를 기록하기도 했다. 삼성전자 주가 상승세는 외국인이 이끌고 있다. 외국인 투자자는 올 들어 삼성전자 주식을 9조1033억원 규모로 순매수했다. 이달 들어서만 1조2700억원을 사들였다. 올 하반기 메모리반도체 수급 안정화에 대한 기대감이 호재로 작용했다는 분석이다. 삼성전자는 지난달 올해 1분기 잠정 실적 발표와 함께 반도체 감산을 공식화했다. 하반기부터 반도체 재고 감소에 따라 가격이 안정화 될 것이라는 관측이 나온다. 증권사에서는 삼성전자의 목표주가를 올리고 있다. 유진투자증권과 유안타증권과 IBK투자증권은 증권사 중 최고 목표가인 9만원을 제시했다. 유진투자증권은 현재 메모리 사이클의 변곡점을 지나는 시점이라면서 목표주가를 기존 8만2000원에서 9만원으로 상향 조정했다. 이승우 유진투자증권 연구원은 이날 보고서에서 “조만간 실적도 주가 반등을 따라 최악의 시점을 통과하게 될 가능성이 높다”면서 “메모리 반도체는 감산이라는 카드로 충격을 흡수하면서 업황 반전을 꾀할 것”이라고 설명했다. 이 연구원은 “내년에 메모리 재고의 감소와 가격 반등이 진행되면 반도체 중심으로 큰 폭의 실적 개선이 가능할 전망”이라며 “내년 매출은 11% 증가한 307조원, 영업이익은 300% 급증한 40조4000억원에 이를 것”이라고 내다봤다. 업계에서는 삼성전자 실적이 2분기 바닥을 찍고 점차 회복할 것이라는 기대가 나온다. 삼성전자의 올해 실적은 매출 276조원, 영업이익 10조원으로 전년 대비 9%, 77% 감소할 것으로 전망된다. 이익 감소율은 역대 가장 큰 폭이 될 것으로 예상된다. 김동원 KB증권 연구원은 “메모리 반도체 제조사의 경우 2분기 이후 뚜렷한 재고감소 추세가 나타날 전망”이라면서 “하반기부터 반도체 업종은 재고감소, 가격하락 둔화, 감산에 따른 공급축소 등으로 분명한 수급개선이 예상된다”고 말했다.\"\n",
    "]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "  sentiment = classify_sentiment(sentences[i])\n",
    "  print(f\"[{i+1}번째 문장] 감성 분류 결과: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMEePtgooALD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNdcsJvQWUZq6EavjCa8HHl",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
