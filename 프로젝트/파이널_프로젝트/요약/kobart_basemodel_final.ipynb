{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mQKDlSmxVVr3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\""],"metadata":{"id":"1nZMzDNB6oGA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 필요한 모듈 install"],"metadata":{"id":"3dT4v3a_aZMi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsKTJfE8VgNK"},"outputs":[],"source":[" !pip install accelerate>=0.20.1\n"," !pip install transformers\n"," !pip install pytorch-lightning"]},{"cell_type":"code","source":["from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n","    DataCollatorForSeq2Seq,\n","    BartConfig\n",")\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","import pandas as pd\n","import random"],"metadata":{"id":"VcgWlnZLA4Ex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 데이터 로드 - 전체 데이터 9:1"],"metadata":{"id":"a5rwJChqHobs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4uY9GJLVhG4"},"outputs":[],"source":["data_path = '/content/drive/MyDrive/final_models/요약/data/data_100_1000/'\n","data_size = 100\n","\n","train_df = pd.read_csv(f'{data_path}summary_train_dataset_{data_size}.tsv', sep='\\t')\n","valid_df = pd.read_csv(f'{data_path}summary_val_dataset_{data_size}.tsv', sep='\\t')"]},{"cell_type":"markdown","source":["# 모델, tokenizer 설정"],"metadata":{"id":"dBNWzu5RHuJ1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSQ6VN52VjRR"},"outputs":[],"source":["model_name = 'digit82/kobart-summarization'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"]},{"cell_type":"markdown","source":["# 데이터 전처리\n","- target 데이터에 bos, eos 붙이기\n","- max_length : 1024"],"metadata":{"id":"Z6mM0ZstH_R6"}},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, tokenizer, max_length=1024):\n","        self.max_length = max_length\n","        self.tokenizer = tokenizer\n","\n","    def get_inputs(self, df):\n","        input_pairs = []\n","\n","        for i in range(len(df)):\n","            input_text = df.iloc[i, 0]\n","            target_text = df.iloc[i, 1]\n","\n","            input_ids = self.tokenizer(input_text, padding=\"max_length\", max_length=self.max_length, truncation=True)[\"input_ids\"]\n","            attention = self.tokenizer(input_text, padding=\"max_length\", max_length=self.max_length, truncation=True)[\"attention_mask\"]\n","            target_ids = self.tokenizer('<s>' + target_text + '</s>', padding=\"max_length\", max_length=self.max_length, truncation=True)[\"input_ids\"]\n","\n","            input_pair = {'input_ids' : torch.LongTensor(input_ids),\n","                    'attention_mask' : torch.LongTensor(attention),\n","                    'labels' : torch.LongTensor(target_ids)}\n","\n","            input_pairs.append(input_pair)\n","\n","        return input_pairs\n","\n","    def get_input_ids(self, df):\n","        input_ids_list = []\n","\n","        for i in range(len(df)):\n","            input_text = df.iloc[i, 0]\n","            target_text = df.iloc[i, 1]\n","\n","            input_ids = self.tokenizer(input_text, padding=\"max_length\", max_length=self.max_length, truncation=True)[\"input_ids\"]\n","            input_ids_list.append(input_ids)\n","\n","        return torch.tensor(input_ids_list)"],"metadata":{"id":"4iSfJP9w7jYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7Nd9BlfVytf"},"outputs":[],"source":["custom_dataset = CustomDataset(tokenizer=tokenizer)\n","\n","dataset_train = custom_dataset.get_inputs(train_df)\n","dataset_val = custom_dataset.get_inputs(valid_df)"]},{"cell_type":"code","source":["print(f'dataset_train len : {len(dataset_train)}')\n","dataset_train[0]"],"metadata":{"id":"6THiYe0rQSUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'dataset_val len : {len(dataset_val)}')\n","dataset_val[0]"],"metadata":{"id":"KlR2xYKTQWxW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-Tuning\n","- decoder_layers : 디코더 개수\n","- lm_head : 마지막 linear 레이어"],"metadata":{"id":"WuvpzS9XR2rD"}},{"cell_type":"code","source":["class CustomModel(nn.Module):\n","  def __init__(self, custom_dataset, model_name, decoder_layers=6, dropout=None):\n","    super().__init__()\n","\n","    self.custom_dataset = custom_dataset\n","\n","    config = BartConfig.from_pretrained(model_name)\n","\n","    config.decoder_layers = decoder_layers\n","\n","    self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config = config)\n","\n","    if dropout:\n","      self.model.dropout = dropout\n","\n","    # encoder freezing\n","    for param in self.model.get_encoder().parameters():\n","        param.requires_grad = False\n","\n","    self.batch_size = -1\n","    self.batch_cnt = -1\n","\n","    self.test_result = [[],[]]\n","\n","  def train(self, training_args, train_df, valid_df):\n","      train_dataset = self.custom_dataset.get_inputs(train_df)\n","      valid_dataset = self.custom_dataset.get_inputs(valid_df)\n","\n","      trainer = Seq2SeqTrainer(\n","        model=self.model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=valid_dataset,\n","      )\n","\n","      trainer.train()\n","\n","  def generate(self, df):\n","      input_ids = self.custom_dataset.get_input_ids(df)\n","      outputs = self.model.generate(input_ids.to(device), max_length=68)\n","\n","      output_sentences = []\n","      for output in outputs:\n","          output_sentences.append(tokenizer.decode(output, skip_special_tokens=True))\n","\n","      return output_sentences\n","\n","  def test_accuracy(self, df, batch_size=100):\n","      self.batch_size = batch_size\n","      self.batch_cnt = 0\n","      self.test_result = [[], []]\n","\n","      df_len = len(df)\n","      start,end = 0,self.batch_size\n","\n","      performance = 0\n","      while 1:\n","          if start >= df_len:\n","              break\n","\n","          end = min(end, df_len)\n","          performance += self._test_accuracy(df[start:end])\n","\n","          start = end\n","          end += self.batch_size\n","\n","      performance /= self.batch_cnt\n","      print(f'final performance : {performance}')\n","\n","      return performance\n","\n","  def _test_accuracy(self, df):\n","      self.batch_cnt += 1\n","\n","      input_sentences = self.generate(df)\n","      target_sentences = [sentence for sentence in df['summary']]\n","\n","      df_len = len(df)\n","      cnt_prev, cnt, cntO = ((self.batch_cnt-1) * self.batch_size),0,0\n","\n","      print(f'### {self.batch_cnt} batch start ###')\n","      for i in range(df_len) :\n","          cnt += 1\n","          cnt_global = cnt_prev + cnt\n","\n","          input_sentence = input_sentences[i]\n","          target_sentence = target_sentences[i]\n","\n","          if input_sentence == target_sentence :\n","              cntO += 1\n","          else :\n","              self.test_result[0].append(input_sentence)\n","              self.test_result[1].append(target_sentence)\n","\n","          # if cnt % 100 == 0:\n","          #     print(f'{cnt_global} generated')\n","      # if cnt % 100 != 0:\n","      #     print(f'{cnt_global} generated')\n","\n","      performance = cntO/cnt\n","      print(f'{self.batch_cnt} batch performance : {performance}\\n')\n","\n","      return performance\n","\n","\n","  def return_model(self):\n","      return self.model"],"metadata":{"id":"zf5e4DQIR15Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model 1\n","- nothing changed"],"metadata":{"id":"3GWsY2s3kWUL"}},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/final_models/요약/data/data_100_1000/'\n","# data_path = '/content/drive/MyDrive/final_models/요약/data/'\n","data_size = 1000\n","\n","train_df = pd.read_csv(f'{data_path}summary_train_dataset_{data_size}.tsv', sep='\\t')\n","valid_df = pd.read_csv(f'{data_path}summary_val_dataset_{data_size}.tsv', sep='\\t')"],"metadata":{"id":"r8V2lUCmS6Ar","executionInfo":{"status":"ok","timestamp":1694944441297,"user_tz":-540,"elapsed":2448,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["한 번에 에폭 다 돌고 성능 측정"],"metadata":{"id":"TmOt6F3pTAUI"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = CustomModel(custom_dataset, model_name)\n","# model.return_model()\n","\n","model_path = \"/content/drive/MyDrive/model/\"\n","\n","num_train_epochs = 100\n","batch_size = 10\n","step = data_size / batch_size * 100\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=model_path, #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=num_train_epochs, # number of training epochs\n","    per_device_train_batch_size=batch_size, # batch size for training\n","    per_device_eval_batch_size=batch_size,  # batch size for evaluation\n","    eval_steps=step, # Number of update steps between two evaluations.\n","    save_steps=step, # after # steps model is saved\n","    logging_steps=step,\n","    prediction_loss_only=True,\n","    evaluation_strategy=\"steps\",\n","    save_total_limit=3\n","    )\n","\n","model.train(training_args, train_df, valid_df)"],"metadata":{"id":"7yp5e8E1ljy-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.test_accuracy(train_df, 10)"],"metadata":{"id":"C9O-0E7t23H8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.test_result"],"metadata":{"id":"zAgqviN4VyIk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["에폭 별로 성능 측정"],"metadata":{"id":"qd3ne0MPImEm"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = CustomModel(custom_dataset, model_name)\n","\n","model.return_model()\n","\n","model_path = \"/content/drive/MyDrive/model/\"\n","\n","num_train_epochs = 100\n","batch_size = 10\n","step = 500\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=model_path, #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=num_train_epochs, # number of training epochs\n","    per_device_train_batch_size=batch_size, # batch size for training\n","    per_device_eval_batch_size=batch_size,  # batch size for evaluation\n","    eval_steps=step, # Number of update steps between two evaluations.\n","    save_steps=step, # after # steps model is saved\n","    logging_steps=step,\n","    prediction_loss_only=True,\n","    evaluation_strategy=\"steps\",\n","    save_total_limit=3\n","    )\n","\n","performance_max = 0\n","\n","for i in range(10):\n","    model.train(training_args, train_df, valid_df)\n","    performance = model.test_accuracy(train_df)\n","    print(f'{i+1} epoch performance : {performance}')\n","\n","    if performance_max < performance :\n","        performance_max = performance\n","        print(f'{i+1} epoch is best model\\n')"],"metadata":{"id":"DfGagPn9YY6R","colab":{"base_uri":"https://localhost:8080/","height":483},"executionInfo":{"status":"error","timestamp":1694944592670,"user_tz":-540,"elapsed":143722,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"9edc361d-ee07-407d-892c-4d5adb1a8fd3"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='243' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  243/10000 02:15 < 1:31:17, 1.78 it/s, Epoch 2.42/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-0158d7bb66ac>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{i+1} epoch performance : {performance}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-260d3a7392c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_args, train_df, valid_df)\u001b[0m\n\u001b[1;32m     34\u001b[0m       )\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1554\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1835\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1983\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["for i in range(10) :\n","  print(f'gen : {model.test_result[0][i]}')\n","  print(f'tar : {model.test_result[1][i]}\\n')"],"metadata":{"id":"L92H75Sc9rnU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델 학습 재개"],"metadata":{"id":"DAP9j3kBp6Cl"}},{"cell_type":"code","source":["# 모델 초기화\n","model_name = 'digit82/kobart-summarization'\n","model1 = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","# 모델 가중치 로드\n","model1.load_state_dict(torch.load(\"/content/drive/MyDrive/model/base_model_10000.pt/pytorch_model.bin\"))"],"metadata":{"id":"HGQVF7QWoHLO"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}