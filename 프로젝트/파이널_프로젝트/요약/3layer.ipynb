{"cells":[{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":4778,"status":"ok","timestamp":1694420212999,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"mQKDlSmxVVr3"},"outputs":[],"source":["!pip install accelerate>=0.20.1"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"wkMpgdZzVfVN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694420215332,"user_tz":-540,"elapsed":2344,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"4f04c37f-7d47-49f3-989d-1a519fcad8f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9746,"status":"ok","timestamp":1694420225073,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"nsKTJfE8VgNK","outputId":"8ffa2f3c-d5f4-407a-a323-78510f2e3ff2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.0.8)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.1.1)\n","Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n","Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.6)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n"]}],"source":[" !pip install transformers\n"," !pip install pytorch-lightning"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"VcgWlnZLA4Ex","executionInfo":{"status":"ok","timestamp":1694420225074,"user_tz":-540,"elapsed":6,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n","    DataCollatorForSeq2Seq,\n",")"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"K4uY9GJLVhG4","executionInfo":{"status":"ok","timestamp":1694420231313,"user_tz":-540,"elapsed":6244,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["import pandas as pd\n","train_df = pd.read_csv('/content/drive/MyDrive/data/news_summary_train_dataset.tsv', sep='\\t')\n","df = train_df[:100]"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"_FxmuxvPViP5","executionInfo":{"status":"ok","timestamp":1694420231313,"user_tz":-540,"elapsed":9,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2394,"status":"ok","timestamp":1694420233698,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"WSQ6VN52VjRR","outputId":"93a6d8ef-5ddb-4904-c3ba-5e1853541eff"},"outputs":[{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]}],"source":["import torch\n","from transformers import PreTrainedTokenizerFast\n","from transformers import BartForConditionalGeneration\n","\n","model_name = 'hyunwoongko/kobart'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","# import torch\n","# from transformers import PreTrainedTokenizerFast\n","# from transformers import BartForConditionalGeneration\n","# import torch.nn as nn\n","\n","# model_name = 'hyunwoongko/kobart'\n","# tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"h54vAWXeVl-h","executionInfo":{"status":"ok","timestamp":1694420234213,"user_tz":-540,"elapsed":519,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["input_tokenized = []\n","target_tokenized = []\n","for i in range(len(df)):\n","\n","    input_text = df.iloc[i, 0]\n","    target_text = df.iloc[i, 1]\n","\n","    input_ids = tokenizer(input_text, padding=\"max_length\", max_length=512, truncation=True)[\"input_ids\"]\n","    target_ids = tokenizer(target_text, padding=\"max_length\", max_length=512, truncation=True)[\"input_ids\"]\n","\n","    input_tokenized.append(input_ids)\n","    target_tokenized.append(target_ids)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"KS2MRypDVyB3","executionInfo":{"status":"ok","timestamp":1694420234213,"user_tz":-540,"elapsed":11,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, input_tokenized, target_tokenized):\n","        self.input_tokenized = torch.LongTensor(input_tokenized)\n","        self.target_tokenized = torch.LongTensor(target_tokenized)\n","\n","    def __len__(self):\n","        return len(self.input_tokenized)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.input_tokenized[idx],\n","            'labels': self.target_tokenized[idx]\n","        }\n"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1694420234214,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"o7Nd9BlfVytf","outputId":"90a62e93-cb54-4634-e703-6fb114910291"},"outputs":[{"output_type":"stream","name":"stdout","text":["90 10 90 10\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","# 학습을 위해 train, test set으로 나눈다.\n","input_train, input_test, target_train, target_test = train_test_split(input_tokenized, target_tokenized, test_size=0.1, random_state=42)\n","print(len(input_train), len(input_test), len(target_train), len(target_test))"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3571,"status":"ok","timestamp":1694420237778,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"4a5Qsp8mBVKn","outputId":"d9e2e0d5-356e-43dc-9525-c2c4c8699dc9"},"outputs":[{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]}],"source":["train_dataset = CustomDataset(input_train, target_train)\n","test_dataset = CustomDataset(input_test,target_test)\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=tokenizer, model=model\n",")"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"qJf6m07yV1zQ","executionInfo":{"status":"ok","timestamp":1694420237780,"user_tz":-540,"elapsed":40,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["# from torch.utils.data import DataLoader\n","\n","# train_dataset = CustomDataset(input_train, target_train)\n","# test_dataset = CustomDataset(input_test,target_test)\n","\n","# dl_train = DataLoader(train_dataset, batch_size=1, shuffle=False)\n","# dl_test = DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1694420237781,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"LAmZ3e6hV2k3","outputId":"f5613fac-f1ea-401d-bc3e-3451caf95e9a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":44}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"2WSPakyHHfwr","executionInfo":{"status":"ok","timestamp":1694420237783,"user_tz":-540,"elapsed":35,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["# from transformers import BartConfig\n","# import torch.nn as nn\n","\n","# class SummarizeModel(nn.Module):\n","#     def __init__(self, model_name = 'digit82/kobart-summarization', num_decode_layers = 1):\n","#         super(SummarizeModel, self).__init__()\n","#         self.config = BartConfig.from_pretrained(model_name)\n","#         self.config.decoder_layers = num_decode_layers\n","#         self.bart =  BartForConditionalGeneration.from_pretrained(model_name, config = self.config)\n","#         for param in self.bart.get_encoder().parameters():\n","#                 param.requires_grad = False\n","\n","#     def forward(self, input_ids, dec_input):\n","#         result = self.bart(input_ids, dec_input)\n","\n","#         return result[0]\n","\n","\n","#     def generate(self, input_ids):\n","#         result = self.bart(input_ids)\n","\n","#         return result[0]"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"zfamcVq6JMNW","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"error","timestamp":1694420247076,"user_tz":-540,"elapsed":626,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"31af488b-b951-4f75-e91e-8d4dd7e633f4"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-18bf08814031>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":31,"status":"aborted","timestamp":1694420237785,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"6Tb-cYXEFfK2"},"outputs":[],"source":["import torch\n","from transformers import PreTrainedTokenizerFast\n","from transformers import BartForConditionalGeneration\n","from transformers import BartConfig\n","\n","model_name = 'hyunwoongko/kobart'\n","config = BartConfig.from_pretrained(model_name)\n","config.decoder_layers = 3\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config = config)\n","for param in model.get_encoder().parameters():\n","    param.requires_grad = False\n","\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJyTLcQmBZjp","executionInfo":{"status":"aborted","timestamp":1694420237786,"user_tz":-540,"elapsed":32,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["model_path = \"/content/drive/MyDrive/park/models/\"\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=model_path, #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=2, # number of training epochs\n","    per_device_train_batch_size=16, # batch size for training\n","    per_device_eval_batch_size=16,  # batch size for evaluation\n","    eval_steps=500, # Number of update steps between two evaluations.\n","    save_steps=1000, # after # steps model is saved\n","    warmup_steps=300,# number of warmup steps for learning rate scheduler\n","    prediction_loss_only=True,\n","    evaluation_strategy=\"steps\",\n","    save_total_limit=3\n","    )\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":31,"status":"aborted","timestamp":1694420237786,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"h0ufGHqdBe0S"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Bfg9s-9TlUc"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phB1l4uETmQE"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6JSZJXRBi5r"},"outputs":[],"source":["torch.save(model, \"/content/drive/MyDrive/park/models/model_summary.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpVrlbi5Bni_"},"outputs":[],"source":["input_ids = tokenizer(df.iloc[120, 0], return_tensors=\"pt\")[\"input_ids\"]\n","\n","generated_ids = model.generate(input_ids.to(device))\n","generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9PnjkVWsXjA"},"outputs":[],"source":["model_path = \"/content/drive/MyDrive/park/models/\"\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=model_path, #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=2, # number of training epochs\n","    per_device_train_batch_size=64, # batch size for training\n","    per_device_eval_batch_size=64,  # batch size for evaluation\n","    eval_steps=500, # Number of update steps between two evaluations.\n","    save_steps=1000, # after # steps model is saved\n","    warmup_steps=300,# number of warmup steps for learning rate scheduler\n","    prediction_loss_only=True,\n","    evaluation_strategy=\"steps\",\n","    save_total_limit=3\n","    )\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fnh5ZfctsYUO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qx-MWiU3BpKZ"},"outputs":[],"source":["generated_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mu9FYrh6BpGq"},"outputs":[],"source":["df.iloc[120, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_K-1GI0BpEa"},"outputs":[],"source":["generated_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZK6ES4aBpCD"},"outputs":[],"source":["df.iloc[120, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoGZEo52Bo_R"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kC8JFm3vBo4Q"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPzIHWL6Bn8N"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvFKG68HXahr"},"outputs":[],"source":["from transformers import BartConfig\n","import torch.nn as nn\n","\n","class SummarizeModel(nn.Module):\n","    def __init__(self, model_name = 'digit82/kobart-summarization', num_decode_layers = 1):\n","        super(SummarizeModel, self).__init__()\n","        self.config = BartConfig.from_pretrained(model_name)\n","        self.config.decoder_layers = num_decode_layers\n","        self.bart =  BartForConditionalGeneration.from_pretrained(model_name, config = self.config)\n","        for param in self.bart.get_encoder().parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input_ids, dec_input):\n","        result = self.bart(input_ids, dec_input)\n","\n","        return result[0]\n","\n","\n","    def generate(self, input_ids):\n","        result = self.bart(input_ids)\n","\n","        return result[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144,"referenced_widgets":["d4c1a2c3c34e44cab0f9ce05bc585473","1706f05512e2483cbae9fff3bfce463d","93db35e4b5c74d16a975a4988111d3e2","5763624462824c9db56dccf579612e7e","1743494651e24d239ff2ade243ce623b","67efc3eaf433469fbbb4a4df70600ad1","d0aee27f5ace41e0a16f8eecf0bddf82","25bdfd7c65e045f29253690b66f9d6fa","8d4848d39ffc4831a2e5e1094e726ece","ef71f2e792d242a5a2cd535802b2dc0b","cbcf75181dca448a8c6017cf611fe3f1"]},"executionInfo":{"elapsed":8715,"status":"ok","timestamp":1694398201283,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"5TE2OnEGDVdN","outputId":"29bc235e-e769-454d-9832-478eef513051"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4c1a2c3c34e44cab0f9ce05bc585473","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at gogamza/kobart-summarization were not used when initializing BartForConditionalGeneration: ['model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias']\n","- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model = SummarizeModel().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1694398201284,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"w_ljAJ-vHdrF","outputId":"c19385c1-2a7b-4aa8-8c02-e1e05e28010e"},"outputs":[{"data":{"text/plain":["SummarizeModel(\n","  (bart): BartForConditionalGeneration(\n","    (model): BartModel(\n","      (shared): Embedding(30000, 768, padding_idx=3)\n","      (encoder): BartEncoder(\n","        (embed_tokens): Embedding(30000, 768, padding_idx=3)\n","        (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n","        (layers): ModuleList(\n","          (0-5): 6 x BartEncoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (activation_fn): GELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (decoder): BartDecoder(\n","        (embed_tokens): Embedding(30000, 768, padding_idx=3)\n","        (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n","        (layers): ModuleList(\n","          (0): BartDecoderLayer(\n","            (self_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (activation_fn): GELUActivation()\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (encoder_attn): BartAttention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n","  )\n",")"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaMROhDnITzq"},"outputs":[],"source":["result = model.generate(next(iter(dl_train))['input_ids'].to(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694398204434,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"CQ6SvQ8TId4m","outputId":"3e9b46ac-1751-424d-84c0-61c6f42c7880"},"outputs":[{"data":{"text/plain":["torch.Size([1, 512, 30000])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["result.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"executionInfo":{"elapsed":16904,"status":"error","timestamp":1694398221336,"user":{"displayName":"10_bagger","userId":"06014791066569861598"},"user_tz":-540},"id":"yV_3zc2kC8DU","outputId":"7cac145e-3e2b-480e-df14-0f2aabba4355"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/100 [00:15<?, ?it/s]\n"]},{"ename":"OutOfMemoryError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-df7f795df6d8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-155d19ea2e87>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, dec_input)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1404\u001b[0m         )\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_logits\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 39.56 GiB total capacity; 37.87 GiB already allocated; 8.56 MiB free; 38.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["from tqdm import tqdm\n","epochs = 100\n","loss = nn.CrossEntropyLoss().to(device)\n","optim = torch.optim.Adam(params=model.parameters(), lr=0.001)\n","\n","for epoch in tqdm(range(epochs)):\n","    epoch_loss = 0.0\n","    for batch in dl_train:\n","        input, output = batch['input_ids'].to(device), batch['labels'].to(device)\n","        pred = model(input, output)\n","        pred = pred.transpose(0, 1)\n","        output = output.transpose(0, 1)\n","        batch_loss = 0.0\n","        for i in range(512):  # input_size만큼 돌기\n","            batch_loss += loss(pred[i], output[i])\n","        batch_loss = batch_loss/512\n","\n","        epoch_loss += batch_loss\n","\n","    epoch_loss = epoch_loss/len(dl_train)\n","    if (epoch+1) % 10 == 0:\n","      print('Epoch:', '%02d' % (epoch + 1), 'cost =', '{:.6f}'.format(epoch_loss))\n","\n","    optim.zero_grad()\n","    epoch_loss.backward()\n","    optim.step()\n","\n","    with torch.no_grad():\n","        epoch_val_loss = 0.0\n","        for batch_val in dl_test:\n","            input_val, output_val = batch_val['input_ids'].to(device), batch_val['labels'].to(device)\n","            pred_val = model.generate(input_val)\n","            pred_val = pred_val.transpose(0, 1)\n","            output_val = output_val.transpose(0, 1)\n","\n","            batch_loss_val = 0.0\n","            for i in range(512):\n","                batch_loss_val += loss(pred_val[i], output_val[i])\n","\n","            batch_loss_val = batch_loss_val/512\n","\n","        epoch_val_loss = epoch_val_loss/len(dl_test)\n","        if (epoch+1) % 10 == 0:\n","            print('validation loss =', '{:.6f}'.format(batch_loss_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRqD1BemULUF"},"outputs":[],"source":["torch.save(model, \"/content/drive/MyDrive/hong/model_simple.pt\")"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyNe4biP6Y8ECkyjtfBnTlCm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1706f05512e2483cbae9fff3bfce463d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67efc3eaf433469fbbb4a4df70600ad1","placeholder":"​","style":"IPY_MODEL_d0aee27f5ace41e0a16f8eecf0bddf82","value":"Downloading model.safetensors: 100%"}},"1743494651e24d239ff2ade243ce623b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25bdfd7c65e045f29253690b66f9d6fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5763624462824c9db56dccf579612e7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef71f2e792d242a5a2cd535802b2dc0b","placeholder":"​","style":"IPY_MODEL_cbcf75181dca448a8c6017cf611fe3f1","value":" 496M/496M [00:01&lt;00:00, 435MB/s]"}},"67efc3eaf433469fbbb4a4df70600ad1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d4848d39ffc4831a2e5e1094e726ece":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93db35e4b5c74d16a975a4988111d3e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_25bdfd7c65e045f29253690b66f9d6fa","max":495589764,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d4848d39ffc4831a2e5e1094e726ece","value":495589764}},"cbcf75181dca448a8c6017cf611fe3f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0aee27f5ace41e0a16f8eecf0bddf82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4c1a2c3c34e44cab0f9ce05bc585473":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1706f05512e2483cbae9fff3bfce463d","IPY_MODEL_93db35e4b5c74d16a975a4988111d3e2","IPY_MODEL_5763624462824c9db56dccf579612e7e"],"layout":"IPY_MODEL_1743494651e24d239ff2ade243ce623b"}},"ef71f2e792d242a5a2cd535802b2dc0b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}