{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"mQKDlSmxVVr3","executionInfo":{"status":"ok","timestamp":1694590355175,"user_tz":-540,"elapsed":4905,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["!pip install accelerate>=0.20.1"]},{"cell_type":"markdown","source":["# 필요한 모듈 install"],"metadata":{"id":"3dT4v3a_aZMi"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"wkMpgdZzVfVN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694590372490,"user_tz":-540,"elapsed":17319,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"7822c910-2fde-49cf-aee6-b23c3c8cdfc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"nsKTJfE8VgNK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694590390169,"user_tz":-540,"elapsed":17682,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"9a8e451d-9819-4021-dcdd-66777a716b0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n","Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.0.8-py3-none-any.whl (727 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.0/727.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.1.2-py3-none-any.whl (764 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.8/764.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n","Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.6)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n","Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n","Successfully installed lightning-utilities-0.9.0 pytorch-lightning-2.0.8 torchmetrics-1.1.2\n"]}],"source":[" !pip install transformers\n"," !pip install pytorch-lightning"]},{"cell_type":"code","source":["from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n","    DataCollatorForSeq2Seq,\n",")\n","import torch\n","import torch.nn as nn"],"metadata":{"id":"VcgWlnZLA4Ex","executionInfo":{"status":"ok","timestamp":1694590398310,"user_tz":-540,"elapsed":8143,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# 데이터 다운로드 - 데이터 개수 100개"],"metadata":{"id":"a5rwJChqHobs"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"K4uY9GJLVhG4","executionInfo":{"status":"ok","timestamp":1694590405281,"user_tz":-540,"elapsed":6973,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["import pandas as pd\n","train_df = pd.read_csv('/content/drive/MyDrive/data/news_summary_train_dataset.tsv', sep='\\t')\n","df = train_df[:100]"]},{"cell_type":"code","source":[],"metadata":{"id":"rRRMSahkHtEO","executionInfo":{"status":"ok","timestamp":1694590405281,"user_tz":-540,"elapsed":10,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# 모델, tokenizer 설정"],"metadata":{"id":"dBNWzu5RHuJ1"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"WSQ6VN52VjRR","colab":{"base_uri":"https://localhost:8080/","height":215,"referenced_widgets":["eefceb0181f6401e8e6488deb3636ecc","a56fae1ebf9943be8f408a2e575bf007","3183654d2d1c470aad8104d12605b587","ca29c750cea54e05ab840d139f6a6797","8acfe089f9284a528348ac7e31cd79a8","68c0b13e68324c5c87b57561a31df34b","34bf2a04090e426c9c7a7060119b4956","02a3b9c6d5b34984af24ca0e470b0c0b","202b41f3a3124dffbc207b9a5a81175a","af9035cc362c434a86299770a19c9b81","29422355853b4b318b4906d608a548e0","0a0a0b477e7c4db3af90640ae4ae214a","4b3d149e984e43178496617c6a7ead76","2bd35b6e1ea24ebd81242cd89de0b6ea","67917403a48e4d648ee0c269a8834604","b07e7a2b05ac412d8142040fc351b0ee","876fa1c2ba5d4cea9849676ba55a94fe","cb5a078caed2427ba38478cb52a215c1","c7affd9b14da400eb28e3c8fd122857b","68cdf88e7b59499683e9977497c0f0ec","6b3379c2ccc2492a996cdec071cc163a","3e447629d54145939fc91f30bb81e859","f60af3f8b85f40c38bf4ed563df21a65","c27803eec97847aa8da7ea24ed679e7d","40a3d9160a22481c8786b8ceebaea846","12da5d47852b441cad5a13be83f6f1de","4574d8c33b4a40d3819f928e5a455148","02ca9bef294b4c1dbdf9882737e2c03b","1ed1350c2e2347f9b7ab7008ed4e2ca6","d7453550a9ff4fbf9b11cf6720bca3ba","cc043681001349ed9d1736f9d863b861","0a9baa8659c7405daf2c35c6a9a69f05","8cc19c89f9d24dc29b9177ac9aa5bab5","f0ea55f566264ab595e441e7dd150fe2","6490c033b575447ea3f18f35006eccea","ffce22e60a774866b6a7f16b107ea641","b0b14ac10e754148a610673fd3e7a53c","281d9df332b4409aaec8522a4896c297","e2df48fadedd49d397fefdc7676fd51c","b1acf29f5da84c49b96c4ba7391bbfbe","d3a6653be0da452da3e725eb313c1e5d","00758b3c330e4badb170ce099c6bd97e","64e63a2410324c9ea63bff495f608cd9","2ba10587e81a47e69228e95ebda1d589","d35909159c1f41ceb007f6843c082018","6a0641f1a7814dd8b3e8a91cbf18cfde","7a5ec64f78b94aeb8ae485a77f05e8b5","a2b4b44115b54be38365c55aeae257d7","3cd8b4fa48284b2f8e9d82988117cba1","38c8bc3f09314ccfb5648b7f20a4413f","61d8eb1838ce4a41bf54d2733893b12c","65376d555fa44e989066f6a495637404","7fb3d694195046cfa60cece68bf882ff","bf8479c4822b48deb1aa4822168ce4c3","685a958c6e8d4a13a851f9fcadaecd51"]},"executionInfo":{"status":"ok","timestamp":1694590411867,"user_tz":-540,"elapsed":6594,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"a88ce8d2-9544-4990-a7df-24ce569c8a62"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/295 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eefceb0181f6401e8e6488deb3636ecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/682k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0a0b477e7c4db3af90640ae4ae214a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60af3f8b85f40c38bf4ed563df21a65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0ea55f566264ab595e441e7dd150fe2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/496M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d35909159c1f41ceb007f6843c082018"}},"metadata":{}}],"source":["model_name = 'digit82/kobart-summarization'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"]},{"cell_type":"markdown","source":["# 데이터 전처리\n","- target 데이터에 bos, eos 붙이기\n","- max_length : 1024"],"metadata":{"id":"Z6mM0ZstH_R6"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"h54vAWXeVl-h","executionInfo":{"status":"ok","timestamp":1694590420903,"user_tz":-540,"elapsed":1,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["input_tokenized = []\n","target_tokenized = []\n","for i in range(len(df)):\n","\n","    input_text = df.iloc[i, 0]\n","    target_text = df.iloc[i, 1]\n","\n","    input_ids = tokenizer(input_text, padding=\"max_length\", max_length=1024, truncation=True)[\"input_ids\"]\n","    target_ids = tokenizer('<s>' + target_text + '</s>', padding=\"max_length\", max_length=1024, truncation=True)[\"input_ids\"]\n","\n","    input_tokenized.append(input_ids)\n","    target_tokenized.append(target_ids)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"KS2MRypDVyB3","executionInfo":{"status":"ok","timestamp":1694590428190,"user_tz":-540,"elapsed":308,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, input_tokenized, target_tokenized):\n","        self.input_tokenized = torch.LongTensor(input_tokenized)\n","        self.target_tokenized = torch.LongTensor(target_tokenized)\n","\n","    def __len__(self):\n","        return len(self.input_tokenized)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.input_tokenized[idx],\n","            'labels': self.target_tokenized[idx]\n","        }"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"o7Nd9BlfVytf","executionInfo":{"status":"ok","timestamp":1694590430205,"user_tz":-540,"elapsed":2,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["input_train, input_test, output_train, output_test = input_tokenized[:90], input_tokenized[90:100], target_tokenized[:90], target_tokenized[90:100]"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"qJf6m07yV1zQ","executionInfo":{"status":"ok","timestamp":1694590609466,"user_tz":-540,"elapsed":1,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"outputs":[],"source":["# from torch.utils.data import DataLoader\n","\n","# train_dataset = CustomDataset(input_train, output_train)\n","# test_dataset = CustomDataset(input_test,output_test)\n","\n","# dl_train = DataLoader(train_dataset, batch_size=5, shuffle=False)\n","# dl_test = DataLoader(test_dataset, batch_size=5, shuffle=False)\n","\n","train_dataset = CustomDataset(input_train, output_train)\n","test_dataset = CustomDataset(input_test,output_test)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"LAmZ3e6hV2k3","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1694590438172,"user_tz":-540,"elapsed":576,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"4504e93b-e04a-4914-e559-40951a19bde2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"zfamcVq6JMNW","executionInfo":{"status":"ok","timestamp":1694590439713,"user_tz":-540,"elapsed":3,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import torch\n","# from transformers import PreTrainedTokenizerFast\n","# from transformers import BartForConditionalGeneration\n","# from transformers import BartConfig\n","\n","# model_name = 'digit82/kobart-summarization'\n","# config = BartConfig.from_pretrained(model_name)\n","# config.decoder_layers = 2\n","\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config = config)\n","for param in model.get_encoder().parameters():\n","    param.requires_grad = False\n","\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=tokenizer, model=model\n",")\n","\n","\n","model"],"metadata":{"id":"6Tb-cYXEFfK2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694590592032,"user_tz":-540,"elapsed":4,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"b6d0cd3f-cf64-4384-eb2f-c7e9e06c6554"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BartForConditionalGeneration(\n","  (model): BartModel(\n","    (shared): Embedding(30000, 768, padding_idx=3)\n","    (encoder): BartEncoder(\n","      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n","      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n","      (layers): ModuleList(\n","        (0-5): 6 x BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): BartDecoder(\n","      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n","      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n","      (layers): ModuleList(\n","        (0-5): 6 x BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",")"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/hong/model/\"\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=model_path, #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=5, # number of training epochs\n","    per_device_train_batch_size=9, # batch size for training\n","    per_device_eval_batch_size=10,  # batch size for evaluation\n","    eval_steps=12, # Number of update steps between two evaluations.\n","    save_steps=1000, # after # steps model is saved\n","    logging_steps=10,\n","    warmup_steps=300,# number of warmup steps for learning rate scheduler\n","    prediction_loss_only=True,\n","    evaluation_strategy=\"steps\",\n","    save_total_limit=3\n","    )\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"],"metadata":{"id":"vF-RCL4HdpL8","executionInfo":{"status":"ok","timestamp":1694591021239,"user_tz":-540,"elapsed":3,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["result = trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"3I_XLM0ueAH4","executionInfo":{"status":"ok","timestamp":1694591051530,"user_tz":-540,"elapsed":26988,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"a0665287-a9a9-47ec-d499-919d9f0fbb21"},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [50/50 00:26, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>12</td>\n","      <td>4.582600</td>\n","      <td>4.588061</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>4.580900</td>\n","      <td>4.585445</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>4.578000</td>\n","      <td>4.581874</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>4.574200</td>\n","      <td>4.578203</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["result.training_loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUQyKkFdf23O","executionInfo":{"status":"ok","timestamp":1694591065328,"user_tz":-540,"elapsed":335,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"c2b3cdf6-70c7-4605-fe4f-6235fb75ae4c"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.576835479736328"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["## 학습 돌리기"],"metadata":{"id":"mol0R9HrM5vI"}},{"cell_type":"code","source":["model = model.to(device)\n","\n","from tqdm import tqdm\n","epochs = 200\n","loss = nn.CrossEntropyLoss().to(device)\n","optim = torch.optim.Adam(params=model.parameters(), lr=0.001)\n","\n","epoch_train_loss_list = []\n","epoch_val_loss_list= []\n","\n","for epoch in tqdm(range(epochs)):\n","    epoch_loss = 0.0\n","    for batch in dl_train:\n","        input, output = batch['input_ids'].to(device), batch['labels'].to(device)\n","        pred = model(input, output)\n","        batch_loss = 0.0\n","        batch_loss += loss(pred[0].transpose(1, 2), output)\n","\n","        epoch_loss += batch_loss\n","\n","    epoch_loss = epoch_loss/len(dl_train)\n","\n","    epoch_loss.append(epoch_loss.item())\n","\n","    optim.zero_grad()\n","    epoch_loss.backward()\n","    optim.step()\n","\n","    if (epoch+1) % 10 == 0:\n","        print('Epoch:', '%02d' % (epoch + 1), 'cost =', '{:.6f}'.format(epoch_loss))\n","\n","    # optim.zero_grad()\n","    # epoch_loss.backward()\n","    # optim.step()\n","\n","    with torch.no_grad():\n","\n","        epoch_val_loss = 0.0\n","        for batch_val in dl_test:\n","            input_val, output_val = batch_val['input_ids'].to(device), batch_val['labels'].to(device)\n","            pred_val = model(input_val)\n","            batch_loss_val = 0.0\n","            batch_loss_val += loss(pred_val[0].transpose(1, 2), output_val)\n","\n","            epoch_val_loss += batch_loss_val\n","\n","        epoch_val_loss = epoch_val_loss/len(dl_test)\n","\n","        epoch_val_loss_list.append(epoch_val_loss.item())\n","        if (epoch+1) % 10 == 0:\n","            print('validation loss =', '{:.6f}'.format(epoch_val_loss))"],"metadata":{"id":"Nwhxz2__M8RY","colab":{"base_uri":"https://localhost:8080/","height":493},"executionInfo":{"status":"error","timestamp":1694589772106,"user_tz":-540,"elapsed":9942,"user":{"displayName":"10_bagger","userId":"06014791066569861598"}},"outputId":"c1b377bb-e567-4386-82cb-505bd234ca61"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/200 [00:04<?, ?it/s]\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-16f4c827ce8f>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 )\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1389\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1275\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1130\u001b[0m                 )\n\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1133\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;31m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n\u001b[0m\u001b[1;32m    447\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0msrc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 0; 39.56 GiB total capacity; 37.38 GiB already allocated; 264.56 MiB free; 37.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"markdown","source":["## ACCURACY찍기!"],"metadata":{"id":"V5cRi0D3n0wV"}},{"cell_type":"code","source":[],"metadata":{"id":"UYVfe0NqnzwQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 100개 데이터 모델로 돌린 결과 리스트에 담기 : 담긴건 토큰들\n","result = []\n","for i in range(100):\n","    input_ids = tokenizer(df.iloc[i, 0], return_tensors=\"pt\")[\"input_ids\"]\n","    output = model.generate(input_ids.to(device))\n","    result.append(output[0])"],"metadata":{"id":"4M83yKK9XeZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# label토큰에서 pad빼기 귀찮으니까 그냥 다시 토큰화 해버리기, special token없이\n","target_tokenized = []\n","for i in range(len(df)):\n","    target_text = df.iloc[i, 1]\n","    target_ids = tokenizer(target_text)[\"input_ids\"]\n","    target_tokenized.append(target_ids)"],"metadata":{"id":"JQQicedWcT0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["int(False)"],"metadata":{"id":"IuYBCYq4e8pV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 결과물에는 [2, 0, ..., 1] 이런식으로 다 되어있어서 스페셜토큰 다 빼주려고 슬라이싱해서 없어버렸습니다. 그리고 반환타입이 텐서인데 그냥 리스트로 바꿔버림\n","result_new = []\n","for i in range(100):\n","    result_new.append(result[i].tolist()[2:-1])\n","\n","result_new[2]"],"metadata":{"id":"punc_7Xqfaey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# count는 예측이랑 정답의 같은 위치에 같은 토큰이 있을경우 +1\n","# total은 정답 토큰 총 개수\n","count = 0\n","total = 0\n","for i in range(100):\n","    total += len(target_tokenized[i])\n","    for j in range(len(result_new[i])):\n","        count += int(result_new[i][j] == target_tokenized[i][j])\n","\n","print(count/total)"],"metadata":{"id":"8aqeyJNSc1M9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["348/12"],"metadata":{"id":"gDcBhwj4dWWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count"],"metadata":{"id":"mG2sHoXNiaCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total"],"metadata":{"id":"RjryZpwAicPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_text = []\n","for i in range(100):\n","    predict_text.append(tokenizer.decode(result_new[i]))"],"metadata":{"id":"m3WcxEzSk5Ih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_result = pd.DataFrame({\"predict\":predict_text})\n","df_result['labels'] = df.iloc[:, 1]"],"metadata":{"id":"cjsuxgOjlRAv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_result.head(10)"],"metadata":{"id":"ylv4K8rNlenM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result"],"metadata":{"id":"IIw-szn6dtLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(100):\n","    if len(result)"],"metadata":{"id":"Hsv1lcOGdDuj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.decode(output[0], skip_special_tokens=True)"],"metadata":{"id":"VOgpLGhqX5GG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.iloc[0, 1]"],"metadata":{"id":"EtHU4t66X_K3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"1Bfg9s-9TlUc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"phB1l4uETmQE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model, \"/content/drive/MyDrive/hong/models/model_summary.pt\")"],"metadata":{"id":"c6JSZJXRBi5r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer(df.iloc[0, 0], return_tensors=\"pt\")[\"input_ids\"]\n","\n","generated_ids = model.generate(input_ids.to(device))\n","generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)"],"metadata":{"id":"xpVrlbi5Bni_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_text"],"metadata":{"id":"JYBpqjVub7qt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer(df.iloc[1, 0], return_tensors=\"pt\")[\"input_ids\"]\n","\n","generated_ids = model.generate(input_ids.to(device))\n","generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","generated_text"],"metadata":{"id":"h_Fh4PsjipVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = []\n","labels = df.iloc[:, 1]\n","for i in range(100):\n","    input_ids = tokenizer(df.iloc[i, 0], return_tensors=\"pt\")[\"input_ids\"]\n","\n","    generated_ids = model.generate(input_ids.to(device))\n","    # generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","    result.append(generated_ids)"],"metadata":{"id":"_V672sWMitxD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_tmp = pd.DataFrame({'result':result})\n","df_tmp['label'] = labels\n","df_tmp"],"metadata":{"id":"cPFqeEhzi_uL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_ids"],"metadata":{"id":"jjjdfZPhjYzp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result[4][0]"],"metadata":{"id":"yl70GzROlK8I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result[0][0][0].item()"],"metadata":{"id":"Ce42oLkjkzik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_tokenized[0]"],"metadata":{"id":"HH287WdHlWlY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while 3 in"],"metadata":{"id":"e6029l_GmNlb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","\n","for i in range(100):\n","    for j in range(len(result[i])):\n","        if result[i][0][j].item() == target_tokenized[i][j]:\n","            count += 1"],"metadata":{"id":"mKL7R7NOjebT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count"],"metadata":{"id":"c4pUa7-4kt5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/hong/models/\"\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=model_path, #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=5, # number of training epochs\n","    per_device_train_batch_size=64, # batch size for training\n","    per_device_eval_batch_size=64,  # batch size for evaluation\n","    eval_steps=500, # Number of update steps between two evaluations.\n","    save_steps=1000, # after # steps model is saved\n","    warmup_steps=300,# number of warmup steps for learning rate scheduler\n","    prediction_loss_only=True,\n","    evaluation_strategy=\"steps\",\n","    save_total_limit=3\n","    )\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"],"metadata":{"id":"m9PnjkVWsXjA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"Fnh5ZfctsYUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"J-E_hU680jPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint = torch.load(\"/content/drive/MyDrive/hong/models/checkpoint-3000/pytorch_model.bin\")\n","model.load_state_dict(checkpoint)"],"metadata":{"id":"qiRx4l-Q_Zjy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = model.to(device)"],"metadata":{"id":"a8_WXuy5_suR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer(df.iloc[120, 0], return_tensors=\"pt\")[\"input_ids\"]\n","\n","generated_ids = model.generate(input_ids.to(device))\n","generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)"],"metadata":{"id":"j_d_U4sjBFdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_text"],"metadata":{"id":"OmQgJnbLBNDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_text"],"metadata":{"id":"Qx-MWiU3BpKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.iloc[120, 1]"],"metadata":{"id":"Mu9FYrh6BpGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_text"],"metadata":{"id":"V_K-1GI0BpEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.iloc[110, 1]"],"metadata":{"id":"NZK6ES4aBpCD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer(df.iloc[110, 0], return_tensors=\"pt\")[\"input_ids\"]\n","\n","generated_ids = model.generate(input_ids.to(device))\n","generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)"],"metadata":{"id":"uoGZEo52Bo_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kC8JFm3vBo4Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uPzIHWL6Bn8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvFKG68HXahr"},"outputs":[],"source":["from transformers import BartConfig\n","import torch.nn as nn\n","\n","class SummarizeModel(nn.Module):\n","    def __init__(self, model_name = 'hyunwoongko/kobart', num_decode_layers = 1):\n","        super(SummarizeModel, self).__init__()\n","        self.config = BartConfig.from_pretrained(model_name)\n","        self.config.decoder_layers = num_decode_layers\n","        self.bart =  BartForConditionalGeneration.from_pretrained(model_name, config = self.config)\n","        for param in self.bart.get_encoder().parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input_ids, dec_input):\n","        result = self.bart(input_ids, dec_input)\n","\n","        return result[0]\n","\n","\n","    def generate(self, input_ids):\n","        result = self.bart(input_ids)\n","\n","        return result[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TE2OnEGDVdN"},"outputs":[],"source":["model = SummarizeModel().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_ljAJ-vHdrF"},"outputs":[],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaMROhDnITzq"},"outputs":[],"source":["result = model.generate(next(iter(dl_train))['input_ids'].to(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQ6SvQ8TId4m"},"outputs":[],"source":["result.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yV_3zc2kC8DU"},"outputs":[],"source":["# from tqdm import tqdm\n","# epochs = 100\n","# loss = nn.CrossEntropyLoss().to(device)\n","# optim = torch.optim.Adam(params=model.parameters(), lr=0.001)\n","\n","# for epoch in tqdm(range(epochs)):\n","#     epoch_loss = 0.0\n","#     for batch in dl_train:\n","#         input, output = batch['input_ids'].to(device), batch['labels'].to(device)\n","#         pred = model(input, output)\n","#         pred = pred.transpose(0, 1)\n","#         output = output.transpose(0, 1)\n","#         batch_loss = 0.0\n","#         for i in range(512):  # input_size만큼 돌기\n","#             batch_loss += loss(pred[i], output[i])\n","#         batch_loss = batch_loss/512\n","\n","#         epoch_loss += batch_loss\n","\n","#     epoch_loss = epoch_loss/len(dl_train)\n","#     if (epoch+1) % 10 == 0:\n","#       print('Epoch:', '%02d' % (epoch + 1), 'cost =', '{:.6f}'.format(epoch_loss))\n","\n","#     optim.zero_grad()\n","#     epoch_loss.backward()\n","#     optim.step()\n","\n","#     with torch.no_grad():\n","#         epoch_val_loss = 0.0\n","#         for batch_val in dl_test:\n","#             input_val, output_val = batch_val['input_ids'].to(device), batch_val['labels'].to(device)\n","#             pred_val = model.generate(input_val)\n","#             pred_val = pred_val.transpose(0, 1)\n","#             output_val = output_val.transpose(0, 1)\n","\n","#             batch_loss_val = 0.0\n","#             for i in range(512):\n","#                 batch_loss_val += loss(pred_val[i], output_val[i])\n","\n","#             batch_loss_val = batch_loss_val/512\n","\n","#         epoch_val_loss = epoch_val_loss/len(dl_test)\n","#         if (epoch+1) % 10 == 0:\n","#             print('validation loss =', '{:.6f}'.format(batch_loss_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRqD1BemULUF"},"outputs":[],"source":["torch.save(model, \"/content/drive/MyDrive/hong/model_simple.pt\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"eefceb0181f6401e8e6488deb3636ecc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a56fae1ebf9943be8f408a2e575bf007","IPY_MODEL_3183654d2d1c470aad8104d12605b587","IPY_MODEL_ca29c750cea54e05ab840d139f6a6797"],"layout":"IPY_MODEL_8acfe089f9284a528348ac7e31cd79a8"}},"a56fae1ebf9943be8f408a2e575bf007":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68c0b13e68324c5c87b57561a31df34b","placeholder":"​","style":"IPY_MODEL_34bf2a04090e426c9c7a7060119b4956","value":"Downloading (…)okenizer_config.json: 100%"}},"3183654d2d1c470aad8104d12605b587":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02a3b9c6d5b34984af24ca0e470b0c0b","max":295,"min":0,"orientation":"horizontal","style":"IPY_MODEL_202b41f3a3124dffbc207b9a5a81175a","value":295}},"ca29c750cea54e05ab840d139f6a6797":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af9035cc362c434a86299770a19c9b81","placeholder":"​","style":"IPY_MODEL_29422355853b4b318b4906d608a548e0","value":" 295/295 [00:00&lt;00:00, 21.5kB/s]"}},"8acfe089f9284a528348ac7e31cd79a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68c0b13e68324c5c87b57561a31df34b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34bf2a04090e426c9c7a7060119b4956":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02a3b9c6d5b34984af24ca0e470b0c0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"202b41f3a3124dffbc207b9a5a81175a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af9035cc362c434a86299770a19c9b81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29422355853b4b318b4906d608a548e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a0a0b477e7c4db3af90640ae4ae214a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b3d149e984e43178496617c6a7ead76","IPY_MODEL_2bd35b6e1ea24ebd81242cd89de0b6ea","IPY_MODEL_67917403a48e4d648ee0c269a8834604"],"layout":"IPY_MODEL_b07e7a2b05ac412d8142040fc351b0ee"}},"4b3d149e984e43178496617c6a7ead76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_876fa1c2ba5d4cea9849676ba55a94fe","placeholder":"​","style":"IPY_MODEL_cb5a078caed2427ba38478cb52a215c1","value":"Downloading (…)/main/tokenizer.json: 100%"}},"2bd35b6e1ea24ebd81242cd89de0b6ea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7affd9b14da400eb28e3c8fd122857b","max":682133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68cdf88e7b59499683e9977497c0f0ec","value":682133}},"67917403a48e4d648ee0c269a8834604":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b3379c2ccc2492a996cdec071cc163a","placeholder":"​","style":"IPY_MODEL_3e447629d54145939fc91f30bb81e859","value":" 682k/682k [00:00&lt;00:00, 3.50MB/s]"}},"b07e7a2b05ac412d8142040fc351b0ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"876fa1c2ba5d4cea9849676ba55a94fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb5a078caed2427ba38478cb52a215c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7affd9b14da400eb28e3c8fd122857b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68cdf88e7b59499683e9977497c0f0ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b3379c2ccc2492a996cdec071cc163a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e447629d54145939fc91f30bb81e859":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f60af3f8b85f40c38bf4ed563df21a65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c27803eec97847aa8da7ea24ed679e7d","IPY_MODEL_40a3d9160a22481c8786b8ceebaea846","IPY_MODEL_12da5d47852b441cad5a13be83f6f1de"],"layout":"IPY_MODEL_4574d8c33b4a40d3819f928e5a455148"}},"c27803eec97847aa8da7ea24ed679e7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ca9bef294b4c1dbdf9882737e2c03b","placeholder":"​","style":"IPY_MODEL_1ed1350c2e2347f9b7ab7008ed4e2ca6","value":"Downloading (…)cial_tokens_map.json: 100%"}},"40a3d9160a22481c8786b8ceebaea846":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7453550a9ff4fbf9b11cf6720bca3ba","max":109,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc043681001349ed9d1736f9d863b861","value":109}},"12da5d47852b441cad5a13be83f6f1de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a9baa8659c7405daf2c35c6a9a69f05","placeholder":"​","style":"IPY_MODEL_8cc19c89f9d24dc29b9177ac9aa5bab5","value":" 109/109 [00:00&lt;00:00, 7.01kB/s]"}},"4574d8c33b4a40d3819f928e5a455148":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02ca9bef294b4c1dbdf9882737e2c03b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ed1350c2e2347f9b7ab7008ed4e2ca6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7453550a9ff4fbf9b11cf6720bca3ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc043681001349ed9d1736f9d863b861":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a9baa8659c7405daf2c35c6a9a69f05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cc19c89f9d24dc29b9177ac9aa5bab5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0ea55f566264ab595e441e7dd150fe2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6490c033b575447ea3f18f35006eccea","IPY_MODEL_ffce22e60a774866b6a7f16b107ea641","IPY_MODEL_b0b14ac10e754148a610673fd3e7a53c"],"layout":"IPY_MODEL_281d9df332b4409aaec8522a4896c297"}},"6490c033b575447ea3f18f35006eccea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2df48fadedd49d397fefdc7676fd51c","placeholder":"​","style":"IPY_MODEL_b1acf29f5da84c49b96c4ba7391bbfbe","value":"Downloading (…)lve/main/config.json: 100%"}},"ffce22e60a774866b6a7f16b107ea641":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3a6653be0da452da3e725eb313c1e5d","max":1199,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00758b3c330e4badb170ce099c6bd97e","value":1199}},"b0b14ac10e754148a610673fd3e7a53c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64e63a2410324c9ea63bff495f608cd9","placeholder":"​","style":"IPY_MODEL_2ba10587e81a47e69228e95ebda1d589","value":" 1.20k/1.20k [00:00&lt;00:00, 110kB/s]"}},"281d9df332b4409aaec8522a4896c297":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2df48fadedd49d397fefdc7676fd51c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1acf29f5da84c49b96c4ba7391bbfbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3a6653be0da452da3e725eb313c1e5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00758b3c330e4badb170ce099c6bd97e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64e63a2410324c9ea63bff495f608cd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ba10587e81a47e69228e95ebda1d589":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d35909159c1f41ceb007f6843c082018":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a0641f1a7814dd8b3e8a91cbf18cfde","IPY_MODEL_7a5ec64f78b94aeb8ae485a77f05e8b5","IPY_MODEL_a2b4b44115b54be38365c55aeae257d7"],"layout":"IPY_MODEL_3cd8b4fa48284b2f8e9d82988117cba1"}},"6a0641f1a7814dd8b3e8a91cbf18cfde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38c8bc3f09314ccfb5648b7f20a4413f","placeholder":"​","style":"IPY_MODEL_61d8eb1838ce4a41bf54d2733893b12c","value":"Downloading pytorch_model.bin: 100%"}},"7a5ec64f78b94aeb8ae485a77f05e8b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65376d555fa44e989066f6a495637404","max":495656447,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7fb3d694195046cfa60cece68bf882ff","value":495656447}},"a2b4b44115b54be38365c55aeae257d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf8479c4822b48deb1aa4822168ce4c3","placeholder":"​","style":"IPY_MODEL_685a958c6e8d4a13a851f9fcadaecd51","value":" 496M/496M [00:03&lt;00:00, 181MB/s]"}},"3cd8b4fa48284b2f8e9d82988117cba1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38c8bc3f09314ccfb5648b7f20a4413f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61d8eb1838ce4a41bf54d2733893b12c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65376d555fa44e989066f6a495637404":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fb3d694195046cfa60cece68bf882ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf8479c4822b48deb1aa4822168ce4c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"685a958c6e8d4a13a851f9fcadaecd51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}